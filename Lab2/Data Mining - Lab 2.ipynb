{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2\n",
    "\n",
    "<b>Class:</b> MSDS 7331 Data Mining\n",
    "<br> <b>Dataset:</b> Belk Endowment Educational Attainment Data \n",
    "\n",
    "<h1 style=\"font-size:150%;\"> Teammates </h1>\n",
    "Maryam Shahini\n",
    "<br> Murtada Shubbar\n",
    "<br> Michael Toolin\n",
    "<br> Steven Millett"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set global variables\n",
    "#Variables for file and school informaiton\n",
    "\n",
    "YEARS = ['2014', '2015', '2016', '2017']\n",
    "SCHOOLS = ['High','Middle','Elementary']\n",
    "\n",
    "#Number of features we will be selecting for feature selection\n",
    "\n",
    "N_FEATURES_OPTIONS = [2, 25 , 50]\n",
    "\n",
    "#Alpha and C we will be using for our classifiers\n",
    "\n",
    "C_OPTIONS = [1e-2, 1e-1, 1e0, 1e1, 1e2]\n",
    "\n",
    "#Import data all necessary libraries we will be using in our estimation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import re\n",
    "import sklearn\n",
    "import statistics\n",
    "\n",
    "\n",
    "#from umap.umap_ import UMAP\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.feature_selection import SelectKBest, chi2, SelectPercentile, RFE\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, Binarizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.a Data Preparation\n",
    "10 points - Deﬁne and prepare your class variables. Use proper variable \n",
    "representations (int, ﬂoat, one-hot, etc.). Use pre-processing methods (as needed) for\n",
    "dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for \n",
    "the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.b Data Preparation\n",
    "5 points - Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 2017 Public Schools Machine Learning \n",
    "# Date Set is being used throughout this \n",
    "# analysis.  The _ML suffix is removed to less \n",
    "# name space size\n",
    "# Load Full Public School Data Frames for each year\n",
    "\n",
    "school_data = pd.DataFrame()\n",
    "\n",
    "for year in YEARS:\n",
    "    #Load public school master file\n",
    "    temp_year = pd.read_csv('../Data/'+str(year)+'/Machine Learning Datasets/PublicSchools'+str(year)+'_ML.csv', low_memory=False)\n",
    "    \n",
    "    #Iterate through adding and merging school data based on school type\n",
    "    for grade in SCHOOLS:\n",
    "        grade_temp_year = pd.read_csv('../Data/'+year+'/Machine Learning Datasets/Public'+grade+'Schools'+year+'_ML.csv', low_memory=False)\n",
    "        cols_to_use = grade_temp_year.columns.difference(temp_year.columns)\n",
    "        cols_to_use = np.append(cols_to_use,'unit_code')\n",
    "        temp_year = pd.merge(temp_year, grade_temp_year[cols_to_use],left_index=True, right_index=True, on='unit_code',how='left' )\n",
    "    \n",
    "    #Add year column and concatonating all data together\n",
    "    temp_year['Year']=year\n",
    "    school_data = pd.concat([school_data,temp_year],ignore_index=True, sort=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the critical threshold\n",
    "CRITICAL_NA = .75\n",
    "\n",
    "#With this we check if the column is less than 75% non-NA, if it is greater than 75% non-NA\n",
    "#We replace the NA with the median of the column, otherwise we replace the value with 0\n",
    "\n",
    "imputed_school_data = school_data.apply(lambda col: col.fillna(0) if col.count()/col.shape[0]<CRITICAL_NA else col.fillna(col.median()),axis=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.a Modeling and Evaluation\n",
    "Using the right evaluation metric for classification system is crucial. Otherwise, it may results in thinking that the model is performing well but in reality, it doesn’t.\n",
    "\n",
    "There are two tasks in this section of “NC Educational Data” project:\n",
    "\n",
    "The first task is to predict a binary classification target, either if the average SAT score of each school is good enough to gets the student to the North Carolina Universities or not. The SAT is a standardized test widely used for college admissions in the United States. For this purpose we have a cut off 1200 out of 1600.\n",
    "The second task is to predict if the crime per 100 students at each school level is higher than the LEA level or not.\n",
    "After considering all evaluation metrics for classification systems, we ended up using ROC Curve. Area under ROC Curve (or AUC for short) is a performance metric for binary classification problems.\n",
    "\n",
    "In fact, a ROC curve can be used to select a threshold for a classifier which maximizes the true positives, while minimizing the false positives. \n",
    "\n",
    "We usually use ROC when both classes detection are important. Here, our models should be able to decrease both false positive rate (which is identifying the schools with enough good average SAT score for getting admission in different universities) and also decreasing the false negative rate (which is detecting schools with not good average SAT scores).\n",
    "\n",
    "The same for the second task, it is important to decrease both false positive and false negative rates.\n",
    "\n",
    "The AUC represents a model’s ability to discriminate between positive and negative classes. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model as good as random. Most classifiers have AUCs that fall somewhere between these two values. Therefore, the overall model performances can be compared by considering the AUC.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.b Modeling and Evaluation\n",
    "10 points - Choose the method you will use for dividing your data into training and why testing splits (i.e., are you using Stratiﬁed 10-fold cross validation? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into X and y dataframes\n",
    "\n",
    "SPG_Grade_col = imputed_school_data.filter(regex=('^SPG\\WGrade')).columns\n",
    "imputed_school_data[SPG_Grade_col] = imputed_school_data[SPG_Grade_col].apply(lambda col: col.astype(int), axis=1)\n",
    "y = imputed_school_data[SPG_Grade_col].apply(lambda row:'A' if row.any()!=1 else row[0]*'A+NG'+row[1]*'B'+row[2]*'C'+row[3]*'D'+row[4]*'F'+row[5]*'I',axis=1)\n",
    "\n",
    "#Removed SPG Grade and unit code(which is primary key for school data table)\n",
    " \n",
    "X = imputed_school_data[school_data.columns.drop(list(school_data.filter(regex='^SPG\\WGrade|^SPG\\WScore|unit_code')))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into test and train sets. We still want\n",
    "# to do this for external Cross Validation\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.c Modeling and Evaluation\n",
    "20 points - Create three different classification/regression models (e.g., random forest, KNN, and SVM). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we establish a basic 10 k-fold internal\n",
    "# Cross Validation seperation that will be used\n",
    "# for training our model.\n",
    "\n",
    "k_fold = KFold(n_splits=10,shuffle=True)\n",
    "\n",
    "#This creates the template for the pipeline\n",
    "# This creates a basic pipeline where we will \n",
    "# test for dementionality reduction, scaling,\n",
    "# and classification.\n",
    "\n",
    "X_train = TSNE(n_components=2).fit_transform(X_train)\n",
    "\n",
    "pipe = Pipeline([\n",
    "                  ('scale', StandardScaler()), \n",
    "                  ('clf', LogisticRegression())])\n",
    "\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Don't run this unless you want to retrain the data.\n",
    "\n",
    "# # Here we are establishing the basic testing criteria\n",
    "# # for our pipeline. This will run through a number of\n",
    "# # parameters for our pipeline, including type of dimensionality\n",
    "# # reduction, number of features to reduce, scaling (yes/no), \n",
    "# # classification models, and parameters of the classification model.\n",
    "\n",
    "# param_grid = [\n",
    "#     {\n",
    "#         'reduce_dim': [NMF(), PCA(),TSNE()],\n",
    "#         'reduce_dim__n_components': N_FEATURES_OPTIONS,\n",
    "#         'scale':[None,StandardScaler()],\n",
    "#         'clf':[SVC(),LogisticRegression()],\n",
    "#         'clf__C': C_OPTIONS\n",
    "#     },\n",
    "#     {\n",
    "#         'reduce_dim': [NMF(), PCA(),TSNE()],\n",
    "#         'reduce_dim__n_components': N_FEATURES_OPTIONS,\n",
    "#         'scale':[None,StandardScaler()],\n",
    "#         'clf':[SGDClassifier(tol=1e-3,max_iter=1000)],\n",
    "#         'clf__alpha': C_OPTIONS\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "\n",
    "# # This will test the parameter dict against our \n",
    "# # pipeline\n",
    "\n",
    "# grid_search = GridSearchCV(pipe, param_grid=param_grid, cv=k_fold,n_jobs=-1, verbose=1 )\n",
    "\n",
    "\n",
    "# # Here we are training the model, this is \n",
    "# # what takes the most amount of time to run\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# #This saves the grid_search variable\n",
    "# # to an external file so we don't have to \n",
    "# # keep running the gridsearch\n",
    "\n",
    "# from sklearn.externals import joblib\n",
    "# joblib.dump(grid_search, 'savedBestModel.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.d Modeling and Evaluation\n",
    "10 points - Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Run this to load the model from the save file\n",
    "\n",
    "# from sklearn.externals import joblib\n",
    "# grid_search = joblib.load('savedBestModel.pkl')\n",
    "\n",
    "\n",
    "# # Loads all parameters run into a dict \n",
    "\n",
    "# params = np.array(grid_search.cv_results_['params'])\n",
    "\n",
    "\n",
    "# # Loads all mean test scores into an array\n",
    "\n",
    "# mean_scores = np.array(grid_search.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assigns all models to an array\n",
    "\n",
    "# classifier_labels=['SVC','LogisticRegression','SGDClassifier']\n",
    "\n",
    "\n",
    "# # Creates an empty dataframe that is to be\n",
    "# # filled with the mean test accuracy by C global\n",
    "# # variable and the different classifiers\n",
    "\n",
    "# classifier_temp = pd.DataFrame(columns=classifier_labels,index=C_OPTIONS,\n",
    "#                                data=np.linspace(.1,.25,num=len(C_OPTIONS)*len(classifier_labels)).reshape(len(C_OPTIONS),len(classifier_labels)))\n",
    "# classifier_temp.fillna(0,inplace=True)\n",
    "\n",
    "# for i, (param, score) in enumerate(zip(params, mean_scores)):\n",
    "#     C = param['clf__C'] if 'clf__C' in param else param['clf__alpha']\n",
    "#     class_state = str(param['clf']).split('(')[0]\n",
    "#     if classifier_temp.at[C,class_state] < score:\n",
    "#         classifier_temp.at[C,class_state] = score\n",
    "\n",
    "\n",
    "# # Printing a grid of the best accuracies\n",
    "        \n",
    "# display(classifier_temp.transpose())   \n",
    "\n",
    "\n",
    "# # Print a line plot which shows the best \n",
    "# # accuracies\n",
    " \n",
    "# classifier_temp.plot(logx=True,ylim=(0,1),figsize=(14,10),title='Accuracy by Classifier'); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assigns all reduction models to an array\n",
    "\n",
    "# reduce_labels=['NMF','PCA','SelectKBest']\n",
    "\n",
    "\n",
    "# # Translates the N Features array\n",
    "# # to an array full of string\n",
    "\n",
    "# temp_N_FEATURES_OPTIONS = [str(r) for r in N_FEATURES_OPTIONS]\n",
    "# temp_N_FEATURES_OPTIONS=temp_N_FEATURES_OPTIONS+['None']\n",
    "\n",
    "\n",
    "# # Creates an empty dataframe that is to be\n",
    "# # filled with the mean test accuracy by N Features\n",
    "# # variable and the different feature reduction models\n",
    "\n",
    "# reduce_temp = pd.DataFrame(columns=reduce_labels,index=temp_N_FEATURES_OPTIONS,\n",
    "#                                data=np.linspace(.1,.25,num=len(temp_N_FEATURES_OPTIONS)*len(reduce_labels)).reshape(+len(temp_N_FEATURES_OPTIONS),len(reduce_labels)))\n",
    "\n",
    "\n",
    "# for i, (param, score) in enumerate(zip(params, mean_scores)):\n",
    "#     trigger=0\n",
    "#     reduce_state = str(param['reduce_dim']).split('(')[0]\n",
    "#     if 'reduce_dim__k' in param:\n",
    "#         N_FEAT = str(param['reduce_dim__k'])\n",
    "#         trigger=1\n",
    "#     elif 'reduce_dim__n_components' in param:\n",
    "#         N_FEAT = str(param['reduce_dim__n_components'])\n",
    "#         trigger=1\n",
    "#     else:\n",
    "#         if reduce_temp.at['None','NMF'] < score:\n",
    "#             reduce_temp.at['None','NMF'] = score\n",
    "#             reduce_temp.at['None','SelectKBest'] = score\n",
    "#     if trigger == 1:\n",
    "#         if reduce_temp.at[N_FEAT,reduce_state] < score:\n",
    "#             reduce_temp.at[N_FEAT,reduce_state] = score\n",
    "\n",
    "            \n",
    "# # Printing a grid of the best accuracies\n",
    "\n",
    "# display(reduce_temp.transpose())\n",
    "\n",
    "\n",
    "# # Print a bar plot which shows the best \n",
    "# # accuracies\n",
    "\n",
    "# reduce_temp.plot(kind='bar',ylim=(0,1),figsize=(14,10),title='Accuracy by Feature Selection',rot=0);           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('The Index of the best model is',grid_search.best_index_)\n",
    "# print('The parameters of the best model is')\n",
    "# display(grid_search.best_params_)\n",
    "# print('The accuracy of the best model is',round(grid_search.best_score_*100,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.e Modeling and Evaluation\n",
    "10 points - Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference signiﬁcant with 95% conﬁdence? Use proper statistical comparison methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.f Modeling and Evaluation\n",
    "10 points - Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classiﬁcation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "5 points - How useful is yolur model for interested parties (i.e., the companies or organizations that might want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exceptional Work\n",
    "10 points - You have free reign to provide additional modeling. \n",
    "One idea: grid search parameters in a parallelized fashion and visualize the \n",
    "performances across attributes. Which parameters are most signiﬁcant for making a \n",
    "good model for each classiﬁcation algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
