{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2\n",
    "\n",
    "<b>Class:</b> MSDS 7331 Data Mining\n",
    "<br> <b>Dataset:</b> Belk Endowment Educational Attainment Data \n",
    "\n",
    "<h1 style=\"font-size:150%;\"> Teammates </h1>\n",
    "Maryam Shahini\n",
    "<br> Murtada Shubbar\n",
    "<br> Michael Toolin\n",
    "<br> Steven Millett"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set global variables\n",
    "#Variables for file and school informaiton\n",
    "\n",
    "YEARS = ['2014', '2015', '2016', '2017']\n",
    "SCHOOLS = ['High','Middle','Elementary']\n",
    "\n",
    "#Number of features we will be selecting for feature selection\n",
    "\n",
    "N_FEATURES_OPTIONS = [100, 300, \"all\"]\n",
    "\n",
    "#Alpha and C we will be using for our classifiers\n",
    "\n",
    "C_ESTIMATORS = [50, 100, 200, 500]\n",
    "C_DEPTH = [2, 3, 5]\n",
    "\n",
    "#Import data all necessary libraries we will be using in our estimation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import re\n",
    "import sklearn\n",
    "import statistics\n",
    "\n",
    "\n",
    "#from umap.umap_ import UMAP\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.feature_selection import SelectKBest, chi2, SelectPercentile, RFE\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, Binarizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier,GradientBoostingRegressor,AdaBoostClassifier,RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.a Data Preparation\n",
    "10 points - Deﬁne and prepare your class variables. Use proper variable \n",
    "representations (int, ﬂoat, one-hot, etc.). Use pre-processing methods (as needed) for\n",
    "dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for \n",
    "the analysis.\n",
    "\n",
    "# The Belk Endowment Educational Attainment Data Repository for North Carolina Public Schools\n",
    "Our data set originates from the North Carolina Public Schools Reports and Statistics. This public site contains large amounts of information covering many aspects of the performance of students and schools across the state of North Carolina. It includes public and charter schools ranging from the elementary level to high schools. http://www.ncpublicschools.org/\n",
    "\n",
    "The data used in our lab consists of portions of this data which includes the school years 2014-2017. The data used is the result of combining and cleaning the raw data sets available on the North Carolina website. The machine learning data sets are broken down by school year and then sub-setted by elementary school, middle school and high school information. \n",
    "\n",
    "In this lab our data set consists of all the data available for school years 2015-2017 from the Machine Learning data available.  First step is to combine all the data from previous years and add the variable ‘Year’ to each row, keeping track of which year this data was collected.\n",
    "\n",
    "Next the each feature is inspected for NA values.  If more than 75% of the feature contains NA, we replace that field with 0.  If less than 75% is NA, then the median value of the column is used to replace the NA.\n",
    "\n",
    "###Need description of what was done in Altyrex.\n",
    "\n",
    "Two binary classifications are performed.  The first classification looks at what\n",
    "\n",
    "\n",
    "\n",
    "|<p align=\"\">Variable|<p align=\"\">Type|<p align=\"\">Note|\n",
    "|--------|----|----|\n",
    "|<p align=\"\">Year|<p align=\"\">Object|<p align=\"\">Tracks year data is from|\n",
    "|<p align=\"\">local_crime_greater|<p align=\"\">int64|<p align=\"\">1 if crime in school > LEA average crime, 0 otherwise\n",
    "|<p align=\"\">X_crime_reduced|<p align=\"\">Data Frame|<p align=\"\">Used in crime reduced scope model, removes racial information from data|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.b Data Preparation\n",
    "5 points - Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\users\\mtool\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The 2017 Public Schools Machine Learning \n",
    "# Date Set is being used throughout this \n",
    "# analysis.  The _ML suffix is removed to less \n",
    "# name space size\n",
    "# Load Full Public School Data Frames for each year\n",
    "\n",
    "school_data = pd.DataFrame()\n",
    "\n",
    "for year in YEARS:\n",
    "    #Load public school master file\n",
    "    temp_year = pd.read_csv('../Data/'+str(year)+'/Machine Learning Datasets/PublicSchools'+str(year)+'_ML.csv', low_memory=False)\n",
    "    \n",
    "    #Iterate through adding and merging school data based on school type\n",
    "    for grade in SCHOOLS:\n",
    "        grade_temp_year = pd.read_csv('../Data/'+year+'/Machine Learning Datasets/Public'+grade+'Schools'+year+'_ML.csv', low_memory=False)\n",
    "        cols_to_use = grade_temp_year.columns.difference(temp_year.columns)\n",
    "        cols_to_use = np.append(cols_to_use,'unit_code')\n",
    "        temp_year = pd.merge(temp_year, grade_temp_year[cols_to_use],left_index=True, right_index=True, on='unit_code',how='left' )\n",
    "    \n",
    "    #Add year column and concatonating all data together\n",
    "    temp_year['Year']=year\n",
    "    school_data = pd.concat([school_data,temp_year],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the critical threshold\n",
    "CRITICAL_NA = .75\n",
    "\n",
    "#With this we check if the column is less than 75% non-NA, if it is greater than 75% non-NA\n",
    "#We replace the NA with the median of the column, otherwise we replace the value with 0\n",
    "\n",
    "imputed_school_data = school_data.apply(lambda col: col.fillna(0) if col.count()/col.shape[0]<CRITICAL_NA else col.fillna(col.median()),axis=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row</th>\n",
       "      <th>Year</th>\n",
       "      <th>avg_daily_attend_pct</th>\n",
       "      <th>lea_federal_perpupil_num</th>\n",
       "      <th>lea_local_perpupil_num</th>\n",
       "      <th>lea_salary_expense_pct</th>\n",
       "      <th>lea_services_expense_pct</th>\n",
       "      <th>lea_state_perpupil_num</th>\n",
       "      <th>lea_supplies_expense_pct</th>\n",
       "      <th>lea_total_expense_num</th>\n",
       "      <th>...</th>\n",
       "      <th>EOGReadingGr4_CACR_SWD</th>\n",
       "      <th>sat_avg_score_num</th>\n",
       "      <th>sat_participation_pct</th>\n",
       "      <th>student_num</th>\n",
       "      <th>SAT_SCORE_ZERO</th>\n",
       "      <th>SAT_Score_above1000</th>\n",
       "      <th>SAT_participation_number</th>\n",
       "      <th>EST_Student_College_NO</th>\n",
       "      <th>Student_Num_College_Ready_SAT</th>\n",
       "      <th>Unnamed: 272</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.872</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>925</td>\n",
       "      <td>0.12</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.36</td>\n",
       "      <td>68.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.957</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>979</td>\n",
       "      <td>0.63</td>\n",
       "      <td>539</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>339.57</td>\n",
       "      <td>199.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.954</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>884</td>\n",
       "      <td>0.70</td>\n",
       "      <td>547</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>382.90</td>\n",
       "      <td>164.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.943</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>824</td>\n",
       "      <td>0.45</td>\n",
       "      <td>800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>360.00</td>\n",
       "      <td>440.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.944</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>14.3</td>\n",
       "      <td>962</td>\n",
       "      <td>0.55</td>\n",
       "      <td>664</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>365.20</td>\n",
       "      <td>298.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.958</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.60</td>\n",
       "      <td>330</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>198.00</td>\n",
       "      <td>132.00</td>\n",
       "      <td>198.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.926</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1008</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1192</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>762.88</td>\n",
       "      <td>429.12</td>\n",
       "      <td>762.88</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.947</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>983</td>\n",
       "      <td>0.32</td>\n",
       "      <td>492</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>157.44</td>\n",
       "      <td>334.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.952</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>959</td>\n",
       "      <td>0.42</td>\n",
       "      <td>628</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>263.76</td>\n",
       "      <td>364.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.960</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>6.3</td>\n",
       "      <td>883</td>\n",
       "      <td>0.62</td>\n",
       "      <td>621</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>385.02</td>\n",
       "      <td>235.98</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.950</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>12.5</td>\n",
       "      <td>805</td>\n",
       "      <td>0.49</td>\n",
       "      <td>419</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>205.31</td>\n",
       "      <td>213.69</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.950</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>798</td>\n",
       "      <td>0.87</td>\n",
       "      <td>674</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>586.38</td>\n",
       "      <td>87.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.912</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>997</td>\n",
       "      <td>0.50</td>\n",
       "      <td>809</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>404.50</td>\n",
       "      <td>404.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.946</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>969</td>\n",
       "      <td>0.23</td>\n",
       "      <td>668</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>153.64</td>\n",
       "      <td>514.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.949</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>12.5</td>\n",
       "      <td>949</td>\n",
       "      <td>0.33</td>\n",
       "      <td>574</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>189.42</td>\n",
       "      <td>384.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.943</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>700</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>700.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.956</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.23</td>\n",
       "      <td>700</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>161.00</td>\n",
       "      <td>539.00</td>\n",
       "      <td>161.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.950</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>505</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>505.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.953</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>920</td>\n",
       "      <td>0.33</td>\n",
       "      <td>532</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>175.56</td>\n",
       "      <td>356.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.916</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>924</td>\n",
       "      <td>0.45</td>\n",
       "      <td>847</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>381.15</td>\n",
       "      <td>465.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.964</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>899</td>\n",
       "      <td>0.54</td>\n",
       "      <td>477</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>257.58</td>\n",
       "      <td>219.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>2014</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>737</td>\n",
       "      <td>0.55</td>\n",
       "      <td>377</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>207.35</td>\n",
       "      <td>169.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.950</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>826</td>\n",
       "      <td>0.75</td>\n",
       "      <td>240</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180.00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.952</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>827</td>\n",
       "      <td>0.46</td>\n",
       "      <td>588</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>270.48</td>\n",
       "      <td>317.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.879</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>855</td>\n",
       "      <td>0.50</td>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45.50</td>\n",
       "      <td>45.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.952</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1018</td>\n",
       "      <td>0.29</td>\n",
       "      <td>603</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>174.87</td>\n",
       "      <td>428.13</td>\n",
       "      <td>174.87</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.958</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>25.0</td>\n",
       "      <td>960</td>\n",
       "      <td>0.39</td>\n",
       "      <td>546</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>212.94</td>\n",
       "      <td>333.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.923</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1022</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1443</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>447.33</td>\n",
       "      <td>995.67</td>\n",
       "      <td>447.33</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.934</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>937</td>\n",
       "      <td>0.35</td>\n",
       "      <td>836</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>292.60</td>\n",
       "      <td>543.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.947</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>18.2</td>\n",
       "      <td>1070</td>\n",
       "      <td>0.56</td>\n",
       "      <td>303</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>169.68</td>\n",
       "      <td>133.32</td>\n",
       "      <td>169.68</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9701</th>\n",
       "      <td>9701</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.965</td>\n",
       "      <td>1245.45</td>\n",
       "      <td>1664.07</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5912.12</td>\n",
       "      <td>0.085</td>\n",
       "      <td>8821.64</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>227</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>227.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9702</th>\n",
       "      <td>9702</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.933</td>\n",
       "      <td>1245.45</td>\n",
       "      <td>1664.07</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5912.12</td>\n",
       "      <td>0.085</td>\n",
       "      <td>8821.64</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>430</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>430.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9703</th>\n",
       "      <td>9703</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.971</td>\n",
       "      <td>1245.45</td>\n",
       "      <td>1664.07</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5912.12</td>\n",
       "      <td>0.085</td>\n",
       "      <td>8821.64</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>381</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>381.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9704</th>\n",
       "      <td>9704</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.981</td>\n",
       "      <td>1245.45</td>\n",
       "      <td>1664.07</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5912.12</td>\n",
       "      <td>0.085</td>\n",
       "      <td>8821.64</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>537</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>537.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9705</th>\n",
       "      <td>9705</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.966</td>\n",
       "      <td>1245.45</td>\n",
       "      <td>1664.07</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5912.12</td>\n",
       "      <td>0.085</td>\n",
       "      <td>8821.64</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>59.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9706</th>\n",
       "      <td>9706</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.983</td>\n",
       "      <td>1245.45</td>\n",
       "      <td>1664.07</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5912.12</td>\n",
       "      <td>0.085</td>\n",
       "      <td>8821.64</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>290</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>290.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9707</th>\n",
       "      <td>9707</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.935</td>\n",
       "      <td>1245.45</td>\n",
       "      <td>1664.07</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5912.12</td>\n",
       "      <td>0.085</td>\n",
       "      <td>8821.64</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>356</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>356.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9708</th>\n",
       "      <td>9708</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.954</td>\n",
       "      <td>1100.08</td>\n",
       "      <td>1655.07</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.054</td>\n",
       "      <td>6828.77</td>\n",
       "      <td>0.060</td>\n",
       "      <td>9583.92</td>\n",
       "      <td>...</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>347</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>347.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9709</th>\n",
       "      <td>9709</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.962</td>\n",
       "      <td>1100.08</td>\n",
       "      <td>1655.07</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.054</td>\n",
       "      <td>6828.77</td>\n",
       "      <td>0.060</td>\n",
       "      <td>9583.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>239</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>239.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9710</th>\n",
       "      <td>9710</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.961</td>\n",
       "      <td>1100.08</td>\n",
       "      <td>1655.07</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.054</td>\n",
       "      <td>6828.77</td>\n",
       "      <td>0.060</td>\n",
       "      <td>9583.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>258</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>258.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9711</th>\n",
       "      <td>9711</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.959</td>\n",
       "      <td>1100.08</td>\n",
       "      <td>1655.07</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.054</td>\n",
       "      <td>6828.77</td>\n",
       "      <td>0.060</td>\n",
       "      <td>9583.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>269</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>269.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9712</th>\n",
       "      <td>9712</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.967</td>\n",
       "      <td>1100.08</td>\n",
       "      <td>1655.07</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.054</td>\n",
       "      <td>6828.77</td>\n",
       "      <td>0.060</td>\n",
       "      <td>9583.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>246</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>246.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9713</th>\n",
       "      <td>9713</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.946</td>\n",
       "      <td>1100.08</td>\n",
       "      <td>1655.07</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.054</td>\n",
       "      <td>6828.77</td>\n",
       "      <td>0.060</td>\n",
       "      <td>9583.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>459</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>459.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9714</th>\n",
       "      <td>9714</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.951</td>\n",
       "      <td>1100.08</td>\n",
       "      <td>1655.07</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.054</td>\n",
       "      <td>6828.77</td>\n",
       "      <td>0.060</td>\n",
       "      <td>9583.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>860</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>860.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9715</th>\n",
       "      <td>9715</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.954</td>\n",
       "      <td>1100.08</td>\n",
       "      <td>1655.07</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.054</td>\n",
       "      <td>6828.77</td>\n",
       "      <td>0.060</td>\n",
       "      <td>9583.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>326</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>326.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9716</th>\n",
       "      <td>9716</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.956</td>\n",
       "      <td>1100.08</td>\n",
       "      <td>1655.07</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.054</td>\n",
       "      <td>6828.77</td>\n",
       "      <td>0.060</td>\n",
       "      <td>9583.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>319</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>319.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9717</th>\n",
       "      <td>9717</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.944</td>\n",
       "      <td>1100.08</td>\n",
       "      <td>1655.07</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.054</td>\n",
       "      <td>6828.77</td>\n",
       "      <td>0.060</td>\n",
       "      <td>9583.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>575</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>575.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9718</th>\n",
       "      <td>9718</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.949</td>\n",
       "      <td>1100.08</td>\n",
       "      <td>1655.07</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.054</td>\n",
       "      <td>6828.77</td>\n",
       "      <td>0.060</td>\n",
       "      <td>9583.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>508</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>508.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9719</th>\n",
       "      <td>9719</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.991</td>\n",
       "      <td>1100.08</td>\n",
       "      <td>1655.07</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.054</td>\n",
       "      <td>6828.77</td>\n",
       "      <td>0.060</td>\n",
       "      <td>9583.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>221</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>221.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9720</th>\n",
       "      <td>9720</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.955</td>\n",
       "      <td>1100.08</td>\n",
       "      <td>1655.07</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.054</td>\n",
       "      <td>6828.77</td>\n",
       "      <td>0.060</td>\n",
       "      <td>9583.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>556</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>556.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9721</th>\n",
       "      <td>9721</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.837</td>\n",
       "      <td>1100.08</td>\n",
       "      <td>1655.07</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.054</td>\n",
       "      <td>6828.77</td>\n",
       "      <td>0.060</td>\n",
       "      <td>9583.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>49.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9722</th>\n",
       "      <td>9722</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.950</td>\n",
       "      <td>1235.54</td>\n",
       "      <td>2119.55</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.073</td>\n",
       "      <td>7463.07</td>\n",
       "      <td>0.060</td>\n",
       "      <td>10818.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>161</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>161.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9723</th>\n",
       "      <td>9723</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.953</td>\n",
       "      <td>1235.54</td>\n",
       "      <td>2119.55</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.073</td>\n",
       "      <td>7463.07</td>\n",
       "      <td>0.060</td>\n",
       "      <td>10818.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>43.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9724</th>\n",
       "      <td>9724</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.939</td>\n",
       "      <td>1235.54</td>\n",
       "      <td>2119.55</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.073</td>\n",
       "      <td>7463.07</td>\n",
       "      <td>0.060</td>\n",
       "      <td>10818.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>328</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>328.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9725</th>\n",
       "      <td>9725</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.942</td>\n",
       "      <td>1235.54</td>\n",
       "      <td>2119.55</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.073</td>\n",
       "      <td>7463.07</td>\n",
       "      <td>0.060</td>\n",
       "      <td>10818.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>241</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>241.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9726</th>\n",
       "      <td>9726</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.942</td>\n",
       "      <td>1235.54</td>\n",
       "      <td>2119.55</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.073</td>\n",
       "      <td>7463.07</td>\n",
       "      <td>0.060</td>\n",
       "      <td>10818.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>120.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9727</th>\n",
       "      <td>9727</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.950</td>\n",
       "      <td>1235.54</td>\n",
       "      <td>2119.55</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.073</td>\n",
       "      <td>7463.07</td>\n",
       "      <td>0.060</td>\n",
       "      <td>10818.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>281</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>281.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9728</th>\n",
       "      <td>9728</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.944</td>\n",
       "      <td>1235.54</td>\n",
       "      <td>2119.55</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.073</td>\n",
       "      <td>7463.07</td>\n",
       "      <td>0.060</td>\n",
       "      <td>10818.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>177</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>177.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9729</th>\n",
       "      <td>9729</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.945</td>\n",
       "      <td>1235.54</td>\n",
       "      <td>2119.55</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.073</td>\n",
       "      <td>7463.07</td>\n",
       "      <td>0.060</td>\n",
       "      <td>10818.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>692</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>692.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9730</th>\n",
       "      <td>9730</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.939</td>\n",
       "      <td>1235.54</td>\n",
       "      <td>2119.55</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.073</td>\n",
       "      <td>7463.07</td>\n",
       "      <td>0.060</td>\n",
       "      <td>10818.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>115.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9731 rows × 273 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Row  Year  avg_daily_attend_pct  lea_federal_perpupil_num  \\\n",
       "0        0  2014                 0.872                   1006.40   \n",
       "1        1  2014                 0.957                   1006.40   \n",
       "2        2  2014                 0.954                   1006.40   \n",
       "3        3  2014                 0.943                   1006.40   \n",
       "4        4  2014                 0.944                   1006.40   \n",
       "5        5  2014                 0.958                   1006.40   \n",
       "6        6  2014                 0.926                   1006.40   \n",
       "7        7  2014                 0.947                   1006.40   \n",
       "8        8  2014                 0.952                   1006.40   \n",
       "9        9  2014                 0.960                   1006.40   \n",
       "10      10  2014                 0.950                   1006.40   \n",
       "11      11  2014                 0.950                   1006.40   \n",
       "12      12  2014                 0.912                   1006.40   \n",
       "13      13  2014                 0.946                   1006.40   \n",
       "14      14  2014                 0.949                   1006.40   \n",
       "15      15  2014                 0.943                   1006.40   \n",
       "16      16  2014                 0.956                   1006.40   \n",
       "17      17  2014                 0.950                   1006.40   \n",
       "18      18  2014                 0.953                   1006.40   \n",
       "19      19  2014                 0.916                   1006.40   \n",
       "20      20  2014                 0.964                   1006.40   \n",
       "21      21  2014                 1.000                   1006.40   \n",
       "22      22  2014                 0.950                   1006.40   \n",
       "23      23  2014                 0.952                   1006.40   \n",
       "24      24  2014                 0.879                   1006.40   \n",
       "25      25  2014                 0.952                   1006.40   \n",
       "26      26  2014                 0.958                   1006.40   \n",
       "27      27  2014                 0.923                   1006.40   \n",
       "28      28  2014                 0.934                   1006.40   \n",
       "29      29  2014                 0.947                   1006.40   \n",
       "...    ...   ...                   ...                       ...   \n",
       "9701  9701  2017                 0.965                   1245.45   \n",
       "9702  9702  2017                 0.933                   1245.45   \n",
       "9703  9703  2017                 0.971                   1245.45   \n",
       "9704  9704  2017                 0.981                   1245.45   \n",
       "9705  9705  2017                 0.966                   1245.45   \n",
       "9706  9706  2017                 0.983                   1245.45   \n",
       "9707  9707  2017                 0.935                   1245.45   \n",
       "9708  9708  2017                 0.954                   1100.08   \n",
       "9709  9709  2017                 0.962                   1100.08   \n",
       "9710  9710  2017                 0.961                   1100.08   \n",
       "9711  9711  2017                 0.959                   1100.08   \n",
       "9712  9712  2017                 0.967                   1100.08   \n",
       "9713  9713  2017                 0.946                   1100.08   \n",
       "9714  9714  2017                 0.951                   1100.08   \n",
       "9715  9715  2017                 0.954                   1100.08   \n",
       "9716  9716  2017                 0.956                   1100.08   \n",
       "9717  9717  2017                 0.944                   1100.08   \n",
       "9718  9718  2017                 0.949                   1100.08   \n",
       "9719  9719  2017                 0.991                   1100.08   \n",
       "9720  9720  2017                 0.955                   1100.08   \n",
       "9721  9721  2017                 0.837                   1100.08   \n",
       "9722  9722  2017                 0.950                   1235.54   \n",
       "9723  9723  2017                 0.953                   1235.54   \n",
       "9724  9724  2017                 0.939                   1235.54   \n",
       "9725  9725  2017                 0.942                   1235.54   \n",
       "9726  9726  2017                 0.942                   1235.54   \n",
       "9727  9727  2017                 0.950                   1235.54   \n",
       "9728  9728  2017                 0.944                   1235.54   \n",
       "9729  9729  2017                 0.945                   1235.54   \n",
       "9730  9730  2017                 0.939                   1235.54   \n",
       "\n",
       "      lea_local_perpupil_num  lea_salary_expense_pct  \\\n",
       "0                    1845.78                   0.613   \n",
       "1                    1845.78                   0.613   \n",
       "2                    1845.78                   0.613   \n",
       "3                    1845.78                   0.613   \n",
       "4                    1845.78                   0.613   \n",
       "5                    1845.78                   0.613   \n",
       "6                    1845.78                   0.613   \n",
       "7                    1845.78                   0.613   \n",
       "8                    1845.78                   0.613   \n",
       "9                    1845.78                   0.613   \n",
       "10                   1845.78                   0.613   \n",
       "11                   1845.78                   0.613   \n",
       "12                   1845.78                   0.613   \n",
       "13                   1845.78                   0.613   \n",
       "14                   1845.78                   0.613   \n",
       "15                   1845.78                   0.613   \n",
       "16                   1845.78                   0.613   \n",
       "17                   1845.78                   0.613   \n",
       "18                   1845.78                   0.613   \n",
       "19                   1845.78                   0.613   \n",
       "20                   1845.78                   0.613   \n",
       "21                   1845.78                   0.613   \n",
       "22                   1845.78                   0.613   \n",
       "23                   1845.78                   0.613   \n",
       "24                   1845.78                   0.613   \n",
       "25                   1845.78                   0.613   \n",
       "26                   1845.78                   0.613   \n",
       "27                   1845.78                   0.613   \n",
       "28                   1845.78                   0.613   \n",
       "29                   1845.78                   0.613   \n",
       "...                      ...                     ...   \n",
       "9701                 1664.07                   0.828   \n",
       "9702                 1664.07                   0.828   \n",
       "9703                 1664.07                   0.828   \n",
       "9704                 1664.07                   0.828   \n",
       "9705                 1664.07                   0.828   \n",
       "9706                 1664.07                   0.828   \n",
       "9707                 1664.07                   0.828   \n",
       "9708                 1655.07                   0.874   \n",
       "9709                 1655.07                   0.874   \n",
       "9710                 1655.07                   0.874   \n",
       "9711                 1655.07                   0.874   \n",
       "9712                 1655.07                   0.874   \n",
       "9713                 1655.07                   0.874   \n",
       "9714                 1655.07                   0.874   \n",
       "9715                 1655.07                   0.874   \n",
       "9716                 1655.07                   0.874   \n",
       "9717                 1655.07                   0.874   \n",
       "9718                 1655.07                   0.874   \n",
       "9719                 1655.07                   0.874   \n",
       "9720                 1655.07                   0.874   \n",
       "9721                 1655.07                   0.874   \n",
       "9722                 2119.55                   0.860   \n",
       "9723                 2119.55                   0.860   \n",
       "9724                 2119.55                   0.860   \n",
       "9725                 2119.55                   0.860   \n",
       "9726                 2119.55                   0.860   \n",
       "9727                 2119.55                   0.860   \n",
       "9728                 2119.55                   0.860   \n",
       "9729                 2119.55                   0.860   \n",
       "9730                 2119.55                   0.860   \n",
       "\n",
       "      lea_services_expense_pct  lea_state_perpupil_num  \\\n",
       "0                        0.078                 5176.41   \n",
       "1                        0.078                 5176.41   \n",
       "2                        0.078                 5176.41   \n",
       "3                        0.078                 5176.41   \n",
       "4                        0.078                 5176.41   \n",
       "5                        0.078                 5176.41   \n",
       "6                        0.078                 5176.41   \n",
       "7                        0.078                 5176.41   \n",
       "8                        0.078                 5176.41   \n",
       "9                        0.078                 5176.41   \n",
       "10                       0.078                 5176.41   \n",
       "11                       0.078                 5176.41   \n",
       "12                       0.078                 5176.41   \n",
       "13                       0.078                 5176.41   \n",
       "14                       0.078                 5176.41   \n",
       "15                       0.078                 5176.41   \n",
       "16                       0.078                 5176.41   \n",
       "17                       0.078                 5176.41   \n",
       "18                       0.078                 5176.41   \n",
       "19                       0.078                 5176.41   \n",
       "20                       0.078                 5176.41   \n",
       "21                       0.078                 5176.41   \n",
       "22                       0.078                 5176.41   \n",
       "23                       0.078                 5176.41   \n",
       "24                       0.078                 5176.41   \n",
       "25                       0.078                 5176.41   \n",
       "26                       0.078                 5176.41   \n",
       "27                       0.078                 5176.41   \n",
       "28                       0.078                 5176.41   \n",
       "29                       0.078                 5176.41   \n",
       "...                        ...                     ...   \n",
       "9701                     0.078                 5912.12   \n",
       "9702                     0.078                 5912.12   \n",
       "9703                     0.078                 5912.12   \n",
       "9704                     0.078                 5912.12   \n",
       "9705                     0.078                 5912.12   \n",
       "9706                     0.078                 5912.12   \n",
       "9707                     0.078                 5912.12   \n",
       "9708                     0.054                 6828.77   \n",
       "9709                     0.054                 6828.77   \n",
       "9710                     0.054                 6828.77   \n",
       "9711                     0.054                 6828.77   \n",
       "9712                     0.054                 6828.77   \n",
       "9713                     0.054                 6828.77   \n",
       "9714                     0.054                 6828.77   \n",
       "9715                     0.054                 6828.77   \n",
       "9716                     0.054                 6828.77   \n",
       "9717                     0.054                 6828.77   \n",
       "9718                     0.054                 6828.77   \n",
       "9719                     0.054                 6828.77   \n",
       "9720                     0.054                 6828.77   \n",
       "9721                     0.054                 6828.77   \n",
       "9722                     0.073                 7463.07   \n",
       "9723                     0.073                 7463.07   \n",
       "9724                     0.073                 7463.07   \n",
       "9725                     0.073                 7463.07   \n",
       "9726                     0.073                 7463.07   \n",
       "9727                     0.073                 7463.07   \n",
       "9728                     0.073                 7463.07   \n",
       "9729                     0.073                 7463.07   \n",
       "9730                     0.073                 7463.07   \n",
       "\n",
       "      lea_supplies_expense_pct  lea_total_expense_num      ...       \\\n",
       "0                        0.086                8028.59      ...        \n",
       "1                        0.086                8028.59      ...        \n",
       "2                        0.086                8028.59      ...        \n",
       "3                        0.086                8028.59      ...        \n",
       "4                        0.086                8028.59      ...        \n",
       "5                        0.086                8028.59      ...        \n",
       "6                        0.086                8028.59      ...        \n",
       "7                        0.086                8028.59      ...        \n",
       "8                        0.086                8028.59      ...        \n",
       "9                        0.086                8028.59      ...        \n",
       "10                       0.086                8028.59      ...        \n",
       "11                       0.086                8028.59      ...        \n",
       "12                       0.086                8028.59      ...        \n",
       "13                       0.086                8028.59      ...        \n",
       "14                       0.086                8028.59      ...        \n",
       "15                       0.086                8028.59      ...        \n",
       "16                       0.086                8028.59      ...        \n",
       "17                       0.086                8028.59      ...        \n",
       "18                       0.086                8028.59      ...        \n",
       "19                       0.086                8028.59      ...        \n",
       "20                       0.086                8028.59      ...        \n",
       "21                       0.086                8028.59      ...        \n",
       "22                       0.086                8028.59      ...        \n",
       "23                       0.086                8028.59      ...        \n",
       "24                       0.086                8028.59      ...        \n",
       "25                       0.086                8028.59      ...        \n",
       "26                       0.086                8028.59      ...        \n",
       "27                       0.086                8028.59      ...        \n",
       "28                       0.086                8028.59      ...        \n",
       "29                       0.086                8028.59      ...        \n",
       "...                        ...                    ...      ...        \n",
       "9701                     0.085                8821.64      ...        \n",
       "9702                     0.085                8821.64      ...        \n",
       "9703                     0.085                8821.64      ...        \n",
       "9704                     0.085                8821.64      ...        \n",
       "9705                     0.085                8821.64      ...        \n",
       "9706                     0.085                8821.64      ...        \n",
       "9707                     0.085                8821.64      ...        \n",
       "9708                     0.060                9583.92      ...        \n",
       "9709                     0.060                9583.92      ...        \n",
       "9710                     0.060                9583.92      ...        \n",
       "9711                     0.060                9583.92      ...        \n",
       "9712                     0.060                9583.92      ...        \n",
       "9713                     0.060                9583.92      ...        \n",
       "9714                     0.060                9583.92      ...        \n",
       "9715                     0.060                9583.92      ...        \n",
       "9716                     0.060                9583.92      ...        \n",
       "9717                     0.060                9583.92      ...        \n",
       "9718                     0.060                9583.92      ...        \n",
       "9719                     0.060                9583.92      ...        \n",
       "9720                     0.060                9583.92      ...        \n",
       "9721                     0.060                9583.92      ...        \n",
       "9722                     0.060               10818.16      ...        \n",
       "9723                     0.060               10818.16      ...        \n",
       "9724                     0.060               10818.16      ...        \n",
       "9725                     0.060               10818.16      ...        \n",
       "9726                     0.060               10818.16      ...        \n",
       "9727                     0.060               10818.16      ...        \n",
       "9728                     0.060               10818.16      ...        \n",
       "9729                     0.060               10818.16      ...        \n",
       "9730                     0.060               10818.16      ...        \n",
       "\n",
       "      EOGReadingGr4_CACR_SWD  sat_avg_score_num  sat_participation_pct  \\\n",
       "0                        0.0                925                   0.12   \n",
       "1                        0.0                979                   0.63   \n",
       "2                       30.0                884                   0.70   \n",
       "3                        0.0                824                   0.45   \n",
       "4                       14.3                962                   0.55   \n",
       "5                        0.0               1024                   0.60   \n",
       "6                        0.0               1008                   0.64   \n",
       "7                        0.0                983                   0.32   \n",
       "8                       10.0                959                   0.42   \n",
       "9                        6.3                883                   0.62   \n",
       "10                      12.5                805                   0.49   \n",
       "11                      10.0                798                   0.87   \n",
       "12                       0.0                997                   0.50   \n",
       "13                       0.0                969                   0.23   \n",
       "14                      12.5                949                   0.33   \n",
       "15                       0.0                  0                   0.00   \n",
       "16                       0.0               1024                   0.23   \n",
       "17                       0.0                  0                   0.00   \n",
       "18                      20.0                920                   0.33   \n",
       "19                       0.0                924                   0.45   \n",
       "20                       0.0                899                   0.54   \n",
       "21                      20.0                737                   0.55   \n",
       "22                       0.0                826                   0.75   \n",
       "23                       0.0                827                   0.46   \n",
       "24                       0.0                855                   0.50   \n",
       "25                       0.0               1018                   0.29   \n",
       "26                      25.0                960                   0.39   \n",
       "27                       0.0               1022                   0.31   \n",
       "28                       0.0                937                   0.35   \n",
       "29                      18.2               1070                   0.56   \n",
       "...                      ...                ...                    ...   \n",
       "9701                     0.0                  0                   0.00   \n",
       "9702                     0.0                  0                   0.00   \n",
       "9703                     0.0                  0                   0.00   \n",
       "9704                     0.0                  0                   0.00   \n",
       "9705                     0.0                  0                   0.00   \n",
       "9706                     0.0                  0                   0.00   \n",
       "9707                     0.0                  0                   0.00   \n",
       "9708                    25.0                  0                   0.00   \n",
       "9709                     0.0                  0                   0.00   \n",
       "9710                     0.0                  0                   0.00   \n",
       "9711                     0.0                  0                   0.00   \n",
       "9712                     0.0                  0                   0.00   \n",
       "9713                     0.0                  0                   0.00   \n",
       "9714                     0.0                  0                   0.00   \n",
       "9715                     0.0                  0                   0.00   \n",
       "9716                     0.0                  0                   0.00   \n",
       "9717                     0.0                  0                   0.00   \n",
       "9718                     0.0                  0                   0.00   \n",
       "9719                     0.0                  0                   0.00   \n",
       "9720                     0.0                  0                   0.00   \n",
       "9721                     0.0                  0                   0.00   \n",
       "9722                     0.0                  0                   0.00   \n",
       "9723                     0.0                  0                   0.00   \n",
       "9724                     0.0                  0                   0.00   \n",
       "9725                     0.0                  0                   0.00   \n",
       "9726                     0.0                  0                   0.00   \n",
       "9727                     0.0                  0                   0.00   \n",
       "9728                     0.0                  0                   0.00   \n",
       "9729                     0.0                  0                   0.00   \n",
       "9730                     0.0                  0                   0.00   \n",
       "\n",
       "      student_num  SAT_SCORE_ZERO  SAT_Score_above1000  \\\n",
       "0              78               0                    0   \n",
       "1             539               0                    0   \n",
       "2             547               0                    0   \n",
       "3             800               0                    0   \n",
       "4             664               0                    0   \n",
       "5             330               0                    1   \n",
       "6            1192               0                    1   \n",
       "7             492               0                    0   \n",
       "8             628               0                    0   \n",
       "9             621               0                    0   \n",
       "10            419               0                    0   \n",
       "11            674               0                    0   \n",
       "12            809               0                    0   \n",
       "13            668               0                    0   \n",
       "14            574               0                    0   \n",
       "15            700               1                    0   \n",
       "16            700               0                    1   \n",
       "17            505               1                    0   \n",
       "18            532               0                    0   \n",
       "19            847               0                    0   \n",
       "20            477               0                    0   \n",
       "21            377               0                    0   \n",
       "22            240               0                    0   \n",
       "23            588               0                    0   \n",
       "24             91               0                    0   \n",
       "25            603               0                    1   \n",
       "26            546               0                    0   \n",
       "27           1443               0                    1   \n",
       "28            836               0                    0   \n",
       "29            303               0                    1   \n",
       "...           ...             ...                  ...   \n",
       "9701          227               1                    0   \n",
       "9702          430               1                    0   \n",
       "9703          381               1                    0   \n",
       "9704          537               1                    0   \n",
       "9705           59               1                    0   \n",
       "9706          290               1                    0   \n",
       "9707          356               1                    0   \n",
       "9708          347               1                    0   \n",
       "9709          239               1                    0   \n",
       "9710          258               1                    0   \n",
       "9711          269               1                    0   \n",
       "9712          246               1                    0   \n",
       "9713          459               1                    0   \n",
       "9714          860               1                    0   \n",
       "9715          326               1                    0   \n",
       "9716          319               1                    0   \n",
       "9717          575               1                    0   \n",
       "9718          508               1                    0   \n",
       "9719          221               1                    0   \n",
       "9720          556               1                    0   \n",
       "9721           49               1                    0   \n",
       "9722          161               1                    0   \n",
       "9723           43               1                    0   \n",
       "9724          328               1                    0   \n",
       "9725          241               1                    0   \n",
       "9726          120               1                    0   \n",
       "9727          281               1                    0   \n",
       "9728          177               1                    0   \n",
       "9729          692               1                    0   \n",
       "9730          115               1                    0   \n",
       "\n",
       "      SAT_participation_number  EST_Student_College_NO  \\\n",
       "0                         9.36                   68.64   \n",
       "1                       339.57                  199.43   \n",
       "2                       382.90                  164.10   \n",
       "3                       360.00                  440.00   \n",
       "4                       365.20                  298.80   \n",
       "5                       198.00                  132.00   \n",
       "6                       762.88                  429.12   \n",
       "7                       157.44                  334.56   \n",
       "8                       263.76                  364.24   \n",
       "9                       385.02                  235.98   \n",
       "10                      205.31                  213.69   \n",
       "11                      586.38                   87.62   \n",
       "12                      404.50                  404.50   \n",
       "13                      153.64                  514.36   \n",
       "14                      189.42                  384.58   \n",
       "15                        0.00                  700.00   \n",
       "16                      161.00                  539.00   \n",
       "17                        0.00                  505.00   \n",
       "18                      175.56                  356.44   \n",
       "19                      381.15                  465.85   \n",
       "20                      257.58                  219.42   \n",
       "21                      207.35                  169.65   \n",
       "22                      180.00                   60.00   \n",
       "23                      270.48                  317.52   \n",
       "24                       45.50                   45.50   \n",
       "25                      174.87                  428.13   \n",
       "26                      212.94                  333.06   \n",
       "27                      447.33                  995.67   \n",
       "28                      292.60                  543.40   \n",
       "29                      169.68                  133.32   \n",
       "...                        ...                     ...   \n",
       "9701                      0.00                  227.00   \n",
       "9702                      0.00                  430.00   \n",
       "9703                      0.00                  381.00   \n",
       "9704                      0.00                  537.00   \n",
       "9705                      0.00                   59.00   \n",
       "9706                      0.00                  290.00   \n",
       "9707                      0.00                  356.00   \n",
       "9708                      0.00                  347.00   \n",
       "9709                      0.00                  239.00   \n",
       "9710                      0.00                  258.00   \n",
       "9711                      0.00                  269.00   \n",
       "9712                      0.00                  246.00   \n",
       "9713                      0.00                  459.00   \n",
       "9714                      0.00                  860.00   \n",
       "9715                      0.00                  326.00   \n",
       "9716                      0.00                  319.00   \n",
       "9717                      0.00                  575.00   \n",
       "9718                      0.00                  508.00   \n",
       "9719                      0.00                  221.00   \n",
       "9720                      0.00                  556.00   \n",
       "9721                      0.00                   49.00   \n",
       "9722                      0.00                  161.00   \n",
       "9723                      0.00                   43.00   \n",
       "9724                      0.00                  328.00   \n",
       "9725                      0.00                  241.00   \n",
       "9726                      0.00                  120.00   \n",
       "9727                      0.00                  281.00   \n",
       "9728                      0.00                  177.00   \n",
       "9729                      0.00                  692.00   \n",
       "9730                      0.00                  115.00   \n",
       "\n",
       "      Student_Num_College_Ready_SAT  Unnamed: 272  \n",
       "0                              0.00           NaN  \n",
       "1                              0.00           NaN  \n",
       "2                              0.00           NaN  \n",
       "3                              0.00           NaN  \n",
       "4                              0.00           NaN  \n",
       "5                            198.00           NaN  \n",
       "6                            762.88           NaN  \n",
       "7                              0.00           NaN  \n",
       "8                              0.00           NaN  \n",
       "9                              0.00           NaN  \n",
       "10                             0.00           NaN  \n",
       "11                             0.00           NaN  \n",
       "12                             0.00           NaN  \n",
       "13                             0.00           NaN  \n",
       "14                             0.00           NaN  \n",
       "15                             0.00           NaN  \n",
       "16                           161.00           NaN  \n",
       "17                             0.00           NaN  \n",
       "18                             0.00           NaN  \n",
       "19                             0.00           NaN  \n",
       "20                             0.00           NaN  \n",
       "21                             0.00           NaN  \n",
       "22                             0.00           NaN  \n",
       "23                             0.00           NaN  \n",
       "24                             0.00           NaN  \n",
       "25                           174.87           NaN  \n",
       "26                             0.00           NaN  \n",
       "27                           447.33           NaN  \n",
       "28                             0.00           NaN  \n",
       "29                           169.68           NaN  \n",
       "...                             ...           ...  \n",
       "9701                           0.00           NaN  \n",
       "9702                           0.00           NaN  \n",
       "9703                           0.00           NaN  \n",
       "9704                           0.00           NaN  \n",
       "9705                           0.00           NaN  \n",
       "9706                           0.00           NaN  \n",
       "9707                           0.00           NaN  \n",
       "9708                           0.00           NaN  \n",
       "9709                           0.00           NaN  \n",
       "9710                           0.00           NaN  \n",
       "9711                           0.00           NaN  \n",
       "9712                           0.00           NaN  \n",
       "9713                           0.00           NaN  \n",
       "9714                           0.00           NaN  \n",
       "9715                           0.00           NaN  \n",
       "9716                           0.00           NaN  \n",
       "9717                           0.00           NaN  \n",
       "9718                           0.00           NaN  \n",
       "9719                           0.00           NaN  \n",
       "9720                           0.00           NaN  \n",
       "9721                           0.00           NaN  \n",
       "9722                           0.00           NaN  \n",
       "9723                           0.00           NaN  \n",
       "9724                           0.00           NaN  \n",
       "9725                           0.00           NaN  \n",
       "9726                           0.00           NaN  \n",
       "9727                           0.00           NaN  \n",
       "9728                           0.00           NaN  \n",
       "9729                           0.00           NaN  \n",
       "9730                           0.00           1.0  \n",
       "\n",
       "[9731 rows x 273 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#ed = imputed_school_data.copy()\n",
    "\n",
    "#ed.to_csv('C:\\\\Users\\\\Bahr\\\\Desktop\\\\SMU_DS_Summer_2018\\\\Data_Mining\\\\Assignments\\\\Lab TWO\\\\MSDS7331_NorthCarolinaDataset\\\\Lab2\\\\RAW.csv')\n",
    "\n",
    "#ED is processed using Alteryx \n",
    "\n",
    "\n",
    "#reading in new dataset. \n",
    "SAT_Filtered = pd.DataFrame()\n",
    "SAT_Filtered = pd.read_csv('../Data/School Datasets/Alteryx_Product.csv', low_memory=False)\n",
    "\n",
    "\n",
    "SAT_Filtered\n",
    "\n",
    "#SAT_SCORE_ZERO\tBoolean: If SAT equals zero. Used it to test if anyone participated but produced zero scores. \n",
    "#SAT_Score_above1000\t Boolean: SAT score above 1000 or not to get into a decent college in NC. \n",
    "#SAT_participation_number\t Estimate of number of students that took the SAT\n",
    "#EST_Student_College_NO\t Estimate of the number of students who didn't attempt the SAT\n",
    "#Student_Num_College_Ready_SAT\t Estimate of the number of students that can apply to college without SAT score hinderance from that school. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get list of highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_matrix = imputed_school_data.corr().abs()\n",
    "\n",
    "# # Select upper triangle of correlation matrix\n",
    "# upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# # Find index of feature columns with correlation greater than 0.95\n",
    "# to_drop = [column for column in upper.columns if any(upper[column] > 0.99)]\n",
    "\n",
    "# print(to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.a Modeling and Evaluation\n",
    "Using the right evaluation metric for classification system is crucial. Otherwise, it may results in thinking that the model is performing well but in reality, it doesn’t.\n",
    "\n",
    "There are two tasks in this section of “NC Educational Data” project:\n",
    "\n",
    "The first task is to predict a binary classification target, either if the average SAT score of each school is good enough to gets the student to the North Carolina Universities or not. The SAT is a standardized test widely used for college admissions in the United States. For this purpose we have a cut off 1200 out of 1600. The second task is to predict if the crime per 100 students at each school level is higher than the LEA level or not. After considering all evaluation metrics for classification systems, we ended up using ROC Curve. Area under ROC Curve (or AUC for short) is a performance metric for binary classification problems.\n",
    "\n",
    "In fact, a ROC curve can be used to select a threshold for a classifier which maximizes the true positives, while minimizing the false positives.\n",
    "\n",
    "We usually use ROC when both classes detection are important. Here, our models should be able to decrease both false positive rate (which is identifying the schools with enough good average SAT score for getting admission in different universities) and also decreasing the false negative rate (which is detecting schools with not good average SAT scores).\n",
    "\n",
    "The same for the second task, it is important to decrease both false positive and false negative rates.\n",
    "\n",
    "The AUC represents a model’s ability to discriminate between positive and negative classes. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model as good as random. Most classifiers have AUCs that fall somewhere between these two values. Therefore, the overall model performances can be compared by considering the AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.b Modeling and Evaluation\n",
    "10 points - Choose the method you will use for dividing your data into training and why testing splits (i.e., are you using Stratiﬁed 10-fold cross validation? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n"
     ]
    }
   ],
   "source": [
    "# split X and y into test and train sets. We still want\n",
    "# to do this for external Cross Validation\n",
    "\n",
    "crime_imputed_school_data = imputed_school_data\n",
    "\n",
    "crime_imputed_school_data['local_crime_greater'] = crime_imputed_school_data.apply(lambda each_row: 1 if (each_row['crime_per_c_num']-each_row['lea_crime_per_c_num'])<0 else 0,axis=1)\n",
    "\n",
    "#split data into X and y dataframes\n",
    "\n",
    "y_crime = crime_imputed_school_data['local_crime_greater']\n",
    "\n",
    "#Removed SPG Grade and unit code(which is primary key for school data table)\n",
    " \n",
    "X_crime = imputed_school_data[school_data.columns.drop(list(school_data.filter(regex='crime|unit_code|lea|LEA|^st\\_')))]\n",
    "\n",
    "X_crime_train, X_crime_test, y_crime_train, y_crime_test = train_test_split(X_crime, y_crime, test_size=.2)\n",
    "\n",
    "print (crime_imputed_school_data['local_crime_greater'].dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.c Modeling and Evaluation\n",
    "20 points - Create three different classification/regression models (e.g., random forest, KNN, and SVM). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = KFold(n_splits=10,shuffle=True)\n",
    "\n",
    "#This creates the template for the pipeline\n",
    "# This creates a basic pipeline where we will \n",
    "# test for dementionality reduction, scaling,\n",
    "# and classification.\n",
    "\n",
    "\n",
    "pipe = Pipeline([ ('reduce_dim',SelectKBest(chi2)),\n",
    "                  ('scale', StandardScaler()), \n",
    "                  ('clf', GradientBoostingRegressor())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 36 candidates, totalling 360 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a31089d94293>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# # Here we are training the model, this is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# # what takes the most amount of time to run\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mcrime_GradientBoost_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_crime_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_crime_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\users\\mtool\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[1;32m--> 639\u001b[1;33m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[1;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\users\\mtool\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    790\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\users\\mtool\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    697\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\users\\mtool\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 638\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    639\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\users\\mtool\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\users\\mtool\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\users\\mtool\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " param_grid = [\n",
    "    {\n",
    "         'reduce_dim__k': N_FEATURES_OPTIONS,\n",
    "         'clf__n_estimators': C_ESTIMATORS, \n",
    "         'clf__max_depth': C_DEPTH,\n",
    "     }\n",
    "]\n",
    "\n",
    "\n",
    "# # This will test the parameter dict against our \n",
    "# # pipeline\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid=param_grid, cv=k_fold,n_jobs=-1, verbose=1 )\n",
    "\n",
    "\n",
    "# # Here we are training the model, this is \n",
    "# # what takes the most amount of time to run\n",
    "crime_GradientBoost_model = grid_search.fit(X_crime_train, y_crime_train)\n",
    "\n",
    "\n",
    "y_crime_score = grid_search.predict(X_crime_test)\n",
    "\n",
    "print(roc_auc_score(y_crime_test, y_crime_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.set_params(**crime_GradientBoost_model.best_params_)\n",
    "pipe.fit(X_crime_train, y_crime_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = pipe.steps[2][1].feature_importances_\n",
    "\n",
    "mask = pipe.steps[0][1].get_support()\n",
    "new_features=[]\n",
    "feature_names=list(X_crime_train.columns.values)\n",
    "for bool, feature in zip(mask, feature_names):\n",
    "    if bool:\n",
    "        new_features.append(feature)\n",
    "\n",
    "#Creates a new dataframe with the coefficients and the \n",
    "predicted_data = pd.DataFrame(data=coef,index=new_features,columns=['Influence'])\n",
    "print(\"The top 20 features that influence SPG Grade are the following\")\n",
    "\n",
    "\n",
    "\n",
    "display(predicted_data.sort_values(by='Influence', ascending=False)[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " param_grid = [\n",
    "    {\n",
    "         'reduce_dim__k': N_FEATURES_OPTIONS,\n",
    "         'clf': [AdaBoostClassifier()],\n",
    "         'clf__n_estimators': C_ESTIMATORS\n",
    "\n",
    "     }\n",
    "]\n",
    "\n",
    "\n",
    "# # This will test the parameter dict against our \n",
    "# # pipeline\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid=param_grid, cv=k_fold,n_jobs=-1, verbose=1 )\n",
    "\n",
    "\n",
    "# # Here we are training the model, this is \n",
    "# # what takes the most amount of time to run\n",
    "crime_ADABoost_model = grid_search.fit(X_crime_train, y_crime_train)\n",
    "\n",
    "y_crime_score = grid_search.predict(X_crime_test)\n",
    "\n",
    "print(roc_auc_score(y_crime_test, y_crime_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(crime_ADABoost_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " param_grid = [\n",
    "    {\n",
    "         'reduce_dim__k': N_FEATURES_OPTIONS,\n",
    "         'clf': [RandomForestClassifier()],\n",
    "         'clf__n_estimators': C_ESTIMATORS, \n",
    "         'clf__max_depth': C_DEPTH,\n",
    "     }\n",
    "]\n",
    "\n",
    "\n",
    "# # This will test the parameter dict against our \n",
    "# # pipeline\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid=param_grid, cv=k_fold,n_jobs=-1, verbose=1 )\n",
    "\n",
    "\n",
    "# # Here we are training the model, this is \n",
    "# # what takes the most amount of time to run\n",
    "crime_RandomForest_model = grid_search.fit(X_crime_train, y_crime_train)\n",
    "\n",
    "y_crime_score = grid_search.predict(X_crime_test)\n",
    "\n",
    "print(roc_auc_score(y_crime_test, y_crime_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(crime_RandomForest_model.best_params_)\n",
    "print(crime_RandomForest_model.multimetric_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_NEIGHBORS = 5\n",
    "param_grid = [\n",
    "    {\n",
    "         'reduce_dim__k': N_FEATURES_OPTIONS,\n",
    "         'clf': [KNeighborsClassifier()],\n",
    "         'clf__n_neighbors': C_NEIGHBORS, \n",
    "     }\n",
    "]\n",
    "\n",
    "\n",
    "# # This will test the parameter dict against our \n",
    "# # pipeline\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid=param_grid, cv=k_fold,n_jobs=-1, verbose=1 )\n",
    "\n",
    "\n",
    "# # Here we are training the model, this is \n",
    "# # what takes the most amount of time to run\n",
    "crime_KNearest_model = grid_search.fit(X_crime_train, y_crime_train)\n",
    "\n",
    "y_crime_score = grid_search.predict(X_crime_test)\n",
    "\n",
    "print(roc_auc_score(y_crime_test, y_crime_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(crime_KNearest_model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduced scope model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_crime_reduced = X_crime[X_crime.columns.drop(list(X_crime.filter(regex='[Aa]sian|[Hh]ispanic|[Rr]ace|[Bb]lack|[Mm]inority|[Tw]wo[Oo]r[Mm]ore|[Ii]ndian|[Ww]hite')))]\n",
    "X_crime_reduced_train, X_crime_reduced_test, y_crime_reduced_train, y_crime_reduced_test = train_test_split(X_crime_reduced, y_crime, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_crime_reduced.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " param_grid = [\n",
    "    {\n",
    "         'reduce_dim__k': N_FEATURES_OPTIONS,\n",
    "         'clf__n_estimators': C_ESTIMATORS, \n",
    "         'clf__max_depth': C_DEPTH,\n",
    "     }\n",
    "]\n",
    "\n",
    "\n",
    "# # This will test the parameter dict against our \n",
    "# # pipeline\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid=param_grid, cv=k_fold,n_jobs=-1, verbose=1 )\n",
    "\n",
    "\n",
    "# # Here we are training the model, this is \n",
    "# # what takes the most amount of time to run\n",
    "crime_reduced_GradientBoost_model = grid_search.fit(X_crime_reduced_train, y_crime_reduced_train)\n",
    "\n",
    "\n",
    "y_crime_reduced_score = grid_search.predict(X_crime_reduced_test)\n",
    "\n",
    "print(roc_auc_score(y_crime_reduced_test, y_crime_reduced_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.set_params(**crime_reduced_GradientBoost_model.best_params_)\n",
    "pipe.fit(X_crime_reduced_train, y_crime_reduced_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = pipe.steps[2][1].feature_importances_\n",
    "\n",
    "mask = pipe.steps[0][1].get_support()\n",
    "new_features=[]\n",
    "feature_names=list(X_crime_reduced_train.columns.values)\n",
    "for bool, feature in zip(mask, feature_names):\n",
    "    if bool:\n",
    "        new_features.append(feature)\n",
    "\n",
    "#Creates a new dataframe with the coefficients and the \n",
    "predicted_data = pd.DataFrame(data=coef,index=new_features,columns=['Influence'])\n",
    "print(\"The top 20 features that influence SPG Grade are the following\")\n",
    "\n",
    "\n",
    "\n",
    "display(predicted_data.sort_values(by='Influence', ascending=False)[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.d Modeling and Evaluation\n",
    "10 points - Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Run this to load the model from the save file\n",
    "\n",
    "# from sklearn.externals import joblib\n",
    "# grid_search = joblib.load('savedBestModel.pkl')\n",
    "\n",
    "\n",
    "# # Loads all parameters run into a dict \n",
    "\n",
    "# params = np.array(grid_search.cv_results_['params'])\n",
    "\n",
    "\n",
    "# # Loads all mean test scores into an array\n",
    "\n",
    "# mean_scores = np.array(grid_search.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assigns all models to an array\n",
    "\n",
    "# classifier_labels=['SVC','LogisticRegression','SGDClassifier']\n",
    "\n",
    "\n",
    "# # Creates an empty dataframe that is to be\n",
    "# # filled with the mean test accuracy by C global\n",
    "# # variable and the different classifiers\n",
    "\n",
    "# classifier_temp = pd.DataFrame(columns=classifier_labels,index=C_OPTIONS,\n",
    "#                                data=np.linspace(.1,.25,num=len(C_OPTIONS)*len(classifier_labels)).reshape(len(C_OPTIONS),len(classifier_labels)))\n",
    "# classifier_temp.fillna(0,inplace=True)\n",
    "\n",
    "# for i, (param, score) in enumerate(zip(params, mean_scores)):\n",
    "#     C = param['clf__C'] if 'clf__C' in param else param['clf__alpha']\n",
    "#     class_state = str(param['clf']).split('(')[0]\n",
    "#     if classifier_temp.at[C,class_state] < score:\n",
    "#         classifier_temp.at[C,class_state] = score\n",
    "\n",
    "\n",
    "# # Printing a grid of the best accuracies\n",
    "        \n",
    "# display(classifier_temp.transpose())   \n",
    "\n",
    "\n",
    "# # Print a line plot which shows the best \n",
    "# # accuracies\n",
    " \n",
    "# classifier_temp.plot(logx=True,ylim=(0,1),figsize=(14,10),title='Accuracy by Classifier'); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assigns all reduction models to an array\n",
    "\n",
    "# reduce_labels=['NMF','PCA','SelectKBest']\n",
    "\n",
    "\n",
    "# # Translates the N Features array\n",
    "# # to an array full of string\n",
    "\n",
    "# temp_N_FEATURES_OPTIONS = [str(r) for r in N_FEATURES_OPTIONS]\n",
    "# temp_N_FEATURES_OPTIONS=temp_N_FEATURES_OPTIONS+['None']\n",
    "\n",
    "\n",
    "# # Creates an empty dataframe that is to be\n",
    "# # filled with the mean test accuracy by N Features\n",
    "# # variable and the different feature reduction models\n",
    "\n",
    "# reduce_temp = pd.DataFrame(columns=reduce_labels,index=temp_N_FEATURES_OPTIONS,\n",
    "#                                data=np.linspace(.1,.25,num=len(temp_N_FEATURES_OPTIONS)*len(reduce_labels)).reshape(+len(temp_N_FEATURES_OPTIONS),len(reduce_labels)))\n",
    "\n",
    "\n",
    "# for i, (param, score) in enumerate(zip(params, mean_scores)):\n",
    "#     trigger=0\n",
    "#     reduce_state = str(param['reduce_dim']).split('(')[0]\n",
    "#     if 'reduce_dim__k' in param:\n",
    "#         N_FEAT = str(param['reduce_dim__k'])\n",
    "#         trigger=1\n",
    "#     elif 'reduce_dim__n_components' in param:\n",
    "#         N_FEAT = str(param['reduce_dim__n_components'])\n",
    "#         trigger=1\n",
    "#     else:\n",
    "#         if reduce_temp.at['None','NMF'] < score:\n",
    "#             reduce_temp.at['None','NMF'] = score\n",
    "#             reduce_temp.at['None','SelectKBest'] = score\n",
    "#     if trigger == 1:\n",
    "#         if reduce_temp.at[N_FEAT,reduce_state] < score:\n",
    "#             reduce_temp.at[N_FEAT,reduce_state] = score\n",
    "\n",
    "            \n",
    "# # Printing a grid of the best accuracies\n",
    "\n",
    "# display(reduce_temp.transpose())\n",
    "\n",
    "\n",
    "# # Print a bar plot which shows the best \n",
    "# # accuracies\n",
    "\n",
    "# reduce_temp.plot(kind='bar',ylim=(0,1),figsize=(14,10),title='Accuracy by Feature Selection',rot=0);           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('The Index of the best model is',grid_search.best_index_)\n",
    "# print('The parameters of the best model is')\n",
    "# display(grid_search.best_params_)\n",
    "# print('The accuracy of the best model is',round(grid_search.best_score_*100,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.e Modeling and Evaluation\n",
    "10 points - Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference signiﬁcant with 95% conﬁdence? Use proper statistical comparison methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.f Modeling and Evaluation\n",
    "10 points - Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classiﬁcation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "5 points - How useful is yolur model for interested parties (i.e., the companies or organizations that might want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exceptional Work\n",
    "10 points - You have free reign to provide additional modeling. \n",
    "One idea: grid search parameters in a parallelized fashion and visualize the \n",
    "performances across attributes. Which parameters are most signiﬁcant for making a \n",
    "good model for each classiﬁcation algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
