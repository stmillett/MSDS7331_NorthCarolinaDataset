{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2\n",
    "\n",
    "<b>Class:</b> MSDS 7331 Data Mining\n",
    "<br> <b>Dataset:</b> Belk Endowment Educational Attainment Data \n",
    "\n",
    "<h1 style=\"font-size:150%;\"> Teammates </h1>\n",
    "Maryam Shahini\n",
    "<br> Murtada Shubbar\n",
    "<br> Michael Toolin\n",
    "<br> Steven Millett"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set global variables\n",
    "#Variables for file and school informaiton\n",
    "\n",
    "YEARS = ['2014', '2015', '2016', '2017']\n",
    "SCHOOLS = ['High','Middle','Elementary']\n",
    "\n",
    "#Number of features we will be selecting for feature selection\n",
    "\n",
    "N_FEATURES_OPTIONS = [2, 25 , 50]\n",
    "\n",
    "#Alpha and C we will be using for our classifiers\n",
    "\n",
    "C_OPTIONS = [1e-2, 1e-1, 1e0, 1e1, 1e2]\n",
    "\n",
    "#Import data all necessary libraries we will be using in our estimation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import re\n",
    "import sklearn\n",
    "import statistics\n",
    "\n",
    "\n",
    "#from umap.umap_ import UMAP\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.feature_selection import SelectKBest, chi2, SelectPercentile, RFE\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, Binarizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.a Data Preparation\n",
    "10 points - Deﬁne and prepare your class variables. Use proper variable \n",
    "representations (int, ﬂoat, one-hot, etc.). Use pre-processing methods (as needed) for\n",
    "dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for \n",
    "the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.b Data Preparation\n",
    "5 points - Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bahr\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The 2017 Public Schools Machine Learning \n",
    "# Date Set is being used throughout this \n",
    "# analysis.  The _ML suffix is removed to less \n",
    "# name space size\n",
    "# Load Full Public School Data Frames for each year\n",
    "\n",
    "school_data = pd.DataFrame()\n",
    "\n",
    "for year in YEARS:\n",
    "    #Load public school master file\n",
    "    temp_year = pd.read_csv('../Data/'+str(year)+'/Machine Learning Datasets/PublicSchools'+str(year)+'_ML.csv', low_memory=False)\n",
    "    \n",
    "    #Iterate through adding and merging school data based on school type\n",
    "    for grade in SCHOOLS:\n",
    "        grade_temp_year = pd.read_csv('../Data/'+year+'/Machine Learning Datasets/Public'+grade+'Schools'+year+'_ML.csv', low_memory=False)\n",
    "        cols_to_use = grade_temp_year.columns.difference(temp_year.columns)\n",
    "        cols_to_use = np.append(cols_to_use,'unit_code')\n",
    "        temp_year = pd.merge(temp_year, grade_temp_year[cols_to_use],left_index=True, right_index=True, on='unit_code',how='left' )\n",
    "    \n",
    "    #Add year column and concatonating all data together\n",
    "    temp_year['Year']=year\n",
    "    school_data = pd.concat([school_data,temp_year],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the critical threshold\n",
    "CRITICAL_NA = .75\n",
    "\n",
    "#With this we check if the column is less than 75% non-NA, if it is greater than 75% non-NA\n",
    "#We replace the NA with the median of the column, otherwise we replace the value with 0\n",
    "\n",
    "imputed_school_data = school_data.apply(lambda col: col.fillna(0) if col.count()/col.shape[0]<CRITICAL_NA else col.fillna(col.median()),axis=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row</th>\n",
       "      <th>Year</th>\n",
       "      <th>avg_daily_attend_pct</th>\n",
       "      <th>lea_federal_perpupil_num</th>\n",
       "      <th>lea_local_perpupil_num</th>\n",
       "      <th>lea_salary_expense_pct</th>\n",
       "      <th>lea_services_expense_pct</th>\n",
       "      <th>lea_state_perpupil_num</th>\n",
       "      <th>lea_supplies_expense_pct</th>\n",
       "      <th>lea_total_expense_num</th>\n",
       "      <th>...</th>\n",
       "      <th>EOGReadingGr4_CACR_SWD</th>\n",
       "      <th>sat_avg_score_num</th>\n",
       "      <th>sat_participation_pct</th>\n",
       "      <th>student_num</th>\n",
       "      <th>SAT_SCORE_ZERO</th>\n",
       "      <th>SAT_Score_above1000</th>\n",
       "      <th>SAT_participation_number</th>\n",
       "      <th>EST_Student_College_NO</th>\n",
       "      <th>Student_Num_College_Ready_SAT</th>\n",
       "      <th>Unnamed: 272</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.872</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>925</td>\n",
       "      <td>0.12</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.36</td>\n",
       "      <td>68.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.957</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>979</td>\n",
       "      <td>0.63</td>\n",
       "      <td>539</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>339.57</td>\n",
       "      <td>199.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.954</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>884</td>\n",
       "      <td>0.70</td>\n",
       "      <td>547</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>382.90</td>\n",
       "      <td>164.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.943</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>824</td>\n",
       "      <td>0.45</td>\n",
       "      <td>800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>360.00</td>\n",
       "      <td>440.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.944</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>14.3</td>\n",
       "      <td>962</td>\n",
       "      <td>0.55</td>\n",
       "      <td>664</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>365.20</td>\n",
       "      <td>298.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.958</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.60</td>\n",
       "      <td>330</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>198.00</td>\n",
       "      <td>132.00</td>\n",
       "      <td>198.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.926</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1008</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1192</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>762.88</td>\n",
       "      <td>429.12</td>\n",
       "      <td>762.88</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.947</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>983</td>\n",
       "      <td>0.32</td>\n",
       "      <td>492</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>157.44</td>\n",
       "      <td>334.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.952</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>959</td>\n",
       "      <td>0.42</td>\n",
       "      <td>628</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>263.76</td>\n",
       "      <td>364.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.960</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>6.3</td>\n",
       "      <td>883</td>\n",
       "      <td>0.62</td>\n",
       "      <td>621</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>385.02</td>\n",
       "      <td>235.98</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.950</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>12.5</td>\n",
       "      <td>805</td>\n",
       "      <td>0.49</td>\n",
       "      <td>419</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>205.31</td>\n",
       "      <td>213.69</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.950</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>798</td>\n",
       "      <td>0.87</td>\n",
       "      <td>674</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>586.38</td>\n",
       "      <td>87.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.912</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>997</td>\n",
       "      <td>0.50</td>\n",
       "      <td>809</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>404.50</td>\n",
       "      <td>404.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.946</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>969</td>\n",
       "      <td>0.23</td>\n",
       "      <td>668</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>153.64</td>\n",
       "      <td>514.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.949</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>12.5</td>\n",
       "      <td>949</td>\n",
       "      <td>0.33</td>\n",
       "      <td>574</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>189.42</td>\n",
       "      <td>384.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.943</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>700</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>700.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.956</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.23</td>\n",
       "      <td>700</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>161.00</td>\n",
       "      <td>539.00</td>\n",
       "      <td>161.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.950</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>505</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>505.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.953</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>920</td>\n",
       "      <td>0.33</td>\n",
       "      <td>532</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>175.56</td>\n",
       "      <td>356.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.916</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>924</td>\n",
       "      <td>0.45</td>\n",
       "      <td>847</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>381.15</td>\n",
       "      <td>465.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.964</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>899</td>\n",
       "      <td>0.54</td>\n",
       "      <td>477</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>257.58</td>\n",
       "      <td>219.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>2014</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>737</td>\n",
       "      <td>0.55</td>\n",
       "      <td>377</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>207.35</td>\n",
       "      <td>169.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.950</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>826</td>\n",
       "      <td>0.75</td>\n",
       "      <td>240</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180.00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.952</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>827</td>\n",
       "      <td>0.46</td>\n",
       "      <td>588</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>270.48</td>\n",
       "      <td>317.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.879</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>855</td>\n",
       "      <td>0.50</td>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45.50</td>\n",
       "      <td>45.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.952</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1018</td>\n",
       "      <td>0.29</td>\n",
       "      <td>603</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>174.87</td>\n",
       "      <td>428.13</td>\n",
       "      <td>174.87</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.958</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>25.0</td>\n",
       "      <td>960</td>\n",
       "      <td>0.39</td>\n",
       "      <td>546</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>212.94</td>\n",
       "      <td>333.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.923</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1022</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1443</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>447.33</td>\n",
       "      <td>995.67</td>\n",
       "      <td>447.33</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.934</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>937</td>\n",
       "      <td>0.35</td>\n",
       "      <td>836</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>292.60</td>\n",
       "      <td>543.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.947</td>\n",
       "      <td>1006.40</td>\n",
       "      <td>1845.78</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5176.41</td>\n",
       "      <td>0.086</td>\n",
       "      <td>8028.59</td>\n",
       "      <td>...</td>\n",
       "      <td>18.2</td>\n",
       "      <td>1070</td>\n",
       "      <td>0.56</td>\n",
       "      <td>303</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>169.68</td>\n",
       "      <td>133.32</td>\n",
       "      <td>169.68</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9701</th>\n",
       "      <td>9701</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.965</td>\n",
       "      <td>1245.45</td>\n",
       "      <td>1664.07</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5912.12</td>\n",
       "      <td>0.085</td>\n",
       "      <td>8821.64</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>227</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>227.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9702</th>\n",
       "      <td>9702</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.933</td>\n",
       "      <td>1245.45</td>\n",
       "      <td>1664.07</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5912.12</td>\n",
       "      <td>0.085</td>\n",
       "      <td>8821.64</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>430</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>430.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9703</th>\n",
       "      <td>9703</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.971</td>\n",
       "      <td>1245.45</td>\n",
       "      <td>1664.07</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5912.12</td>\n",
       "      <td>0.085</td>\n",
       "      <td>8821.64</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>381</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>381.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9704</th>\n",
       "      <td>9704</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.981</td>\n",
       "      <td>1245.45</td>\n",
       "      <td>1664.07</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5912.12</td>\n",
       "      <td>0.085</td>\n",
       "      <td>8821.64</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>537</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>537.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9705</th>\n",
       "      <td>9705</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.966</td>\n",
       "      <td>1245.45</td>\n",
       "      <td>1664.07</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5912.12</td>\n",
       "      <td>0.085</td>\n",
       "      <td>8821.64</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>59.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9706</th>\n",
       "      <td>9706</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.983</td>\n",
       "      <td>1245.45</td>\n",
       "      <td>1664.07</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5912.12</td>\n",
       "      <td>0.085</td>\n",
       "      <td>8821.64</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>290</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>290.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9707</th>\n",
       "      <td>9707</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.935</td>\n",
       "      <td>1245.45</td>\n",
       "      <td>1664.07</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.078</td>\n",
       "      <td>5912.12</td>\n",
       "      <td>0.085</td>\n",
       "      <td>8821.64</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>356</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>356.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9708</th>\n",
       "      <td>9708</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.954</td>\n",
       "      <td>1100.08</td>\n",
       "      <td>1655.07</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.054</td>\n",
       "      <td>6828.77</td>\n",
       "      <td>0.060</td>\n",
       "      <td>9583.92</td>\n",
       "      <td>...</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>347</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>347.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9709</th>\n",
       "      <td>9709</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.962</td>\n",
       "      <td>1100.08</td>\n",
       "      <td>1655.07</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.054</td>\n",
       "      <td>6828.77</td>\n",
       "      <td>0.060</td>\n",
       "      <td>9583.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>239</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>239.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9710</th>\n",
       "      <td>9710</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.961</td>\n",
       "      <td>1100.08</td>\n",
       "      <td>1655.07</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.054</td>\n",
       "      <td>6828.77</td>\n",
       "      <td>0.060</td>\n",
       "      <td>9583.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>258</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>258.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9711</th>\n",
       "      <td>9711</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.959</td>\n",
       "      <td>1100.08</td>\n",
       "      <td>1655.07</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.054</td>\n",
       "      <td>6828.77</td>\n",
       "      <td>0.060</td>\n",
       "      <td>9583.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>269</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>269.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9712</th>\n",
       "      <td>9712</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.967</td>\n",
       "      <td>1100.08</td>\n",
       "      <td>1655.07</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.054</td>\n",
       "      <td>6828.77</td>\n",
       "      <td>0.060</td>\n",
       "      <td>9583.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>246</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>246.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9713</th>\n",
       "      <td>9713</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.946</td>\n",
       "      <td>1100.08</td>\n",
       "      <td>1655.07</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.054</td>\n",
       "      <td>6828.77</td>\n",
       "      <td>0.060</td>\n",
       "      <td>9583.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>459</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>459.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9714</th>\n",
       "      <td>9714</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.951</td>\n",
       "      <td>1100.08</td>\n",
       "      <td>1655.07</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.054</td>\n",
       "      <td>6828.77</td>\n",
       "      <td>0.060</td>\n",
       "      <td>9583.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>860</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>860.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9715</th>\n",
       "      <td>9715</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.954</td>\n",
       "      <td>1100.08</td>\n",
       "      <td>1655.07</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.054</td>\n",
       "      <td>6828.77</td>\n",
       "      <td>0.060</td>\n",
       "      <td>9583.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>326</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>326.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9716</th>\n",
       "      <td>9716</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.956</td>\n",
       "      <td>1100.08</td>\n",
       "      <td>1655.07</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.054</td>\n",
       "      <td>6828.77</td>\n",
       "      <td>0.060</td>\n",
       "      <td>9583.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>319</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>319.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9717</th>\n",
       "      <td>9717</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.944</td>\n",
       "      <td>1100.08</td>\n",
       "      <td>1655.07</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.054</td>\n",
       "      <td>6828.77</td>\n",
       "      <td>0.060</td>\n",
       "      <td>9583.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>575</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>575.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9718</th>\n",
       "      <td>9718</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.949</td>\n",
       "      <td>1100.08</td>\n",
       "      <td>1655.07</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.054</td>\n",
       "      <td>6828.77</td>\n",
       "      <td>0.060</td>\n",
       "      <td>9583.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>508</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>508.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9719</th>\n",
       "      <td>9719</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.991</td>\n",
       "      <td>1100.08</td>\n",
       "      <td>1655.07</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.054</td>\n",
       "      <td>6828.77</td>\n",
       "      <td>0.060</td>\n",
       "      <td>9583.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>221</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>221.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9720</th>\n",
       "      <td>9720</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.955</td>\n",
       "      <td>1100.08</td>\n",
       "      <td>1655.07</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.054</td>\n",
       "      <td>6828.77</td>\n",
       "      <td>0.060</td>\n",
       "      <td>9583.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>556</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>556.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9721</th>\n",
       "      <td>9721</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.837</td>\n",
       "      <td>1100.08</td>\n",
       "      <td>1655.07</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.054</td>\n",
       "      <td>6828.77</td>\n",
       "      <td>0.060</td>\n",
       "      <td>9583.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>49.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9722</th>\n",
       "      <td>9722</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.950</td>\n",
       "      <td>1235.54</td>\n",
       "      <td>2119.55</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.073</td>\n",
       "      <td>7463.07</td>\n",
       "      <td>0.060</td>\n",
       "      <td>10818.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>161</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>161.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9723</th>\n",
       "      <td>9723</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.953</td>\n",
       "      <td>1235.54</td>\n",
       "      <td>2119.55</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.073</td>\n",
       "      <td>7463.07</td>\n",
       "      <td>0.060</td>\n",
       "      <td>10818.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>43.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9724</th>\n",
       "      <td>9724</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.939</td>\n",
       "      <td>1235.54</td>\n",
       "      <td>2119.55</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.073</td>\n",
       "      <td>7463.07</td>\n",
       "      <td>0.060</td>\n",
       "      <td>10818.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>328</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>328.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9725</th>\n",
       "      <td>9725</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.942</td>\n",
       "      <td>1235.54</td>\n",
       "      <td>2119.55</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.073</td>\n",
       "      <td>7463.07</td>\n",
       "      <td>0.060</td>\n",
       "      <td>10818.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>241</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>241.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9726</th>\n",
       "      <td>9726</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.942</td>\n",
       "      <td>1235.54</td>\n",
       "      <td>2119.55</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.073</td>\n",
       "      <td>7463.07</td>\n",
       "      <td>0.060</td>\n",
       "      <td>10818.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>120.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9727</th>\n",
       "      <td>9727</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.950</td>\n",
       "      <td>1235.54</td>\n",
       "      <td>2119.55</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.073</td>\n",
       "      <td>7463.07</td>\n",
       "      <td>0.060</td>\n",
       "      <td>10818.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>281</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>281.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9728</th>\n",
       "      <td>9728</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.944</td>\n",
       "      <td>1235.54</td>\n",
       "      <td>2119.55</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.073</td>\n",
       "      <td>7463.07</td>\n",
       "      <td>0.060</td>\n",
       "      <td>10818.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>177</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>177.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9729</th>\n",
       "      <td>9729</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.945</td>\n",
       "      <td>1235.54</td>\n",
       "      <td>2119.55</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.073</td>\n",
       "      <td>7463.07</td>\n",
       "      <td>0.060</td>\n",
       "      <td>10818.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>692</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>692.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9730</th>\n",
       "      <td>9730</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.939</td>\n",
       "      <td>1235.54</td>\n",
       "      <td>2119.55</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.073</td>\n",
       "      <td>7463.07</td>\n",
       "      <td>0.060</td>\n",
       "      <td>10818.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>115.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9731 rows × 273 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Row  Year  avg_daily_attend_pct  lea_federal_perpupil_num  \\\n",
       "0        0  2014                 0.872                   1006.40   \n",
       "1        1  2014                 0.957                   1006.40   \n",
       "2        2  2014                 0.954                   1006.40   \n",
       "3        3  2014                 0.943                   1006.40   \n",
       "4        4  2014                 0.944                   1006.40   \n",
       "5        5  2014                 0.958                   1006.40   \n",
       "6        6  2014                 0.926                   1006.40   \n",
       "7        7  2014                 0.947                   1006.40   \n",
       "8        8  2014                 0.952                   1006.40   \n",
       "9        9  2014                 0.960                   1006.40   \n",
       "10      10  2014                 0.950                   1006.40   \n",
       "11      11  2014                 0.950                   1006.40   \n",
       "12      12  2014                 0.912                   1006.40   \n",
       "13      13  2014                 0.946                   1006.40   \n",
       "14      14  2014                 0.949                   1006.40   \n",
       "15      15  2014                 0.943                   1006.40   \n",
       "16      16  2014                 0.956                   1006.40   \n",
       "17      17  2014                 0.950                   1006.40   \n",
       "18      18  2014                 0.953                   1006.40   \n",
       "19      19  2014                 0.916                   1006.40   \n",
       "20      20  2014                 0.964                   1006.40   \n",
       "21      21  2014                 1.000                   1006.40   \n",
       "22      22  2014                 0.950                   1006.40   \n",
       "23      23  2014                 0.952                   1006.40   \n",
       "24      24  2014                 0.879                   1006.40   \n",
       "25      25  2014                 0.952                   1006.40   \n",
       "26      26  2014                 0.958                   1006.40   \n",
       "27      27  2014                 0.923                   1006.40   \n",
       "28      28  2014                 0.934                   1006.40   \n",
       "29      29  2014                 0.947                   1006.40   \n",
       "...    ...   ...                   ...                       ...   \n",
       "9701  9701  2017                 0.965                   1245.45   \n",
       "9702  9702  2017                 0.933                   1245.45   \n",
       "9703  9703  2017                 0.971                   1245.45   \n",
       "9704  9704  2017                 0.981                   1245.45   \n",
       "9705  9705  2017                 0.966                   1245.45   \n",
       "9706  9706  2017                 0.983                   1245.45   \n",
       "9707  9707  2017                 0.935                   1245.45   \n",
       "9708  9708  2017                 0.954                   1100.08   \n",
       "9709  9709  2017                 0.962                   1100.08   \n",
       "9710  9710  2017                 0.961                   1100.08   \n",
       "9711  9711  2017                 0.959                   1100.08   \n",
       "9712  9712  2017                 0.967                   1100.08   \n",
       "9713  9713  2017                 0.946                   1100.08   \n",
       "9714  9714  2017                 0.951                   1100.08   \n",
       "9715  9715  2017                 0.954                   1100.08   \n",
       "9716  9716  2017                 0.956                   1100.08   \n",
       "9717  9717  2017                 0.944                   1100.08   \n",
       "9718  9718  2017                 0.949                   1100.08   \n",
       "9719  9719  2017                 0.991                   1100.08   \n",
       "9720  9720  2017                 0.955                   1100.08   \n",
       "9721  9721  2017                 0.837                   1100.08   \n",
       "9722  9722  2017                 0.950                   1235.54   \n",
       "9723  9723  2017                 0.953                   1235.54   \n",
       "9724  9724  2017                 0.939                   1235.54   \n",
       "9725  9725  2017                 0.942                   1235.54   \n",
       "9726  9726  2017                 0.942                   1235.54   \n",
       "9727  9727  2017                 0.950                   1235.54   \n",
       "9728  9728  2017                 0.944                   1235.54   \n",
       "9729  9729  2017                 0.945                   1235.54   \n",
       "9730  9730  2017                 0.939                   1235.54   \n",
       "\n",
       "      lea_local_perpupil_num  lea_salary_expense_pct  \\\n",
       "0                    1845.78                   0.613   \n",
       "1                    1845.78                   0.613   \n",
       "2                    1845.78                   0.613   \n",
       "3                    1845.78                   0.613   \n",
       "4                    1845.78                   0.613   \n",
       "5                    1845.78                   0.613   \n",
       "6                    1845.78                   0.613   \n",
       "7                    1845.78                   0.613   \n",
       "8                    1845.78                   0.613   \n",
       "9                    1845.78                   0.613   \n",
       "10                   1845.78                   0.613   \n",
       "11                   1845.78                   0.613   \n",
       "12                   1845.78                   0.613   \n",
       "13                   1845.78                   0.613   \n",
       "14                   1845.78                   0.613   \n",
       "15                   1845.78                   0.613   \n",
       "16                   1845.78                   0.613   \n",
       "17                   1845.78                   0.613   \n",
       "18                   1845.78                   0.613   \n",
       "19                   1845.78                   0.613   \n",
       "20                   1845.78                   0.613   \n",
       "21                   1845.78                   0.613   \n",
       "22                   1845.78                   0.613   \n",
       "23                   1845.78                   0.613   \n",
       "24                   1845.78                   0.613   \n",
       "25                   1845.78                   0.613   \n",
       "26                   1845.78                   0.613   \n",
       "27                   1845.78                   0.613   \n",
       "28                   1845.78                   0.613   \n",
       "29                   1845.78                   0.613   \n",
       "...                      ...                     ...   \n",
       "9701                 1664.07                   0.828   \n",
       "9702                 1664.07                   0.828   \n",
       "9703                 1664.07                   0.828   \n",
       "9704                 1664.07                   0.828   \n",
       "9705                 1664.07                   0.828   \n",
       "9706                 1664.07                   0.828   \n",
       "9707                 1664.07                   0.828   \n",
       "9708                 1655.07                   0.874   \n",
       "9709                 1655.07                   0.874   \n",
       "9710                 1655.07                   0.874   \n",
       "9711                 1655.07                   0.874   \n",
       "9712                 1655.07                   0.874   \n",
       "9713                 1655.07                   0.874   \n",
       "9714                 1655.07                   0.874   \n",
       "9715                 1655.07                   0.874   \n",
       "9716                 1655.07                   0.874   \n",
       "9717                 1655.07                   0.874   \n",
       "9718                 1655.07                   0.874   \n",
       "9719                 1655.07                   0.874   \n",
       "9720                 1655.07                   0.874   \n",
       "9721                 1655.07                   0.874   \n",
       "9722                 2119.55                   0.860   \n",
       "9723                 2119.55                   0.860   \n",
       "9724                 2119.55                   0.860   \n",
       "9725                 2119.55                   0.860   \n",
       "9726                 2119.55                   0.860   \n",
       "9727                 2119.55                   0.860   \n",
       "9728                 2119.55                   0.860   \n",
       "9729                 2119.55                   0.860   \n",
       "9730                 2119.55                   0.860   \n",
       "\n",
       "      lea_services_expense_pct  lea_state_perpupil_num  \\\n",
       "0                        0.078                 5176.41   \n",
       "1                        0.078                 5176.41   \n",
       "2                        0.078                 5176.41   \n",
       "3                        0.078                 5176.41   \n",
       "4                        0.078                 5176.41   \n",
       "5                        0.078                 5176.41   \n",
       "6                        0.078                 5176.41   \n",
       "7                        0.078                 5176.41   \n",
       "8                        0.078                 5176.41   \n",
       "9                        0.078                 5176.41   \n",
       "10                       0.078                 5176.41   \n",
       "11                       0.078                 5176.41   \n",
       "12                       0.078                 5176.41   \n",
       "13                       0.078                 5176.41   \n",
       "14                       0.078                 5176.41   \n",
       "15                       0.078                 5176.41   \n",
       "16                       0.078                 5176.41   \n",
       "17                       0.078                 5176.41   \n",
       "18                       0.078                 5176.41   \n",
       "19                       0.078                 5176.41   \n",
       "20                       0.078                 5176.41   \n",
       "21                       0.078                 5176.41   \n",
       "22                       0.078                 5176.41   \n",
       "23                       0.078                 5176.41   \n",
       "24                       0.078                 5176.41   \n",
       "25                       0.078                 5176.41   \n",
       "26                       0.078                 5176.41   \n",
       "27                       0.078                 5176.41   \n",
       "28                       0.078                 5176.41   \n",
       "29                       0.078                 5176.41   \n",
       "...                        ...                     ...   \n",
       "9701                     0.078                 5912.12   \n",
       "9702                     0.078                 5912.12   \n",
       "9703                     0.078                 5912.12   \n",
       "9704                     0.078                 5912.12   \n",
       "9705                     0.078                 5912.12   \n",
       "9706                     0.078                 5912.12   \n",
       "9707                     0.078                 5912.12   \n",
       "9708                     0.054                 6828.77   \n",
       "9709                     0.054                 6828.77   \n",
       "9710                     0.054                 6828.77   \n",
       "9711                     0.054                 6828.77   \n",
       "9712                     0.054                 6828.77   \n",
       "9713                     0.054                 6828.77   \n",
       "9714                     0.054                 6828.77   \n",
       "9715                     0.054                 6828.77   \n",
       "9716                     0.054                 6828.77   \n",
       "9717                     0.054                 6828.77   \n",
       "9718                     0.054                 6828.77   \n",
       "9719                     0.054                 6828.77   \n",
       "9720                     0.054                 6828.77   \n",
       "9721                     0.054                 6828.77   \n",
       "9722                     0.073                 7463.07   \n",
       "9723                     0.073                 7463.07   \n",
       "9724                     0.073                 7463.07   \n",
       "9725                     0.073                 7463.07   \n",
       "9726                     0.073                 7463.07   \n",
       "9727                     0.073                 7463.07   \n",
       "9728                     0.073                 7463.07   \n",
       "9729                     0.073                 7463.07   \n",
       "9730                     0.073                 7463.07   \n",
       "\n",
       "      lea_supplies_expense_pct  lea_total_expense_num      ...       \\\n",
       "0                        0.086                8028.59      ...        \n",
       "1                        0.086                8028.59      ...        \n",
       "2                        0.086                8028.59      ...        \n",
       "3                        0.086                8028.59      ...        \n",
       "4                        0.086                8028.59      ...        \n",
       "5                        0.086                8028.59      ...        \n",
       "6                        0.086                8028.59      ...        \n",
       "7                        0.086                8028.59      ...        \n",
       "8                        0.086                8028.59      ...        \n",
       "9                        0.086                8028.59      ...        \n",
       "10                       0.086                8028.59      ...        \n",
       "11                       0.086                8028.59      ...        \n",
       "12                       0.086                8028.59      ...        \n",
       "13                       0.086                8028.59      ...        \n",
       "14                       0.086                8028.59      ...        \n",
       "15                       0.086                8028.59      ...        \n",
       "16                       0.086                8028.59      ...        \n",
       "17                       0.086                8028.59      ...        \n",
       "18                       0.086                8028.59      ...        \n",
       "19                       0.086                8028.59      ...        \n",
       "20                       0.086                8028.59      ...        \n",
       "21                       0.086                8028.59      ...        \n",
       "22                       0.086                8028.59      ...        \n",
       "23                       0.086                8028.59      ...        \n",
       "24                       0.086                8028.59      ...        \n",
       "25                       0.086                8028.59      ...        \n",
       "26                       0.086                8028.59      ...        \n",
       "27                       0.086                8028.59      ...        \n",
       "28                       0.086                8028.59      ...        \n",
       "29                       0.086                8028.59      ...        \n",
       "...                        ...                    ...      ...        \n",
       "9701                     0.085                8821.64      ...        \n",
       "9702                     0.085                8821.64      ...        \n",
       "9703                     0.085                8821.64      ...        \n",
       "9704                     0.085                8821.64      ...        \n",
       "9705                     0.085                8821.64      ...        \n",
       "9706                     0.085                8821.64      ...        \n",
       "9707                     0.085                8821.64      ...        \n",
       "9708                     0.060                9583.92      ...        \n",
       "9709                     0.060                9583.92      ...        \n",
       "9710                     0.060                9583.92      ...        \n",
       "9711                     0.060                9583.92      ...        \n",
       "9712                     0.060                9583.92      ...        \n",
       "9713                     0.060                9583.92      ...        \n",
       "9714                     0.060                9583.92      ...        \n",
       "9715                     0.060                9583.92      ...        \n",
       "9716                     0.060                9583.92      ...        \n",
       "9717                     0.060                9583.92      ...        \n",
       "9718                     0.060                9583.92      ...        \n",
       "9719                     0.060                9583.92      ...        \n",
       "9720                     0.060                9583.92      ...        \n",
       "9721                     0.060                9583.92      ...        \n",
       "9722                     0.060               10818.16      ...        \n",
       "9723                     0.060               10818.16      ...        \n",
       "9724                     0.060               10818.16      ...        \n",
       "9725                     0.060               10818.16      ...        \n",
       "9726                     0.060               10818.16      ...        \n",
       "9727                     0.060               10818.16      ...        \n",
       "9728                     0.060               10818.16      ...        \n",
       "9729                     0.060               10818.16      ...        \n",
       "9730                     0.060               10818.16      ...        \n",
       "\n",
       "      EOGReadingGr4_CACR_SWD  sat_avg_score_num  sat_participation_pct  \\\n",
       "0                        0.0                925                   0.12   \n",
       "1                        0.0                979                   0.63   \n",
       "2                       30.0                884                   0.70   \n",
       "3                        0.0                824                   0.45   \n",
       "4                       14.3                962                   0.55   \n",
       "5                        0.0               1024                   0.60   \n",
       "6                        0.0               1008                   0.64   \n",
       "7                        0.0                983                   0.32   \n",
       "8                       10.0                959                   0.42   \n",
       "9                        6.3                883                   0.62   \n",
       "10                      12.5                805                   0.49   \n",
       "11                      10.0                798                   0.87   \n",
       "12                       0.0                997                   0.50   \n",
       "13                       0.0                969                   0.23   \n",
       "14                      12.5                949                   0.33   \n",
       "15                       0.0                  0                   0.00   \n",
       "16                       0.0               1024                   0.23   \n",
       "17                       0.0                  0                   0.00   \n",
       "18                      20.0                920                   0.33   \n",
       "19                       0.0                924                   0.45   \n",
       "20                       0.0                899                   0.54   \n",
       "21                      20.0                737                   0.55   \n",
       "22                       0.0                826                   0.75   \n",
       "23                       0.0                827                   0.46   \n",
       "24                       0.0                855                   0.50   \n",
       "25                       0.0               1018                   0.29   \n",
       "26                      25.0                960                   0.39   \n",
       "27                       0.0               1022                   0.31   \n",
       "28                       0.0                937                   0.35   \n",
       "29                      18.2               1070                   0.56   \n",
       "...                      ...                ...                    ...   \n",
       "9701                     0.0                  0                   0.00   \n",
       "9702                     0.0                  0                   0.00   \n",
       "9703                     0.0                  0                   0.00   \n",
       "9704                     0.0                  0                   0.00   \n",
       "9705                     0.0                  0                   0.00   \n",
       "9706                     0.0                  0                   0.00   \n",
       "9707                     0.0                  0                   0.00   \n",
       "9708                    25.0                  0                   0.00   \n",
       "9709                     0.0                  0                   0.00   \n",
       "9710                     0.0                  0                   0.00   \n",
       "9711                     0.0                  0                   0.00   \n",
       "9712                     0.0                  0                   0.00   \n",
       "9713                     0.0                  0                   0.00   \n",
       "9714                     0.0                  0                   0.00   \n",
       "9715                     0.0                  0                   0.00   \n",
       "9716                     0.0                  0                   0.00   \n",
       "9717                     0.0                  0                   0.00   \n",
       "9718                     0.0                  0                   0.00   \n",
       "9719                     0.0                  0                   0.00   \n",
       "9720                     0.0                  0                   0.00   \n",
       "9721                     0.0                  0                   0.00   \n",
       "9722                     0.0                  0                   0.00   \n",
       "9723                     0.0                  0                   0.00   \n",
       "9724                     0.0                  0                   0.00   \n",
       "9725                     0.0                  0                   0.00   \n",
       "9726                     0.0                  0                   0.00   \n",
       "9727                     0.0                  0                   0.00   \n",
       "9728                     0.0                  0                   0.00   \n",
       "9729                     0.0                  0                   0.00   \n",
       "9730                     0.0                  0                   0.00   \n",
       "\n",
       "      student_num  SAT_SCORE_ZERO  SAT_Score_above1000  \\\n",
       "0              78               0                    0   \n",
       "1             539               0                    0   \n",
       "2             547               0                    0   \n",
       "3             800               0                    0   \n",
       "4             664               0                    0   \n",
       "5             330               0                    1   \n",
       "6            1192               0                    1   \n",
       "7             492               0                    0   \n",
       "8             628               0                    0   \n",
       "9             621               0                    0   \n",
       "10            419               0                    0   \n",
       "11            674               0                    0   \n",
       "12            809               0                    0   \n",
       "13            668               0                    0   \n",
       "14            574               0                    0   \n",
       "15            700               1                    0   \n",
       "16            700               0                    1   \n",
       "17            505               1                    0   \n",
       "18            532               0                    0   \n",
       "19            847               0                    0   \n",
       "20            477               0                    0   \n",
       "21            377               0                    0   \n",
       "22            240               0                    0   \n",
       "23            588               0                    0   \n",
       "24             91               0                    0   \n",
       "25            603               0                    1   \n",
       "26            546               0                    0   \n",
       "27           1443               0                    1   \n",
       "28            836               0                    0   \n",
       "29            303               0                    1   \n",
       "...           ...             ...                  ...   \n",
       "9701          227               1                    0   \n",
       "9702          430               1                    0   \n",
       "9703          381               1                    0   \n",
       "9704          537               1                    0   \n",
       "9705           59               1                    0   \n",
       "9706          290               1                    0   \n",
       "9707          356               1                    0   \n",
       "9708          347               1                    0   \n",
       "9709          239               1                    0   \n",
       "9710          258               1                    0   \n",
       "9711          269               1                    0   \n",
       "9712          246               1                    0   \n",
       "9713          459               1                    0   \n",
       "9714          860               1                    0   \n",
       "9715          326               1                    0   \n",
       "9716          319               1                    0   \n",
       "9717          575               1                    0   \n",
       "9718          508               1                    0   \n",
       "9719          221               1                    0   \n",
       "9720          556               1                    0   \n",
       "9721           49               1                    0   \n",
       "9722          161               1                    0   \n",
       "9723           43               1                    0   \n",
       "9724          328               1                    0   \n",
       "9725          241               1                    0   \n",
       "9726          120               1                    0   \n",
       "9727          281               1                    0   \n",
       "9728          177               1                    0   \n",
       "9729          692               1                    0   \n",
       "9730          115               1                    0   \n",
       "\n",
       "      SAT_participation_number  EST_Student_College_NO  \\\n",
       "0                         9.36                   68.64   \n",
       "1                       339.57                  199.43   \n",
       "2                       382.90                  164.10   \n",
       "3                       360.00                  440.00   \n",
       "4                       365.20                  298.80   \n",
       "5                       198.00                  132.00   \n",
       "6                       762.88                  429.12   \n",
       "7                       157.44                  334.56   \n",
       "8                       263.76                  364.24   \n",
       "9                       385.02                  235.98   \n",
       "10                      205.31                  213.69   \n",
       "11                      586.38                   87.62   \n",
       "12                      404.50                  404.50   \n",
       "13                      153.64                  514.36   \n",
       "14                      189.42                  384.58   \n",
       "15                        0.00                  700.00   \n",
       "16                      161.00                  539.00   \n",
       "17                        0.00                  505.00   \n",
       "18                      175.56                  356.44   \n",
       "19                      381.15                  465.85   \n",
       "20                      257.58                  219.42   \n",
       "21                      207.35                  169.65   \n",
       "22                      180.00                   60.00   \n",
       "23                      270.48                  317.52   \n",
       "24                       45.50                   45.50   \n",
       "25                      174.87                  428.13   \n",
       "26                      212.94                  333.06   \n",
       "27                      447.33                  995.67   \n",
       "28                      292.60                  543.40   \n",
       "29                      169.68                  133.32   \n",
       "...                        ...                     ...   \n",
       "9701                      0.00                  227.00   \n",
       "9702                      0.00                  430.00   \n",
       "9703                      0.00                  381.00   \n",
       "9704                      0.00                  537.00   \n",
       "9705                      0.00                   59.00   \n",
       "9706                      0.00                  290.00   \n",
       "9707                      0.00                  356.00   \n",
       "9708                      0.00                  347.00   \n",
       "9709                      0.00                  239.00   \n",
       "9710                      0.00                  258.00   \n",
       "9711                      0.00                  269.00   \n",
       "9712                      0.00                  246.00   \n",
       "9713                      0.00                  459.00   \n",
       "9714                      0.00                  860.00   \n",
       "9715                      0.00                  326.00   \n",
       "9716                      0.00                  319.00   \n",
       "9717                      0.00                  575.00   \n",
       "9718                      0.00                  508.00   \n",
       "9719                      0.00                  221.00   \n",
       "9720                      0.00                  556.00   \n",
       "9721                      0.00                   49.00   \n",
       "9722                      0.00                  161.00   \n",
       "9723                      0.00                   43.00   \n",
       "9724                      0.00                  328.00   \n",
       "9725                      0.00                  241.00   \n",
       "9726                      0.00                  120.00   \n",
       "9727                      0.00                  281.00   \n",
       "9728                      0.00                  177.00   \n",
       "9729                      0.00                  692.00   \n",
       "9730                      0.00                  115.00   \n",
       "\n",
       "      Student_Num_College_Ready_SAT  Unnamed: 272  \n",
       "0                              0.00           NaN  \n",
       "1                              0.00           NaN  \n",
       "2                              0.00           NaN  \n",
       "3                              0.00           NaN  \n",
       "4                              0.00           NaN  \n",
       "5                            198.00           NaN  \n",
       "6                            762.88           NaN  \n",
       "7                              0.00           NaN  \n",
       "8                              0.00           NaN  \n",
       "9                              0.00           NaN  \n",
       "10                             0.00           NaN  \n",
       "11                             0.00           NaN  \n",
       "12                             0.00           NaN  \n",
       "13                             0.00           NaN  \n",
       "14                             0.00           NaN  \n",
       "15                             0.00           NaN  \n",
       "16                           161.00           NaN  \n",
       "17                             0.00           NaN  \n",
       "18                             0.00           NaN  \n",
       "19                             0.00           NaN  \n",
       "20                             0.00           NaN  \n",
       "21                             0.00           NaN  \n",
       "22                             0.00           NaN  \n",
       "23                             0.00           NaN  \n",
       "24                             0.00           NaN  \n",
       "25                           174.87           NaN  \n",
       "26                             0.00           NaN  \n",
       "27                           447.33           NaN  \n",
       "28                             0.00           NaN  \n",
       "29                           169.68           NaN  \n",
       "...                             ...           ...  \n",
       "9701                           0.00           NaN  \n",
       "9702                           0.00           NaN  \n",
       "9703                           0.00           NaN  \n",
       "9704                           0.00           NaN  \n",
       "9705                           0.00           NaN  \n",
       "9706                           0.00           NaN  \n",
       "9707                           0.00           NaN  \n",
       "9708                           0.00           NaN  \n",
       "9709                           0.00           NaN  \n",
       "9710                           0.00           NaN  \n",
       "9711                           0.00           NaN  \n",
       "9712                           0.00           NaN  \n",
       "9713                           0.00           NaN  \n",
       "9714                           0.00           NaN  \n",
       "9715                           0.00           NaN  \n",
       "9716                           0.00           NaN  \n",
       "9717                           0.00           NaN  \n",
       "9718                           0.00           NaN  \n",
       "9719                           0.00           NaN  \n",
       "9720                           0.00           NaN  \n",
       "9721                           0.00           NaN  \n",
       "9722                           0.00           NaN  \n",
       "9723                           0.00           NaN  \n",
       "9724                           0.00           NaN  \n",
       "9725                           0.00           NaN  \n",
       "9726                           0.00           NaN  \n",
       "9727                           0.00           NaN  \n",
       "9728                           0.00           NaN  \n",
       "9729                           0.00           NaN  \n",
       "9730                           0.00           1.0  \n",
       "\n",
       "[9731 rows x 273 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#ed = imputed_school_data.copy()\n",
    "\n",
    "#ed.to_csv('C:\\\\Users\\\\Bahr\\\\Desktop\\\\SMU_DS_Summer_2018\\\\Data_Mining\\\\Assignments\\\\Lab TWO\\\\MSDS7331_NorthCarolinaDataset\\\\Lab2\\\\RAW.csv')\n",
    "\n",
    "#ED is processed using Alteryx \n",
    "\n",
    "\n",
    "#reading in new dataset. \n",
    "SAT_Filtered = pd.DataFrame()\n",
    "SAT_Filtered = pd.read_csv('../Data/School Datasets/Alteryx_Product.csv', low_memory=False)\n",
    "\n",
    "\n",
    "SAT_Filtered\n",
    "\n",
    "#SAT_SCORE_ZERO\tBoolean: If SAT equals zero. Used it to test if anyone participated but produced zero scores. \n",
    "#SAT_Score_above1000\t Boolean: SAT score above 1000 or not to get into a decent college in NC. \n",
    "#SAT_participation_number\t Estimate of number of students that took the SAT\n",
    "#EST_Student_College_NO\t Estimate of the number of students who didn't attempt the SAT\n",
    "#Student_Num_College_Ready_SAT\t Estimate of the number of students that can apply to college without SAT score hinderance from that school. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get list of highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GraduationRate_5yr_Female', 'Math Course Rigor Score', 'lea_highqual_class_all_pct', 'lea_highqual_class_pct', 'st_highqual_class_pct', 'st_total_specialized_courses']\n"
     ]
    }
   ],
   "source": [
    "# corr_matrix = imputed_school_data.corr().abs()\n",
    "\n",
    "# # Select upper triangle of correlation matrix\n",
    "# upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# # Find index of feature columns with correlation greater than 0.95\n",
    "# to_drop = [column for column in upper.columns if any(upper[column] > 0.99)]\n",
    "\n",
    "# print(to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.a Modeling and Evaluation\n",
    "Using the right evaluation metric for classification system is crucial. Otherwise, it may results in thinking that the model is performing well but in reality, it doesn’t.\n",
    "\n",
    "There are two tasks in this section of “NC Educational Data” project:\n",
    "\n",
    "The first task is to predict a binary classification target, either if the average SAT score of each school is good enough to gets the student to the North Carolina Universities or not. The SAT is a standardized test widely used for college admissions in the United States. For this purpose we have a cut off 1200 out of 1600. The second task is to predict if the crime per 100 students at each school level is higher than the LEA level or not. After considering all evaluation metrics for classification systems, we ended up using ROC Curve. Area under ROC Curve (or AUC for short) is a performance metric for binary classification problems.\n",
    "\n",
    "In fact, a ROC curve can be used to select a threshold for a classifier which maximizes the true positives, while minimizing the false positives.\n",
    "\n",
    "We usually use ROC when both classes detection are important. Here, our models should be able to decrease both false positive rate (which is identifying the schools with enough good average SAT score for getting admission in different universities) and also decreasing the false negative rate (which is detecting schools with not good average SAT scores).\n",
    "\n",
    "The same for the second task, it is important to decrease both false positive and false negative rates.\n",
    "\n",
    "The AUC represents a model’s ability to discriminate between positive and negative classes. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model as good as random. Most classifiers have AUCs that fall somewhere between these two values. Therefore, the overall model performances can be compared by considering the AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.b Modeling and Evaluation\n",
    "10 points - Choose the method you will use for dividing your data into training and why testing splits (i.e., are you using Stratiﬁed 10-fold cross validation? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into test and train sets. We still want\n",
    "# to do this for external Cross Validation\n",
    "\n",
    "crime_imputed_school_data = imputed_school_data\n",
    "\n",
    "crime_imputed_school_data['local_crime_greater'] = crime_imputed_school_data.apply(lambda each_row: 1 if (each_row['crime_per_c_num']-each_row['lea_crime_per_c_num'])<0 else 0,axis=1)\n",
    "\n",
    "#split data into X and y dataframes\n",
    "\n",
    "y_crime = crime_imputed_school_data['local_crime_greater']\n",
    "\n",
    "#Removed SPG Grade and unit code(which is primary key for school data table)\n",
    " \n",
    "X_crime = imputed_school_data[school_data.columns.drop(list(school_data.filter(regex='crime|unit_code|lea|LEA|^st\\_')))]\n",
    "\n",
    "X_crime_train, X_crime_test, y_crime_train, y_crime_test = train_test_split(X_crime, y_crime, test_size=.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.c Modeling and Evaluation\n",
    "20 points - Create three different classification/regression models (e.g., random forest, KNN, and SVM). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = KFold(n_splits=10,shuffle=True)\n",
    "\n",
    "#This creates the template for the pipeline\n",
    "# This creates a basic pipeline where we will \n",
    "# test for dementionality reduction, scaling,\n",
    "# and classification.\n",
    "\n",
    "\n",
    "pipe = Pipeline([ ('reduce_dim',SelectKBest(chi2)),\n",
    "                  ('scale', StandardScaler()), \n",
    "                  ('clf', GradientBoostingRegressor())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 36 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed:   31.8s\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:  7.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7043625376861664\n"
     ]
    }
   ],
   "source": [
    " param_grid = [\n",
    "    {\n",
    "         'reduce_dim__k': N_FEATURES_OPTIONS,\n",
    "         'clf__n_estimators': C_ESTIMATORS, \n",
    "         'clf__max_depth': C_DEPTH,\n",
    "     }\n",
    "]\n",
    "\n",
    "\n",
    "# # This will test the parameter dict against our \n",
    "# # pipeline\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid=param_grid, cv=k_fold,n_jobs=-1, verbose=1 )\n",
    "\n",
    "\n",
    "# # Here we are training the model, this is \n",
    "# # what takes the most amount of time to run\n",
    "crime_GradientBoost_model = grid_search.fit(X_crime_train, y_crime_train)\n",
    "\n",
    "\n",
    "y_crime_score = grid_search.predict(X_crime_test)\n",
    "\n",
    "print(roc_auc_score(y_crime_test, y_crime_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('reduce_dim', SelectKBest(k='all', score_func=<function chi2 at 0x7ff484499620>)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('clf', GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=5, ma...rs=50, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.set_params(**crime_GradientBoost_model.best_params_)\n",
    "pipe.fit(X_crime_train, y_crime_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 20 features that influence SPG Grade are the following\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Influence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>short_susp_per_c_num</th>\n",
       "      <td>0.089424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_daily_attend_pct</th>\n",
       "      <td>0.024873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPG Score</th>\n",
       "      <td>0.019061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TwoOrMorePct</th>\n",
       "      <td>0.017637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BlackFemalePct</th>\n",
       "      <td>0.017281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EOG/EOCSubjects_CACR_All</th>\n",
       "      <td>0.016024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EOG/EOCSubjects_CACR_Hispanic</th>\n",
       "      <td>0.015840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>books_per_student</th>\n",
       "      <td>0.013949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EOG/EOCSubjects_CACR_White</th>\n",
       "      <td>0.013150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EOGScienceGr8_GLP_SWD</th>\n",
       "      <td>0.011988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long_susp_per_c_num</th>\n",
       "      <td>0.011965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BlackMalePct</th>\n",
       "      <td>0.011592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EOG/EOCSubjects_CACR_Black</th>\n",
       "      <td>0.011327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MinorityMalePct</th>\n",
       "      <td>0.010561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EVAAS Growth Score</th>\n",
       "      <td>0.010119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Proficient_TCHR_Standard 2_Pct</th>\n",
       "      <td>0.009211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_1yr_tchr_trnovr_pct</th>\n",
       "      <td>0.009054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IndianFemalePct</th>\n",
       "      <td>0.008498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>student_num</th>\n",
       "      <td>0.008002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>advance_dgr_pct</th>\n",
       "      <td>0.007695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Influence\n",
       "short_susp_per_c_num             0.089424\n",
       "avg_daily_attend_pct             0.024873\n",
       "SPG Score                        0.019061\n",
       "TwoOrMorePct                     0.017637\n",
       "BlackFemalePct                   0.017281\n",
       "EOG/EOCSubjects_CACR_All         0.016024\n",
       "EOG/EOCSubjects_CACR_Hispanic    0.015840\n",
       "books_per_student                0.013949\n",
       "EOG/EOCSubjects_CACR_White       0.013150\n",
       "EOGScienceGr8_GLP_SWD            0.011988\n",
       "long_susp_per_c_num              0.011965\n",
       "BlackMalePct                     0.011592\n",
       "EOG/EOCSubjects_CACR_Black       0.011327\n",
       "MinorityMalePct                  0.010561\n",
       "EVAAS Growth Score               0.010119\n",
       "Proficient_TCHR_Standard 2_Pct   0.009211\n",
       "_1yr_tchr_trnovr_pct             0.009054\n",
       "IndianFemalePct                  0.008498\n",
       "student_num                      0.008002\n",
       "advance_dgr_pct                  0.007695"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coef = pipe.steps[2][1].feature_importances_\n",
    "\n",
    "mask = pipe.steps[0][1].get_support()\n",
    "new_features=[]\n",
    "feature_names=list(X_crime_train.columns.values)\n",
    "for bool, feature in zip(mask, feature_names):\n",
    "    if bool:\n",
    "        new_features.append(feature)\n",
    "\n",
    "#Creates a new dataframe with the coefficients and the \n",
    "predicted_data = pd.DataFrame(data=coef,index=new_features,columns=['Influence'])\n",
    "print(\"The top 20 features that influence SPG Grade are the following\")\n",
    "\n",
    "\n",
    "\n",
    "display(predicted_data.sort_values(by='Influence', ascending=False)[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6369818484422948\n"
     ]
    }
   ],
   "source": [
    " param_grid = [\n",
    "    {\n",
    "         'reduce_dim__k': N_FEATURES_OPTIONS,\n",
    "         'clf': [AdaBoostClassifier()],\n",
    "         'clf__n_estimators': C_ESTIMATORS\n",
    "\n",
    "     }\n",
    "]\n",
    "\n",
    "\n",
    "# # This will test the parameter dict against our \n",
    "# # pipeline\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid=param_grid, cv=k_fold,n_jobs=-1, verbose=1 )\n",
    "\n",
    "\n",
    "# # Here we are training the model, this is \n",
    "# # what takes the most amount of time to run\n",
    "crime_ADABoost_model = grid_search.fit(X_crime_train, y_crime_train)\n",
    "\n",
    "y_crime_score = grid_search.predict(X_crime_test)\n",
    "\n",
    "print(roc_auc_score(y_crime_test, y_crime_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None), 'clf__n_estimators': 50, 'reduce_dim__k': 'all'}\n"
     ]
    }
   ],
   "source": [
    "print(crime_ADABoost_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 36 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed:   17.3s\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:   58.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6155341446923598\n"
     ]
    }
   ],
   "source": [
    " param_grid = [\n",
    "    {\n",
    "         'reduce_dim__k': N_FEATURES_OPTIONS,\n",
    "         'clf': [RandomForestClassifier()],\n",
    "         'clf__n_estimators': C_ESTIMATORS, \n",
    "         'clf__max_depth': C_DEPTH,\n",
    "     }\n",
    "]\n",
    "\n",
    "\n",
    "# # This will test the parameter dict against our \n",
    "# # pipeline\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid=param_grid, cv=k_fold,n_jobs=-1, verbose=1 )\n",
    "\n",
    "\n",
    "# # Here we are training the model, this is \n",
    "# # what takes the most amount of time to run\n",
    "crime_RandomForest_model = grid_search.fit(X_crime_train, y_crime_train)\n",
    "\n",
    "y_crime_score = grid_search.predict(X_crime_test)\n",
    "\n",
    "print(roc_auc_score(y_crime_test, y_crime_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False), 'clf__max_depth': 5, 'clf__n_estimators': 50, 'reduce_dim__k': 'all'}\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(crime_RandomForest_model.best_params_)\n",
    "print(crime_RandomForest_model.multimetric_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 27 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 270 out of 270 | elapsed: 10.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6263636422738856\n"
     ]
    }
   ],
   "source": [
    " param_grid = [\n",
    "    {\n",
    "         'reduce_dim__k': N_FEATURES_OPTIONS,\n",
    "         'clf': [KNeighborsClassifier()],\n",
    "         'clf__n_neighbors': C_NEIGHBORS, \n",
    "     }\n",
    "]\n",
    "\n",
    "\n",
    "# # This will test the parameter dict against our \n",
    "# # pipeline\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid=param_grid, cv=k_fold,n_jobs=-1, verbose=1 )\n",
    "\n",
    "\n",
    "# # Here we are training the model, this is \n",
    "# # what takes the most amount of time to run\n",
    "crime_KNearest_model = grid_search.fit(X_crime_train, y_crime_train)\n",
    "\n",
    "y_crime_score = grid_search.predict(X_crime_test)\n",
    "\n",
    "print(roc_auc_score(y_crime_test, y_crime_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf': KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=40, p=2,\n",
      "           weights='uniform'), 'clf__n_neighbors': 40, 'reduce_dim__k': 100}\n"
     ]
    }
   ],
   "source": [
    "print(crime_KNearest_model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduced scope model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_crime_reduced = X_crime[X_crime.columns.drop(list(X_crime.filter(regex='[Aa]sian|[Hh]ispanic|[Rr]ace|[Bb]lack|[Mm]inority|[Tw]wo[Oo]r[Mm]ore|[Ii]ndian|[Ww]hite')))]\n",
    "X_crime_reduced_train, X_crime_reduced_test, y_crime_reduced_train, y_crime_reduced_test = train_test_split(X_crime_reduced, y_crime, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00_Size' '01_Size' '02_Size' '03_Size' '04_Size' '05_Size' '06_Size'\n",
      " '07_Size' '08_Size' '09_Size' '10_Size' '11_Size' '12_Size'\n",
      " '4-Year Cohort Graduation Rate Score' 'AAVC_Concentrator_Ct' 'ACT Score'\n",
      " 'ACT WorkKeys Score' 'ACTCompositeScore_UNCMin_AIG'\n",
      " 'ACTCompositeScore_UNCMin_EDS' 'ACTCompositeScore_UNCMin_Female'\n",
      " 'ACTCompositeScore_UNCMin_LEP' 'ACTCompositeScore_UNCMin_Male'\n",
      " 'ACTCompositeScore_UNCMin_SWD' 'ACTEnglish_ACTBenchmark_AIG'\n",
      " 'ACTEnglish_ACTBenchmark_EDS' 'ACTEnglish_ACTBenchmark_Female'\n",
      " 'ACTEnglish_ACTBenchmark_LEP' 'ACTEnglish_ACTBenchmark_Male'\n",
      " 'ACTEnglish_ACTBenchmark_SWD' 'ACTMath_ACTBenchmark_AIG'\n",
      " 'ACTMath_ACTBenchmark_All' 'ACTMath_ACTBenchmark_EDS'\n",
      " 'ACTMath_ACTBenchmark_Female' 'ACTMath_ACTBenchmark_LEP'\n",
      " 'ACTMath_ACTBenchmark_Male' 'ACTMath_ACTBenchmark_SWD'\n",
      " 'ACTReading_ACTBenchmark_AIG' 'ACTReading_ACTBenchmark_All'\n",
      " 'ACTReading_ACTBenchmark_EDS' 'ACTReading_ACTBenchmark_Female'\n",
      " 'ACTReading_ACTBenchmark_LEP' 'ACTReading_ACTBenchmark_Male'\n",
      " 'ACTReading_ACTBenchmark_SWD' 'ACTScience_ACTBenchmark_AIG'\n",
      " 'ACTScience_ACTBenchmark_All' 'ACTScience_ACTBenchmark_EDS'\n",
      " 'ACTScience_ACTBenchmark_Female' 'ACTScience_ACTBenchmark_LEP'\n",
      " 'ACTScience_ACTBenchmark_Male' 'ACTScience_ACTBenchmark_SWD'\n",
      " 'ACTSubtests_BenchmarksMet_AIG' 'ACTSubtests_BenchmarksMet_EDS'\n",
      " 'ACTSubtests_BenchmarksMet_LEP' 'ACTSubtests_BenchmarksMet_Male'\n",
      " 'ACTSubtests_BenchmarksMet_SWD' 'ACTWorkKeys_SilverPlus_AIG'\n",
      " 'ACTWorkKeys_SilverPlus_All' 'ACTWorkKeys_SilverPlus_EDS'\n",
      " 'ACTWorkKeys_SilverPlus_Female' 'ACTWorkKeys_SilverPlus_LEP'\n",
      " 'ACTWorkKeys_SilverPlus_Male' 'ACTWorkKeys_SilverPlus_SWD'\n",
      " 'ACTWorkKeys_pTarget_PctMet' 'ACTWriting_ACTBenchmark_AIG'\n",
      " 'ACTWriting_ACTBenchmark_All' 'ACTWriting_ACTBenchmark_EDS'\n",
      " 'ACTWriting_ACTBenchmark_LEP' 'ACTWriting_ACTBenchmark_Male'\n",
      " 'ACTWriting_ACTBenchmark_SWD' 'ACT_pTarget_PctMet' 'AGNR_Concentrator_Ct'\n",
      " 'ALL_All Students (Total or Subtotal_ENROLL_sch_pct'\n",
      " 'ARCH_Concentrator_Ct' 'Accomplished_TCHR_Standard 1_Pct'\n",
      " 'Accomplished_TCHR_Standard 2_Pct' 'Accomplished_TCHR_Standard 3_Pct'\n",
      " 'Accomplished_TCHR_Standard 4_Pct' 'Accomplished_TCHR_Standard 5_Pct'\n",
      " 'All_All Student (Total or subtotal)_ENROLL_sch_pct'\n",
      " 'BMA_Concentrator_Ct' 'Biology Score' 'Biology_Size' 'Byod_No' 'Byod_YES'\n",
      " 'Byod_Yes' 'Category_Cd_H' 'Cohort Graduation Rate Standard Score'\n",
      " 'CurrentYearEOC_pTarget_PctMet' 'Developing_TCHR_Standard 1_Pct'\n",
      " 'Developing_TCHR_Standard 2_Pct' 'Developing_TCHR_Standard 3_Pct'\n",
      " 'Developing_TCHR_Standard 4_Pct' 'Developing_TCHR_Standard 5_Pct'\n",
      " 'Distinguished_TCHR_Standard 1_Pct' 'Distinguished_TCHR_Standard 2_Pct'\n",
      " 'Distinguished_TCHR_Standard 3_Pct' 'Distinguished_TCHR_Standard 4_Pct'\n",
      " 'Distinguished_TCHR_Standard 5_Pct'\n",
      " 'Does Not Meet Expected Growth_TCHR_Standard 6_Pct'\n",
      " 'Does Not Meet Expected Growth_TCHR_Student Growth_Pct'\n",
      " 'ECODIS_Economically Disadvantaged_ENROLL_sch_pct' 'EOCBiology_CACR_AIG'\n",
      " 'EOCBiology_CACR_EDS' 'EOCBiology_CACR_Female' 'EOCBiology_CACR_LEP'\n",
      " 'EOCBiology_CACR_Male' 'EOCBiology_CACR_SWD' 'EOCBiology_GLP_LEP'\n",
      " 'EOCBiology_GLP_SWD' 'EOCEnglish2_CACR_AIG' 'EOCEnglish2_CACR_EDS'\n",
      " 'EOCEnglish2_CACR_Female' 'EOCEnglish2_CACR_LEP' 'EOCEnglish2_CACR_Male'\n",
      " 'EOCEnglish2_CACR_SWD' 'EOCEnglish2_GLP_LEP' 'EOCEnglish2_GLP_SWD'\n",
      " 'EOCMathI_CACR_AIG' 'EOCMathI_CACR_All' 'EOCMathI_CACR_EDS'\n",
      " 'EOCMathI_CACR_Female' 'EOCMathI_CACR_LEP' 'EOCMathI_CACR_Male'\n",
      " 'EOCMathI_CACR_SWD' 'EOCMathI_GLP_LEP' 'EOCMathI_GLP_SWD'\n",
      " 'EOCSubjects_CACR_AIG' 'EOCSubjects_CACR_All' 'EOCSubjects_CACR_EDS'\n",
      " 'EOCSubjects_CACR_Female' 'EOCSubjects_CACR_LEP' 'EOCSubjects_CACR_Male'\n",
      " 'EOCSubjects_CACR_SWD' 'EOCSubjects_GLP_LEP' 'EOCSubjects_GLP_SWD'\n",
      " 'EOG/EOCSubjects_CACR_AIG' 'EOG/EOCSubjects_CACR_All'\n",
      " 'EOG/EOCSubjects_CACR_EDS' 'EOG/EOCSubjects_CACR_LEP'\n",
      " 'EOG/EOCSubjects_CACR_SWD' 'EOG/EOCSubjects_GLP_AIG'\n",
      " 'EOG/EOCSubjects_GLP_LEP' 'EOG/EOCSubjects_GLP_SWD' 'EOGGr3_CACR_AIG'\n",
      " 'EOGGr3_CACR_All' 'EOGGr3_CACR_EDS' 'EOGGr3_CACR_LEP' 'EOGGr3_CACR_SWD'\n",
      " 'EOGGr3_GLP_LEP' 'EOGGr3_GLP_SWD' 'EOGGr4_CACR_AIG' 'EOGGr4_CACR_All'\n",
      " 'EOGGr4_CACR_EDS' 'EOGGr4_CACR_Female' 'EOGGr4_CACR_LEP'\n",
      " 'EOGGr4_CACR_SWD' 'EOGGr4_GLP_LEP' 'EOGGr4_GLP_SWD' 'EOGGr5_CACR_AIG'\n",
      " 'EOGGr5_CACR_All' 'EOGGr5_CACR_EDS' 'EOGGr5_CACR_LEP' 'EOGGr5_CACR_SWD'\n",
      " 'EOGGr5_GLP_LEP' 'EOGGr5_GLP_SWD' 'EOGGr6_CACR_AIG' 'EOGGr6_CACR_All'\n",
      " 'EOGGr6_CACR_EDS' 'EOGGr6_CACR_Female' 'EOGGr6_CACR_LEP'\n",
      " 'EOGGr6_CACR_Male' 'EOGGr6_CACR_SWD' 'EOGGr6_GLP_LEP' 'EOGGr6_GLP_SWD'\n",
      " 'EOGGr7_CACR_AIG' 'EOGGr7_CACR_All' 'EOGGr7_CACR_EDS'\n",
      " 'EOGGr7_CACR_Female' 'EOGGr7_CACR_LEP' 'EOGGr7_CACR_Male'\n",
      " 'EOGGr7_CACR_SWD' 'EOGGr7_GLP_LEP' 'EOGGr7_GLP_SWD' 'EOGGr8_CACR_AIG'\n",
      " 'EOGGr8_CACR_All' 'EOGGr8_CACR_EDS' 'EOGGr8_CACR_LEP' 'EOGGr8_CACR_SWD'\n",
      " 'EOGGr8_GLP_LEP' 'EOGGr8_GLP_SWD' 'EOGMathGr3-8_CACR_AIG'\n",
      " 'EOGMathGr3-8_CACR_EDS' 'EOGMathGr3-8_CACR_LEP' 'EOGMathGr3-8_CACR_SWD'\n",
      " 'EOGMathGr3-8_GLP_LEP' 'EOGMathGr3-8_GLP_SWD' 'EOGMathGr3_CACR_AIG'\n",
      " 'EOGMathGr3_CACR_EDS' 'EOGMathGr3_CACR_Female' 'EOGMathGr3_CACR_LEP'\n",
      " 'EOGMathGr3_CACR_Male' 'EOGMathGr3_CACR_SWD' 'EOGMathGr3_GLP_EDS'\n",
      " 'EOGMathGr3_GLP_Female' 'EOGMathGr3_GLP_LEP' 'EOGMathGr3_GLP_SWD'\n",
      " 'EOGMathGr4_CACR_AIG' 'EOGMathGr4_CACR_EDS' 'EOGMathGr4_CACR_Female'\n",
      " 'EOGMathGr4_CACR_LEP' 'EOGMathGr4_CACR_Male' 'EOGMathGr4_CACR_SWD'\n",
      " 'EOGMathGr4_GLP_LEP' 'EOGMathGr4_GLP_SWD' 'EOGMathGr5_CACR_AIG'\n",
      " 'EOGMathGr5_CACR_EDS' 'EOGMathGr5_CACR_Female' 'EOGMathGr5_CACR_LEP'\n",
      " 'EOGMathGr5_CACR_Male' 'EOGMathGr5_CACR_SWD' 'EOGMathGr5_GLP_LEP'\n",
      " 'EOGMathGr5_GLP_SWD' 'EOGMathGr6_CACR_AIG' 'EOGMathGr6_CACR_EDS'\n",
      " 'EOGMathGr6_CACR_Female' 'EOGMathGr6_CACR_LEP' 'EOGMathGr6_CACR_Male'\n",
      " 'EOGMathGr6_CACR_SWD' 'EOGMathGr6_GLP_LEP' 'EOGMathGr6_GLP_SWD'\n",
      " 'EOGMathGr7_CACR_AIG' 'EOGMathGr7_CACR_EDS' 'EOGMathGr7_CACR_Female'\n",
      " 'EOGMathGr7_CACR_LEP' 'EOGMathGr7_CACR_Male' 'EOGMathGr7_CACR_SWD'\n",
      " 'EOGMathGr7_GLP_LEP' 'EOGMathGr7_GLP_SWD' 'EOGMathGr8_CACR_AIG'\n",
      " 'EOGMathGr8_CACR_All' 'EOGMathGr8_CACR_EDS' 'EOGMathGr8_CACR_Female'\n",
      " 'EOGMathGr8_CACR_LEP' 'EOGMathGr8_CACR_Male' 'EOGMathGr8_CACR_SWD'\n",
      " 'EOGMathGr8_GLP_LEP' 'EOGMathGr8_GLP_SWD' 'EOGReadingGr3-8_CACR_AIG'\n",
      " 'EOGReadingGr3-8_CACR_EDS' 'EOGReadingGr3-8_CACR_LEP'\n",
      " 'EOGReadingGr3-8_CACR_SWD' 'EOGReadingGr3-8_GLP_LEP'\n",
      " 'EOGReadingGr3-8_GLP_SWD' 'EOGReadingGr3_CACR_EDS'\n",
      " 'EOGReadingGr3_CACR_Female' 'EOGReadingGr3_CACR_LEP'\n",
      " 'EOGReadingGr3_CACR_Male' 'EOGReadingGr3_CACR_SWD'\n",
      " 'EOGReadingGr3_GLP_EDS' 'EOGReadingGr3_GLP_Female'\n",
      " 'EOGReadingGr3_GLP_LEP' 'EOGReadingGr3_GLP_SWD' 'EOGReadingGr4_CACR_EDS'\n",
      " 'EOGReadingGr4_CACR_Female' 'EOGReadingGr4_CACR_LEP'\n",
      " 'EOGReadingGr4_CACR_Male' 'EOGReadingGr4_CACR_SWD'\n",
      " 'EOGReadingGr4_GLP_EDS' 'EOGReadingGr4_GLP_Female'\n",
      " 'EOGReadingGr4_GLP_LEP' 'EOGReadingGr4_GLP_Male' 'EOGReadingGr4_GLP_SWD'\n",
      " 'EOGReadingGr5_CACR_All' 'EOGReadingGr5_CACR_EDS'\n",
      " 'EOGReadingGr5_CACR_Female' 'EOGReadingGr5_CACR_LEP'\n",
      " 'EOGReadingGr5_CACR_Male' 'EOGReadingGr5_CACR_SWD'\n",
      " 'EOGReadingGr5_GLP_EDS' 'EOGReadingGr5_GLP_LEP' 'EOGReadingGr5_GLP_SWD'\n",
      " 'EOGReadingGr6_CACR_EDS' 'EOGReadingGr6_CACR_Female'\n",
      " 'EOGReadingGr6_CACR_LEP' 'EOGReadingGr6_CACR_Male'\n",
      " 'EOGReadingGr6_CACR_SWD' 'EOGReadingGr6_GLP_LEP' 'EOGReadingGr6_GLP_SWD'\n",
      " 'EOGReadingGr7_CACR_All' 'EOGReadingGr7_CACR_EDS'\n",
      " 'EOGReadingGr7_CACR_Female' 'EOGReadingGr7_CACR_LEP'\n",
      " 'EOGReadingGr7_CACR_Male' 'EOGReadingGr7_CACR_SWD'\n",
      " 'EOGReadingGr7_GLP_EDS' 'EOGReadingGr7_GLP_LEP' 'EOGReadingGr7_GLP_SWD'\n",
      " 'EOGReadingGr8_CACR_AIG' 'EOGReadingGr8_CACR_EDS'\n",
      " 'EOGReadingGr8_CACR_Female' 'EOGReadingGr8_CACR_LEP'\n",
      " 'EOGReadingGr8_CACR_Male' 'EOGReadingGr8_CACR_SWD'\n",
      " 'EOGReadingGr8_GLP_EDS' 'EOGReadingGr8_GLP_LEP' 'EOGReadingGr8_GLP_SWD'\n",
      " 'EOGScienceGr5&8_CACR_AIG' 'EOGScienceGr5&8_CACR_EDS'\n",
      " 'EOGScienceGr5&8_CACR_Female' 'EOGScienceGr5&8_CACR_LEP'\n",
      " 'EOGScienceGr5&8_CACR_Male' 'EOGScienceGr5&8_CACR_SWD'\n",
      " 'EOGScienceGr5&8_GLP_LEP' 'EOGScienceGr5&8_GLP_SWD'\n",
      " 'EOGScienceGr5_CACR_EDS' 'EOGScienceGr5_CACR_Female'\n",
      " 'EOGScienceGr5_CACR_LEP' 'EOGScienceGr5_CACR_Male'\n",
      " 'EOGScienceGr5_CACR_SWD' 'EOGScienceGr5_GLP_LEP' 'EOGScienceGr5_GLP_SWD'\n",
      " 'EOGScienceGr8_CACR_All' 'EOGScienceGr8_CACR_EDS'\n",
      " 'EOGScienceGr8_CACR_Female' 'EOGScienceGr8_CACR_Male'\n",
      " 'EOGScienceGr8_CACR_SWD' 'EOGScienceGr8_GLP_SWD' 'EOGSubjects_CACR_AIG'\n",
      " 'EOGSubjects_CACR_LEP' 'EOGSubjects_CACR_SWD' 'EOGSubjects_GLP_LEP'\n",
      " 'EOGSubjects_GLP_SWD' 'EVAAS Growth Score' 'EVAAS Growth Status_Met'\n",
      " 'EVAAS Growth Status_NotMet' 'English II Score' 'English II_Size'\n",
      " 'Exceeds Expected Growth_TCHR_Standard 6_Pct'\n",
      " 'Exceeds Expected Growth_TCHR_Student Growth_Pct'\n",
      " 'F_Female_ENROLL_sch_pct' 'Gr_6_Pct_Prof' 'Gr_9_Pct_Prof'\n",
      " 'Grad_project_status_Y' 'GraduationRate_4yr_AIG' 'GraduationRate_4yr_EDS'\n",
      " 'GraduationRate_4yr_Female' 'GraduationRate_4yr_LEP'\n",
      " 'GraduationRate_4yr_Male' 'GraduationRate_4yr_SWD'\n",
      " 'GraduationRate_5yr_AIG' 'GraduationRate_5yr_All'\n",
      " 'GraduationRate_5yr_EDS' 'GraduationRate_5yr_Female'\n",
      " 'GraduationRate_5yr_LEP' 'GraduationRate_5yr_Male'\n",
      " 'GraduationRate_5yr_SWD' 'HLTH_Concentrator_Ct' 'HOSP_Concentrator_Ct'\n",
      " 'INFO_Concentrator_Ct' 'LEP_Limited English Proficiency_ENROLL_sch_pct'\n",
      " 'MANU_Concentrator_Ct' 'MRKT_Concentrator_Ct'\n",
      " 'MU7_Multiracial_ENROLL_sch_pct' 'M_Male_ENROLL_sch_pct'\n",
      " 'Math Course Rigor Score' 'Math I Score' 'Math I_Size' 'Math SPG Grade'\n",
      " 'Math SPG Grade_B' 'Math SPG Grade_C' 'Math SPG Grade_D'\n",
      " 'Math SPG Grade_F' 'Math SPG Score' 'Math SPG Score_B' 'Math SPG Score_C'\n",
      " 'Math SPG Score_D' 'Math SPG Score_F' 'MathGr10_pTarget_PctMet'\n",
      " 'MathGr3-8_pTarget_PctMet' 'Meets Expected Growth_TCHR_Standard 6_Pct'\n",
      " 'Meets Expected Growth_TCHR_Student Growth_Pct' 'NC Math 1 Score'\n",
      " 'Not Demostrated_TCHR_Standard 1_Pct'\n",
      " 'Not Demostrated_TCHR_Standard 2_Pct'\n",
      " 'Not Demostrated_TCHR_Standard 3_Pct'\n",
      " 'Not Demostrated_TCHR_Standard 4_Pct'\n",
      " 'Not Demostrated_TCHR_Standard 5_Pct' 'Number_Industry_Recognized_Crede'\n",
      " 'PacificIslandFemalePct' 'PacificIslandMalePct' 'PacificIslandPct'\n",
      " 'Passing Math III' 'Passing NC Math 3' 'Proficient_TCHR_Standard 1_Pct'\n",
      " 'Proficient_TCHR_Standard 2_Pct' 'Proficient_TCHR_Standard 3_Pct'\n",
      " 'Proficient_TCHR_Standard 4_Pct' 'Proficient_TCHR_Standard 5_Pct'\n",
      " 'Reading  SPG Score' 'Reading SPG Grade' 'Reading SPG Grade_B'\n",
      " 'Reading SPG Grade_C' 'Reading SPG Grade_D' 'Reading SPG Grade_F'\n",
      " 'Reading SPG Score' 'Reading SPG Score_B' 'Reading SPG Score_C'\n",
      " 'Reading SPG Score_D' 'Reading SPG Score_F' 'ReadingGr10_pTarget_PctMet'\n",
      " 'ReadingGr3-8_pTarget_PctMet' 'SBE District_Northeast'\n",
      " 'SBE District_Northwest' 'SBE District_Piedmont-Triad'\n",
      " 'SBE District_Sandhills' 'SBE District_Southeast'\n",
      " 'SBE District_Southwest' 'SBE District_Western' 'SBE Region'\n",
      " 'SPG Grade_A+NG' 'SPG Grade_B' 'SPG Grade_C' 'SPG Grade_D' 'SPG Grade_F'\n",
      " 'SPG Grade_I' 'SPG Score' 'SRC_Grades_Devices_Sent_Home_10:11:12'\n",
      " 'SRC_Grades_Devices_Sent_Home_10:11:12:13'\n",
      " 'SRC_Grades_Devices_Sent_Home_3' 'SRC_Grades_Devices_Sent_Home_3:04:05'\n",
      " 'SRC_Grades_Devices_Sent_Home_4:05'\n",
      " 'SRC_Grades_Devices_Sent_Home_4:05:06' 'SRC_Grades_Devices_Sent_Home_5'\n",
      " 'SRC_Grades_Devices_Sent_Home_5:06' 'SRC_Grades_Devices_Sent_Home_6'\n",
      " 'SRC_Grades_Devices_Sent_Home_6:07:08'\n",
      " 'SRC_Grades_Devices_Sent_Home_6:7:8:9'\n",
      " 'SRC_Grades_Devices_Sent_Home_6:7:8:9:10:11:12'\n",
      " 'SRC_Grades_Devices_Sent_Home_6:7:8:9:10:11:12:13'\n",
      " 'SRC_Grades_Devices_Sent_Home_7' 'SRC_Grades_Devices_Sent_Home_7:08'\n",
      " 'SRC_Grades_Devices_Sent_Home_8'\n",
      " 'SRC_Grades_Devices_Sent_Home_8:9:10:11:12:13'\n",
      " 'SRC_Grades_Devices_Sent_Home_9:10'\n",
      " 'SRC_Grades_Devices_Sent_Home_9:10:11'\n",
      " 'SRC_Grades_Devices_Sent_Home_9:10:11:12'\n",
      " 'SRC_Grades_Devices_Sent_Home_9:10:11:12:13'\n",
      " 'SRC_Grades_Devices_Sent_Home_9:10:12'\n",
      " 'SRC_Grades_Devices_Sent_Home_K:1:2:3:4:5'\n",
      " 'SRC_Grades_Devices_Sent_Home_K:1:2:3:4:5:6:7:8'\n",
      " 'SRC_Grades_Devices_Sent_Home_PK' 'SRC_devices_sent_home_Yes'\n",
      " 'STEM_Concentrator_Ct' 'SciGr11_pTarget_PctMet' 'SciGr5&8_pTarget_PctMet'\n",
      " 'Science Score' 'State Board Region_Northeast Region'\n",
      " 'State Board Region_Northwest Region'\n",
      " 'State Board Region_Piedmont Triad Region'\n",
      " 'State Board Region_SandHills Region'\n",
      " 'State Board Region_Southeast Region'\n",
      " 'State Board Region_Southwest Region' 'State Board Region_Western Region'\n",
      " 'State Gap Compared_Insufficient Data' 'State Gap Compared_Y'\n",
      " 'TRAN_Concentrator_Ct' 'The ACT Score' 'TotalTargets_pTarget_PctMet'\n",
      " 'WDIS_Students with Disabilities_ENROLL_sch_pct' 'Year'\n",
      " '_1_to_1_access_YES' '_1_to_1_access_Yes' '_1yr_tchr_trnovr_pct'\n",
      " 'advance_dgr_pct' 'ap_ib_courses' 'ap_participation_pct'\n",
      " 'ap_pct_3_or_above' 'avg_age_media_collection' 'avg_daily_attend_pct'\n",
      " 'books_per_student' 'calendar_only_txt_Year-Round Calendar'\n",
      " 'calendar_type_txt_Hospital School'\n",
      " 'calendar_type_txt_Magnet School, Traditional Calendar'\n",
      " 'calendar_type_txt_Magnet School, Year-Round Calendar'\n",
      " 'calendar_type_txt_Regular School, Traditional Calendar'\n",
      " 'calendar_type_txt_Regular School, Traditional Calendar plus Year-Round Calendar'\n",
      " 'calendar_type_txt_Regular School, Year-Round Calendar'\n",
      " 'calendar_type_txt_Special Education, Traditional Calendar'\n",
      " 'calendar_type_txt_Vocational Education, Traditional Calendar'\n",
      " 'category_cd_E' 'category_cd_H' 'category_cd_I' 'category_cd_M'\n",
      " 'category_cd_T' 'class_teach_num' 'closed_ind' 'cte_courses'\n",
      " 'digital_media_pct' 'emer_prov_teach_pct' 'esea_attendance_Met'\n",
      " 'esea_status_P' 'esea_status_R' 'esea_status_RF' 'esea_status_RP'\n",
      " 'expelled_per_c_num' 'flicensed_teach_pct' 'grade_range_cd_11-12'\n",
      " 'grade_range_cd_11-13' 'grade_range_cd_3-12' 'grade_range_cd_4-8'\n",
      " 'grade_range_cd_5-6' 'grade_range_cd_5-8' 'grade_range_cd_6-11'\n",
      " 'grade_range_cd_6-12' 'grade_range_cd_6-13' 'grade_range_cd_6-6'\n",
      " 'grade_range_cd_6-7' 'grade_range_cd_6-8' 'grade_range_cd_6-9'\n",
      " 'grade_range_cd_7-12' 'grade_range_cd_7-13' 'grade_range_cd_7-8'\n",
      " 'grade_range_cd_7-9' 'grade_range_cd_8-12' 'grade_range_cd_8-13'\n",
      " 'grade_range_cd_8-8' 'grade_range_cd_9-10' 'grade_range_cd_9-11'\n",
      " 'grade_range_cd_9-12' 'grade_range_cd_9-13' 'grade_range_cd_9-9'\n",
      " 'grade_range_cd_K-12' 'grade_range_cd_K-8' 'grade_range_cd_PK-12'\n",
      " 'grade_range_cd_PK-13' 'grade_range_cd_PK-7' 'grade_range_cd_PK-8'\n",
      " 'grade_range_cd_PK-9' 'grades_1_to_1_access_10:11:12'\n",
      " 'grades_1_to_1_access_10:11:12:13' 'grades_1_to_1_access_11'\n",
      " 'grades_1_to_1_access_11:12' 'grades_1_to_1_access_11:12:13'\n",
      " 'grades_1_to_1_access_1:2:3:4:5:6:7:8'\n",
      " 'grades_1_to_1_access_2:3:4:5:6:7:8' 'grades_1_to_1_access_3'\n",
      " 'grades_1_to_1_access_3:04:05' 'grades_1_to_1_access_3:4:5:6:7:8'\n",
      " 'grades_1_to_1_access_4' 'grades_1_to_1_access_4:05'\n",
      " 'grades_1_to_1_access_4:05:06' 'grades_1_to_1_access_4:5:6:7:8'\n",
      " 'grades_1_to_1_access_5' 'grades_1_to_1_access_5:06'\n",
      " 'grades_1_to_1_access_5:6:7:8' 'grades_1_to_1_access_6'\n",
      " 'grades_1_to_1_access_6:07' 'grades_1_to_1_access_6:07:08'\n",
      " 'grades_1_to_1_access_6:08' 'grades_1_to_1_access_6:7:8:9:10'\n",
      " 'grades_1_to_1_access_6:7:8:9:10:11:12'\n",
      " 'grades_1_to_1_access_6:7:8:9:10:11:12:13'\n",
      " 'grades_1_to_1_access_6:7:8:9:10:11:12:Early College'\n",
      " 'grades_1_to_1_access_7' 'grades_1_to_1_access_7:08'\n",
      " 'grades_1_to_1_access_7:08:09' 'grades_1_to_1_access_8'\n",
      " 'grades_1_to_1_access_9' 'grades_1_to_1_access_9:10'\n",
      " 'grades_1_to_1_access_9:10:11' 'grades_1_to_1_access_9:10:11:12'\n",
      " 'grades_1_to_1_access_9:10:11:12:.Early College'\n",
      " 'grades_1_to_1_access_9:10:11:12:13'\n",
      " 'grades_1_to_1_access_9:10:11:12:Early'\n",
      " 'grades_1_to_1_access_9:10:11:12:Early College'\n",
      " 'grades_1_to_1_access_9:10:12' 'grades_1_to_1_access_9:11:12:13'\n",
      " 'grades_1_to_1_access_9:12' 'grades_1_to_1_access_Early College'\n",
      " 'grades_1_to_1_access_K:1:2:3' 'grades_1_to_1_access_K:1:2:3:4:5'\n",
      " 'grades_1_to_1_access_K:1:2:3:4:5:6:7:8'\n",
      " 'grades_1_to_1_access_K:1:2:3:4:5:6:7:8:9:10:11:12'\n",
      " 'grades_1_to_1_access_K:7:8' 'grades_1_to_1_access_PK'\n",
      " 'grades_1_to_1_access_PK:K:1:2:3:4:5:6:7:8' 'grades_BYOD_10:11:12'\n",
      " 'grades_BYOD_11:12' 'grades_BYOD_11:12:13' 'grades_BYOD_12'\n",
      " 'grades_BYOD_1:2:3:4:5:6:7:8:9:10:11:12:Early College'\n",
      " 'grades_BYOD_3:4:5:6:7:8' 'grades_BYOD_3:4:5:6:7:8:9:10:11:12'\n",
      " 'grades_BYOD_4:05:06' 'grades_BYOD_4:06' 'grades_BYOD_4:5:6:7:8'\n",
      " 'grades_BYOD_5' 'grades_BYOD_5:06' 'grades_BYOD_5:6:7:8' 'grades_BYOD_6'\n",
      " 'grades_BYOD_6:07' 'grades_BYOD_6:07:08' 'grades_BYOD_6:07:09'\n",
      " 'grades_BYOD_6:08' 'grades_BYOD_6:7:8:9:10:11:12'\n",
      " 'grades_BYOD_6:7:8:9:10:11:12:13' 'grades_BYOD_7' 'grades_BYOD_7:08'\n",
      " 'grades_BYOD_7:8:9:10:11:12' 'grades_BYOD_8' 'grades_BYOD_8:9:10:11:12'\n",
      " 'grades_BYOD_9' 'grades_BYOD_9:10' 'grades_BYOD_9:10:11'\n",
      " 'grades_BYOD_9:10:11:12' 'grades_BYOD_9:10:11:12:13'\n",
      " 'grades_BYOD_9:10:11:12:Early College' 'grades_BYOD_9:11:12'\n",
      " 'grades_BYOD_9:12' 'grades_BYOD_Early College'\n",
      " 'grades_BYOD_K:1:2:3:4:5:6:7:8' 'grades_BYOD_PK:6:7:8'\n",
      " 'grades_BYOD_PK:9:10:11:12' 'grades_BYOD_PK:K:1:2:3:4:5:6:7'\n",
      " 'grades_BYOD_PK:K:1:2:3:4:5:6:7:8'\n",
      " 'grades_BYOD_PK:K:1:2:3:4:5:6:7:8:9:10:11:12:13'\n",
      " 'grades_BYOD_PK:K:1:2:3:4:5:7:8' 'grades_BYOD_PreK:1:2:3:4:5:6:7:8'\n",
      " 'grades_BYOD_PreK:1:2:3:4:5:6:7:8:9:10:11:12:Early College'\n",
      " 'grades_BYOD_PreK:K:1:2:3:4:5:6:7:8' 'highqual_class_pct'\n",
      " 'lateral_teach_pct' 'long_susp_per_c_num' 'nbpts_num' 'pct_GCE_ALL'\n",
      " 'pct_GCE_SWD' 'pct_PASSED_LAA' 'pct_PROMOTED' 'pct_RETAINED' 'pct_eds'\n",
      " 'sat_avg_score_num' 'sat_participation_pct'\n",
      " 'school_type_txt_Magnet School' 'school_type_txt_Regular School'\n",
      " 'short_susp_per_c_num' 'stud_internet_comp_num' 'student_num'\n",
      " 'tchyrs_0thru3_pct' 'tchyrs_11plus_pct' 'tchyrs_4thru10_pct'\n",
      " 'total_specialized_courses' 'univ_college_courses' 'wap_num'\n",
      " 'wap_per_classroom']\n"
     ]
    }
   ],
   "source": [
    "print(X_crime_reduced.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 36 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed:   25.6s\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:  5.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6750940987337503\n"
     ]
    }
   ],
   "source": [
    " param_grid = [\n",
    "    {\n",
    "         'reduce_dim__k': N_FEATURES_OPTIONS,\n",
    "         'clf__n_estimators': C_ESTIMATORS, \n",
    "         'clf__max_depth': C_DEPTH,\n",
    "     }\n",
    "]\n",
    "\n",
    "\n",
    "# # This will test the parameter dict against our \n",
    "# # pipeline\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid=param_grid, cv=k_fold,n_jobs=-1, verbose=1 )\n",
    "\n",
    "\n",
    "# # Here we are training the model, this is \n",
    "# # what takes the most amount of time to run\n",
    "crime_reduced_GradientBoost_model = grid_search.fit(X_crime_reduced_train, y_crime_reduced_train)\n",
    "\n",
    "\n",
    "y_crime_reduced_score = grid_search.predict(X_crime_reduced_test)\n",
    "\n",
    "print(roc_auc_score(y_crime_reduced_test, y_crime_reduced_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('reduce_dim', SelectKBest(k='all', score_func=<function chi2 at 0x7ff484499620>)), ('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('clf', GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=5, ma...rs=50, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.set_params(**crime_reduced_GradientBoost_model.best_params_)\n",
    "pipe.fit(X_crime_reduced_train, y_crime_reduced_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 20 features that influence SPG Grade are the following\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Influence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>short_susp_per_c_num</th>\n",
       "      <td>0.095112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_daily_attend_pct</th>\n",
       "      <td>0.046270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EOG/EOCSubjects_CACR_All</th>\n",
       "      <td>0.027405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPG Score</th>\n",
       "      <td>0.019949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>student_num</th>\n",
       "      <td>0.019401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stud_internet_comp_num</th>\n",
       "      <td>0.016367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EVAAS Growth Score</th>\n",
       "      <td>0.015583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>advance_dgr_pct</th>\n",
       "      <td>0.015271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>books_per_student</th>\n",
       "      <td>0.014892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long_susp_per_c_num</th>\n",
       "      <td>0.013610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>digital_media_pct</th>\n",
       "      <td>0.012521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EOG/EOCSubjects_CACR_EDS</th>\n",
       "      <td>0.011887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EOGScienceGr8_GLP_SWD</th>\n",
       "      <td>0.011385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EOGMathGr3-8_CACR_EDS</th>\n",
       "      <td>0.011039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EOGReadingGr5_GLP_SWD</th>\n",
       "      <td>0.009706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_1yr_tchr_trnovr_pct</th>\n",
       "      <td>0.009103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>03_Size</th>\n",
       "      <td>0.008913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exceeds Expected Growth_TCHR_Standard 6_Pct</th>\n",
       "      <td>0.008696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tchyrs_0thru3_pct</th>\n",
       "      <td>0.008463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EOGGr8_CACR_EDS</th>\n",
       "      <td>0.008035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Influence\n",
       "short_susp_per_c_num                          0.095112\n",
       "avg_daily_attend_pct                          0.046270\n",
       "EOG/EOCSubjects_CACR_All                      0.027405\n",
       "SPG Score                                     0.019949\n",
       "student_num                                   0.019401\n",
       "stud_internet_comp_num                        0.016367\n",
       "EVAAS Growth Score                            0.015583\n",
       "advance_dgr_pct                               0.015271\n",
       "books_per_student                             0.014892\n",
       "long_susp_per_c_num                           0.013610\n",
       "digital_media_pct                             0.012521\n",
       "EOG/EOCSubjects_CACR_EDS                      0.011887\n",
       "EOGScienceGr8_GLP_SWD                         0.011385\n",
       "EOGMathGr3-8_CACR_EDS                         0.011039\n",
       "EOGReadingGr5_GLP_SWD                         0.009706\n",
       "_1yr_tchr_trnovr_pct                          0.009103\n",
       "03_Size                                       0.008913\n",
       "Exceeds Expected Growth_TCHR_Standard 6_Pct   0.008696\n",
       "tchyrs_0thru3_pct                             0.008463\n",
       "EOGGr8_CACR_EDS                               0.008035"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coef = pipe.steps[2][1].feature_importances_\n",
    "\n",
    "mask = pipe.steps[0][1].get_support()\n",
    "new_features=[]\n",
    "feature_names=list(X_crime_reduced_train.columns.values)\n",
    "for bool, feature in zip(mask, feature_names):\n",
    "    if bool:\n",
    "        new_features.append(feature)\n",
    "\n",
    "#Creates a new dataframe with the coefficients and the \n",
    "predicted_data = pd.DataFrame(data=coef,index=new_features,columns=['Influence'])\n",
    "print(\"The top 20 features that influence SPG Grade are the following\")\n",
    "\n",
    "\n",
    "\n",
    "display(predicted_data.sort_values(by='Influence', ascending=False)[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.d Modeling and Evaluation\n",
    "10 points - Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Run this to load the model from the save file\n",
    "\n",
    "# from sklearn.externals import joblib\n",
    "# grid_search = joblib.load('savedBestModel.pkl')\n",
    "\n",
    "\n",
    "# # Loads all parameters run into a dict \n",
    "\n",
    "# params = np.array(grid_search.cv_results_['params'])\n",
    "\n",
    "\n",
    "# # Loads all mean test scores into an array\n",
    "\n",
    "# mean_scores = np.array(grid_search.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assigns all models to an array\n",
    "\n",
    "# classifier_labels=['SVC','LogisticRegression','SGDClassifier']\n",
    "\n",
    "\n",
    "# # Creates an empty dataframe that is to be\n",
    "# # filled with the mean test accuracy by C global\n",
    "# # variable and the different classifiers\n",
    "\n",
    "# classifier_temp = pd.DataFrame(columns=classifier_labels,index=C_OPTIONS,\n",
    "#                                data=np.linspace(.1,.25,num=len(C_OPTIONS)*len(classifier_labels)).reshape(len(C_OPTIONS),len(classifier_labels)))\n",
    "# classifier_temp.fillna(0,inplace=True)\n",
    "\n",
    "# for i, (param, score) in enumerate(zip(params, mean_scores)):\n",
    "#     C = param['clf__C'] if 'clf__C' in param else param['clf__alpha']\n",
    "#     class_state = str(param['clf']).split('(')[0]\n",
    "#     if classifier_temp.at[C,class_state] < score:\n",
    "#         classifier_temp.at[C,class_state] = score\n",
    "\n",
    "\n",
    "# # Printing a grid of the best accuracies\n",
    "        \n",
    "# display(classifier_temp.transpose())   \n",
    "\n",
    "\n",
    "# # Print a line plot which shows the best \n",
    "# # accuracies\n",
    " \n",
    "# classifier_temp.plot(logx=True,ylim=(0,1),figsize=(14,10),title='Accuracy by Classifier'); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assigns all reduction models to an array\n",
    "\n",
    "# reduce_labels=['NMF','PCA','SelectKBest']\n",
    "\n",
    "\n",
    "# # Translates the N Features array\n",
    "# # to an array full of string\n",
    "\n",
    "# temp_N_FEATURES_OPTIONS = [str(r) for r in N_FEATURES_OPTIONS]\n",
    "# temp_N_FEATURES_OPTIONS=temp_N_FEATURES_OPTIONS+['None']\n",
    "\n",
    "\n",
    "# # Creates an empty dataframe that is to be\n",
    "# # filled with the mean test accuracy by N Features\n",
    "# # variable and the different feature reduction models\n",
    "\n",
    "# reduce_temp = pd.DataFrame(columns=reduce_labels,index=temp_N_FEATURES_OPTIONS,\n",
    "#                                data=np.linspace(.1,.25,num=len(temp_N_FEATURES_OPTIONS)*len(reduce_labels)).reshape(+len(temp_N_FEATURES_OPTIONS),len(reduce_labels)))\n",
    "\n",
    "\n",
    "# for i, (param, score) in enumerate(zip(params, mean_scores)):\n",
    "#     trigger=0\n",
    "#     reduce_state = str(param['reduce_dim']).split('(')[0]\n",
    "#     if 'reduce_dim__k' in param:\n",
    "#         N_FEAT = str(param['reduce_dim__k'])\n",
    "#         trigger=1\n",
    "#     elif 'reduce_dim__n_components' in param:\n",
    "#         N_FEAT = str(param['reduce_dim__n_components'])\n",
    "#         trigger=1\n",
    "#     else:\n",
    "#         if reduce_temp.at['None','NMF'] < score:\n",
    "#             reduce_temp.at['None','NMF'] = score\n",
    "#             reduce_temp.at['None','SelectKBest'] = score\n",
    "#     if trigger == 1:\n",
    "#         if reduce_temp.at[N_FEAT,reduce_state] < score:\n",
    "#             reduce_temp.at[N_FEAT,reduce_state] = score\n",
    "\n",
    "            \n",
    "# # Printing a grid of the best accuracies\n",
    "\n",
    "# display(reduce_temp.transpose())\n",
    "\n",
    "\n",
    "# # Print a bar plot which shows the best \n",
    "# # accuracies\n",
    "\n",
    "# reduce_temp.plot(kind='bar',ylim=(0,1),figsize=(14,10),title='Accuracy by Feature Selection',rot=0);           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('The Index of the best model is',grid_search.best_index_)\n",
    "# print('The parameters of the best model is')\n",
    "# display(grid_search.best_params_)\n",
    "# print('The accuracy of the best model is',round(grid_search.best_score_*100,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.e Modeling and Evaluation\n",
    "10 points - Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference signiﬁcant with 95% conﬁdence? Use proper statistical comparison methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.f Modeling and Evaluation\n",
    "10 points - Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classiﬁcation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "5 points - How useful is yolur model for interested parties (i.e., the companies or organizations that might want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exceptional Work\n",
    "10 points - You have free reign to provide additional modeling. \n",
    "One idea: grid search parameters in a parallelized fashion and visualize the \n",
    "performances across attributes. Which parameters are most signiﬁcant for making a \n",
    "good model for each classiﬁcation algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
