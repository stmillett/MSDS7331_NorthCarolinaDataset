{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1\n",
    "\n",
    "<b>Class:</b> MSDS 7331 Data Mining\n",
    "<br> <b>Dataset:</b> Belk Endowment Educational Attainment Data \n",
    "\n",
    "<h1 style=\"font-size:150%;\"> Teammates </h1>\n",
    "Maryam Shahini\n",
    "<br> Murtada Shubbar\n",
    "<br> Michael Toolin\n",
    "<br> Steven Millett"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "import seaborn as sns\n",
    "import math\n",
    "import re\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# The 2017 Public Schools Machine Learning Date Set is being used throughout this analysis.  The _ML suffix is removed to less name space size\n",
    "#\n",
    "# Load Full Public School Data Frames for each year\n",
    "#\n",
    "school_data = pd.read_csv('./Data/2017/machine Learning Datasets/PublicSchools2017_ML.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding \n",
    "\n",
    "The North Carolina General Assmebly passed legislation in 2014-2014 requiring the assignment of School Performance Grades (SPG) for public and charter Schools [1].  This data set is collected in response to this legislation.  A school's SPG is calculated using 80% of the schools achievment score and 20% of the schools growth score.  The achievment score is calculated through a variety of student testing and the growth score is calculated using the EVASS School Accountablityy Growth Composite Index [2]. Schools are assigned a letter grade where A: 100-85 points, B: 84-70 points, C: 69-55 points, D: 54-40 points and F: less than 40 points.  Schools that receive grades of D or F are required by to inform parents of the school district.  In 2016, the North Carolina General Assmebly passed legislation creating the Achievment School District(ASD). This school district is run by a private organization and are run as charter schools [3].\n",
    "\n",
    "This data set contains 334 features describing 2443 schools.  The data includes testing results used to derive the SPG described above.  It also contains school financial data, demographic information, attendence, and student behavior data measured by metrics such as susupension and expulsions. We can look into all these different types of information to see if any correlation with school performances exists, both good and bad.  Do poorly performing schools line up with any specific demographics?  Are there school financial situations that help attribute to a schools performance? Finding correlations of this data with SPG and being able to use that information in a predictive analysis algorithm may help educators identify schools before the perfomance metrics deteriorate, allowing them to intervene. The end result of all the testing and analaysis is providing all students a fair and equal opportunity at a qualtiy eduction.\n",
    "\n",
    "[1] source: http://schools.cms.k12.nc.us/jhgunnES/Documents/School%20Performance%20Grade%20PP%20January%2014,%202015%20(1).pptx\n",
    "[2] (EVASS Growth information available at http://www.ncpublicschools.org/effectiveness-model/evaas/selection/)\n",
    "[3] source: https://www.ncforum.org/committee-on-low-performing-schools/\n",
    "\n",
    "###citation: Drew J., The Belk Endowment Educational Attainment Data Repository for North Carolina Public Schools, (2018), GitHub repository, https://github.com/jakemdrew/EducationDataNC\n",
    "\n",
    "Describe the purpose of the data set you selected (i.e., why was this data collected in the first place?). Describe how you would define and measure the outcomes from the dataset. That is, why is this data important and how do you know if you have mined useful knowledge from the dataset? How would you measure the effectiveness of a good prediction algorithm? Be specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Meaning Type \n",
    "\n",
    "Describe the meaning and type of data (scale, values, etc.) for each attribute in the data file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The comprehensive description of all 334 attributes can be found in the data-dictionary.pdf associated with the NC Report Card database provided by Dr. Drew. We were interested in 60 variables moving forward in the course. We visualize several attributes of interest in this report.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/data_meaning.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "school_data.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatter_matrix(school_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality\n",
    "\n",
    "Verify data quality: Explain any missing values, duplicate data, and outliers. Are those mistakes? How do you deal with these problems? Give justifications for your methods.\n",
    "There is no missing values in our dataset to worry about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find the missing values if any:\n",
    "#Print out all the missing value rows\n",
    "pd.set_option('display.max_rows', 10000)\n",
    "\n",
    "print('\\r\\n**The Remaining Missing Values Below will be set to Zero!**')\n",
    "\n",
    "#Check for Missing values \n",
    "missing_values = school_data.isnull().sum().reset_index()\n",
    "missing_values.columns = ['Variable Name', 'Number Missing Values']\n",
    "missing_values = missing_values[missing_values['Number Missing Values'] > 0] \n",
    "missing_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Statistics\n",
    "\n",
    "Visualize appropriate statistics (e.g., range, mode, mean, median, variance, counts) for a subset of attributes. Describe anything meaningful you found from this or if you found something potentially interesting. Note: You can also use data from other sources for comparison. Explain why the statistics run are meaningful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "school_data.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "school_data_simple = school_data[['SPG Score','lea_avg_student_num','student_num']]\n",
    "school_data_simple.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Statistics on SPG Score, and number of students in schools\n",
    "The table above show us statistics for SPG.  Understanding the mean SPG score is 60.91, which is in the lower part of the \"C\" range.  The median SPG score\n",
    "is 64.00, also in \"C\" range.  It is surprising to see a maximum SPG score of 100 as this indicates perfect scoring for some individual school.  Also the minimum score of 0 mean total failure in testing.  Both of these data points require more investigation to see if see if there are any errors in the data and where these points come from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the schools whose SPG =1 00\n",
    "school_SPG_100 =(school_data.loc[school_data['SPG Score'] == 100])\n",
    "print (school_SPG_100['unit_code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The school with unit code 190501 has a perfect SPG. The data in this field should be examined for errors or to understand why they received a perfect SPG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the schools whose SPG = 0  Purposely coded different than above to show various coding techniques\n",
    "school_SPG_0 =(school_data['unit_code'].loc[school_data['SPG Score'] == 0])\n",
    "print (school_SPG_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list above shows 130 schools with SPG = 0.  This is more problematic than the single school whose SPG = 100.  Nonetheless we need to understand why 130 schools had an SPG = 0 and requires future investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "school_data_finance = school_data[['lea_total_expense_num','lea_salary_expense_pct',\n",
    "                                  'lea_services_expense_pct', 'lea_supplies_expense_pct',\n",
    "                                  'lea_instruct_equip_exp_pct']]\n",
    "school_data_finance.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the expenses, the LEA with largest number of expenses is 90% higher than the mean for all LEA's in the state.  All other quantiles are closer to the mean, indicating a good chance this in outlier.  We will look for this school to see if there are errors in the data and consider how to handle this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demog_male_cols = [col for col in school_data.columns if \"Male\" in col]\n",
    "demog_female_cols = [col for col in school_data.columns if \"male\" in col]\n",
    "school_data_demog_male = school_data[np.intersect1d(school_data.columns, demog_male_cols)]\n",
    "school_data_demog_female = school_data[np.intersect1d(school_data.columns, demog_female_cols)]\n",
    "school_data_demog_all = pd.concat([school_data_demog_female, school_data_demog_male], axis=1)\n",
    "school_data_demog = school_data_demog_all.filter(regex='^(?!(EOC|Two).*?)') \n",
    "\n",
    "school_data_demog.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above table shows actual quantile breakdown by minority demographics.  It is interesting to note there is at least one LEA that has 0 minorities and at least one where the entire male population is made up of minorities.  Identifying these LEA's may provide some information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Attributes\n",
    "\n",
    "Visualize the most interesting attributes (at least 5 attributes, your opinion on what is interesting). Important: Interpret the implications for each visualization. Explain for each attribute why the chosen visualization is appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefixsearch(search_string, missing_value ,start_of_search_string,end_of_search_string):\n",
    "    if re.search(end_of_search_string, search_string):\n",
    "        return re.search('('+ start_of_search_string +'\\S*)(?='+ end_of_search_string +')',search_string).group(0)\n",
    "    else:\n",
    "        return missing_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_temp_col = [col for col in school_data.columns if 'tchyrs' in col]\n",
    "teacher_columns = school_data[teacher_temp_col].melt(var_name='col',value_name='values')\n",
    "\n",
    "teacher_columns['year'] = teacher_columns['col'].apply(lambda name: re.search('(?<=tchyrs_)\\S*(?=_)',name).group(0))\n",
    "teacher_columns['region'] = teacher_columns['col'].apply(lambda name: prefixsearch(name, \"Sch\", '^\\S*','_tch'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a4_dims = (11.7, 8.27)\n",
    "fig, ax = plt.subplots(figsize=a4_dims)\n",
    "sns.boxplot(ax=ax,data=teacher_columns,x='year', y='values', hue='region');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Makeup of teachers in classrooms\n",
    "In the graph above we are looking at the percentage of teachers that makeup a classroom based on their years of tenure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_temp_col = school_data.filter(regex=('[Mm]alePct')).columns\n",
    "sex_teacher_columns = school_data[sex_temp_col].melt(var_name='col',value_name='Values')\n",
    "\n",
    "sex_teacher_columns['Race'] = sex_teacher_columns['col'].apply(lambda name: prefixsearch(name, \"\", '^','Male|Female'))\n",
    "sex_teacher_columns['Sex'] = sex_teacher_columns['col'].apply(lambda name: 'Female' if re.search('Female',name) else 'Male')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a4_dims = (15, 10)\n",
    "fig, ax = plt.subplots(figsize=a4_dims)\n",
    "sns.violinplot(ax=ax,x='Race',y='Values',hue='Sex',data=sex_teacher_columns,palette={\"Male\": \"b\", \"Female\": \"y\"});\n",
    "\n",
    "leg = plt.legend( loc = 'upper right')\n",
    "\n",
    "plt.draw() # Draw the figure so you can find the positon of the legend. \n",
    "\n",
    "# Get the bounding box of the original legend\n",
    "bb = leg.get_bbox_to_anchor().inverse_transformed(ax.transAxes)\n",
    "\n",
    "# Change to location of the legend. \n",
    "xOffset = .1\n",
    "bb.x0 += xOffset\n",
    "bb.x1 += xOffset\n",
    "leg.set_bbox_to_anchor(bb, transform = ax.transAxes)\n",
    "\n",
    "\n",
    "# Update the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Makeup of minorities in classrooms\n",
    "\n",
    "In the graph above we are looking at the percentage of minorities that makeup a classroom as represented with a violin plot. Each half of the violin represents the different sexual make-up of each race.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_col = school_data.filter(regex=('wap|books|stud_internet')).columns\n",
    "support_columns = school_data[support_col].melt(var_name='col',value_name='Values')\n",
    "\n",
    "support_columns = support_columns[support_columns['Values']!=0]\n",
    "\n",
    "support_columns['Values'] = support_columns['Values'].apply(lambda value: math.log(value))\n",
    "\n",
    "\n",
    "#support_columns['media'] = sex_teacher_columns['col'].apply(lambda name: 'Female' if re.search('Female',name) else 'Male')\n",
    "\n",
    "support_columns['region'] = support_columns['col'].apply(lambda name: prefixsearch(name, \"Sch\", '^\\S*','_wap|_books|_stud_int'))\n",
    "support_columns['media'] = support_columns['col'].apply(lambda name: re.sub('lea_','',name) if re.search('lea',name) else name )\n",
    "support_columns = support_columns.sort_values(by='Values')\n",
    "#print(support_columns.sample(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a4_dims = (15, 10)\n",
    "fig, ax = plt.subplots(figsize=a4_dims)\n",
    "sns.boxplot(ax=ax,x='media',y='Values',hue='region',data=support_columns,palette={\"lea\": \"b\", \"Sch\": \"y\"});\n",
    "plt.xticks(rotation=30)\n",
    "\n",
    "leg = plt.legend( loc = 'upper right')\n",
    "\n",
    "plt.draw() # Draw the figure so you can find the positon of the legend. \n",
    "\n",
    "# Get the bounding box of the original legend\n",
    "bb = leg.get_bbox_to_anchor().inverse_transformed(ax.transAxes)\n",
    "\n",
    "# Change to location of the legend. \n",
    "xOffset = .1\n",
    "bb.x0 += xOffset\n",
    "bb.x1 += xOffset\n",
    "leg.set_bbox_to_anchor(bb, transform = ax.transAxes)\n",
    "\n",
    "\n",
    "# Update the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Media and Computer resources available\n",
    "These box plots map the difference in the amount of resources that the average LEA has and the variance per school."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TCHR_col = school_data.filter(regex=('TCHR_Standard')).columns\n",
    "TCHR_columns = school_data[TCHR_col].melt(var_name='col',value_name='Values')\n",
    "\n",
    "\n",
    "TCHR_columns['Standard'] = TCHR_columns['col'].apply(lambda name: re.search('(?<=TCHR_).*(?=_Pct)',name).group(0))\n",
    "TCHR_columns['Level'] = TCHR_columns['col'].apply(lambda name: re.search('(?<=^).*(?=_TCHR)',name).group(0))\n",
    "\n",
    "print(TCHR_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a4_dims = (15, 10)\n",
    "fig, ax = plt.subplots(figsize=a4_dims)\n",
    "sns.violinplot(ax=ax,x='Level',y='Values',hue='Standard',data=TCHR_columns);\n",
    "plt.xticks(rotation=30)\n",
    "\n",
    "\n",
    "leg = plt.legend( loc = 'upper right')\n",
    "\n",
    "plt.draw() # Draw the figure so you can find the positon of the legend. \n",
    "\n",
    "# Get the bounding box of the original legend\n",
    "bb = leg.get_bbox_to_anchor().inverse_transformed(ax.transAxes)\n",
    "\n",
    "# Change to location of the legend. \n",
    "xOffset = .15\n",
    "bb.x0 += xOffset\n",
    "bb.x1 += xOffset\n",
    "leg.set_bbox_to_anchor(bb, transform = ax.transAxes)\n",
    "\n",
    "\n",
    "# Update the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here is some text about the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOG_col = school_data.filter(regex=('EOGG')).columns\n",
    "EOG_columns = school_data[EOG_col].melt(var_name='col',value_name='Values')\n",
    "\n",
    "EOG_columns['Grade'] = EOG_columns['col'].apply(lambda name: re.search('(?<=EOG).*(?=_[CG])',name).group(0))\n",
    "EOG_columns['Level'] = EOG_columns['col'].apply(lambda name: re.split('EOGGr[3-5]_',name)[1])\n",
    "\n",
    "EOG_columns = EOG_columns.sort_values(by='Level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a4_dims = (15, 10)\n",
    "fig, ax = plt.subplots(figsize=a4_dims)\n",
    "plot = sns.violinplot(ax=ax,x='Level',y='Values',hue='Grade',data=EOG_columns);\n",
    "plt.xticks(rotation=30)\n",
    "\n",
    "leg = plt.legend( loc = 'upper right',title='Grade')\n",
    "\n",
    "plt.draw() # Draw the figure so you can find the positon of the legend. \n",
    "\n",
    "# Get the bounding box of the original legend\n",
    "bb = leg.get_bbox_to_anchor().inverse_transformed(ax.transAxes)\n",
    "\n",
    "# Change to location of the legend. \n",
    "xOffset = .1\n",
    "bb.x0 += xOffset\n",
    "bb.x1 += xOffset\n",
    "leg.set_bbox_to_anchor(bb, transform = ax.transAxes)\n",
    "\n",
    "\n",
    "# Update the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here is some text about the EOG stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOC_col = school_data.filter(regex=('^EOC')).columns\n",
    "EOC_columns = school_data[EOC_col].melt(var_name='col',value_name='Values')\n",
    "\n",
    "EOC_columns['Subject'] = EOC_columns['col'].apply(lambda name: re.search('(?<=EOC).*(?=_[CG])',name).group(0))\n",
    "EOC_columns['Level'] = EOC_columns['col'].apply(lambda name: re.split('EOC(Subjects|MathI)_',name)[2])\n",
    "\n",
    "EOC_columns = EOC_columns.sort_values(by='Level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a4_dims = (15, 10)\n",
    "fig, ax = plt.subplots(figsize=a4_dims)\n",
    "sns.violinplot(ax=ax,x='Level',y='Values',hue='Subject',data=EOC_columns);\n",
    "plt.xticks(rotation=30)\n",
    "\n",
    "leg = plt.legend( loc = 'upper right',title='Subject')\n",
    "\n",
    "plt.draw() # Draw the figure so you can find the positon of the legend. \n",
    "\n",
    "# Get the bounding box of the original legend\n",
    "bb = leg.get_bbox_to_anchor().inverse_transformed(ax.transAxes)\n",
    "\n",
    "# Change to location of the legend. \n",
    "xOffset = .12\n",
    "bb.x0 += xOffset\n",
    "bb.x1 += xOffset\n",
    "leg.set_bbox_to_anchor(bb, transform = ax.transAxes)\n",
    "\n",
    "\n",
    "# Update the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here is some text about the EOG stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Joint Attributes\n",
    "\n",
    "Visualize relationships between attributes: Look at the attributes via scatter plots, correlation, cross-tabulation, group-wise averages, etc. as appropriate. Explain any interesting relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Correlation matrix heatmap: between funding and SPG scores/benchmark grades ---add description.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_heat = school_data[['lea_total_expense_num',\n",
    "'lea_salary_expense_pct',\n",
    "'lea_services_expense_pct',\n",
    "'lea_supplies_expense_pct',\n",
    "'lea_instruct_equip_exp_pct',\n",
    "'lea_federal_perpupil_num',\n",
    "'lea_local_perpupil_num',\n",
    "'lea_state_perpupil_num',            \n",
    "'SPG Score',\n",
    "'Reading SPG Score',\n",
    "'EVAAS Growth Score',\n",
    "'lea_sat_avg_score_num',\n",
    "'lea_sat_participation_pct',\n",
    "'lea_ap_participation_pct',\n",
    "'lea_ap_pct_3_or_above',]] \n",
    "                       \n",
    "first_corr = first_heat.corr()\n",
    "\n",
    "plt.figure(figsize = (14,10)) #size of matrix \n",
    "sns.heatmap(first_corr,linewidths=0.5, annot=True); #add correrlation inside boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Correlation matrix heatmap: between funding and attendance/crime rate ---add description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snd_heat = school_data[['lea_total_expense_num',\n",
    "'lea_salary_expense_pct',\n",
    "'lea_services_expense_pct',\n",
    "'lea_supplies_expense_pct',\n",
    "'lea_instruct_equip_exp_pct',\n",
    "'lea_federal_perpupil_num',\n",
    "'lea_local_perpupil_num',\n",
    "'lea_state_perpupil_num',            \n",
    "'lea_avg_daily_attend_pct',\n",
    "'lea_crime_per_c_num',\n",
    "'lea_short_susp_per_c_num',\n",
    "'lea_long_susp_per_c_num',\n",
    "'lea_expelled_per_c_num',]] \n",
    "\n",
    "                       \n",
    "snd_corr = snd_heat.corr()\n",
    "\n",
    "plt.figure(figsize = (14,10)) #size of matrix \n",
    "sns.heatmap(snd_corr,cmap='RdYlGn_r', linewidths=0.5, annot=True); #add correrlation inside boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation matrix heatmap: between race,gender, region ---add description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thd_heat = school_data[['SBE District_Northeast',\n",
    "'SBE District_Northwest',\n",
    "'SBE District_Piedmont-Triad',\n",
    "'SBE District_Sandhills',\n",
    "'SBE District_Southeast',\n",
    "'SBE District_Southwest',\n",
    "'SBE District_Western',\n",
    "'AsianFemalePct',            \n",
    "'AsianMalePct',\n",
    "'BlackFemalePct',\n",
    "'BlackMalePct',\n",
    "'HispanicFemalePct',\n",
    "'HispanicMalePct',\n",
    "'PacificIslandFemalePct',                        \n",
    "'PacificIslandMalePct',                      \n",
    "'MinorityFemalePct',\n",
    "'MinorityMalePct',]] \n",
    "\n",
    "                       \n",
    "thd_corr = thd_heat.corr()\n",
    "\n",
    "plt.figure(figsize = (14,10)) #size of matrix \n",
    "sns.heatmap(thd_corr,linewidths=0.5,annot=True); #add correrlation inside boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data = school_data)\n",
    "\n",
    "def get_redundant_pairs(df):\n",
    "    '''Get diagonal and lower triangular pairs of correlation matrix'''\n",
    "    pairs_to_drop = set()\n",
    "    cols = df.columns\n",
    "    for i in range(0, df.shape[1]):\n",
    "        for j in range(0, i+1):\n",
    "            pairs_to_drop.add((cols[i], cols[j]))\n",
    "    return pairs_to_drop\n",
    "\n",
    "def get_top_abs_correlations(df, n=5):\n",
    "    au_corr = df.corr().abs().unstack()\n",
    "    labels_to_drop = get_redundant_pairs(df)\n",
    "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
    "    return au_corr[0:n]\n",
    "\n",
    "print(\"Top Absolute Correlations\")\n",
    "print(get_top_abs_correlations(df, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_heat = school_data[['SPG Score', 'MinorityFemalePct','MinorityMalePct',\n",
    "                       'BlackFemalePct', 'BlackMalePct', 'IndianFemalePct',\n",
    "                       'AsianFemalePct', 'AsianMalePct' , 'HispanicFemalePct', 'HispanicMalePct', ]]\n",
    "df_heat_corr = df_heat.corr()\n",
    "\n",
    "plt.figure(figsize = (16,12))\n",
    "sns.heatmap(df_heat_corr,annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Attributes and Class\n",
    "\n",
    "Identify and explain interesting relationships between features and the class you are trying to predict (i.e., relationships with variables and the target classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(school_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(school_data):\n",
    "    from matplotlib import pyplot as plt\n",
    "    from matplotlib import cm as cm\n",
    "    \n",
    "      \n",
    "    fig=plt.figure()\n",
    "    ax1=fig.add_subplot(111)\n",
    "    cmap=cm.get_cmap('jet',30)\n",
    "    cmap = cm.get_cmap('jet', 30)\n",
    "    cax = ax1.imshow(df.corr(), interpolation=\"nearest\", cmap=cmap)\n",
    "    ax1.grid(True)\n",
    "    labels=['column_names',]\n",
    "    plt.title('Columns Correlations')\n",
    "    ax1.set_xticklabels(labels,fontsize=3)\n",
    "    ax1.set_yticklabels(labels,fontsize=3)\n",
    "    # Add colorbar\n",
    "    fig.colorbar(cax, ticks=[-1,-.95,-.90,0,.05,.1,.15,.2,.25,.75,.8,.85,.90,.95,1])\n",
    "    plt.show()\n",
    "\n",
    "correlation_matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_heat = school_data[['SPG Score', 'flicensed_teach_pct',#school nad teacher characters\n",
    "                    'lea_local_perpupil_num','lea_total_expense_num','lea_salary_expense_pct', #funding\n",
    "                    'lea_supplies_expense_pct','stud_internet_comp_num',#funding\n",
    "                    'avg_daily_attend_pct','crime_per_c_num', ]] #environment\n",
    "df_heat_corr = df_heat.corr()\n",
    "\n",
    "plt.figure(figsize = (16,12))\n",
    "sns.heatmap(df_heat_corr,annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created a categorical multiclass variable from SP Grade\n",
    "SPG_Grade_col = school_data.filter(regex=('^SPG\\WGrade')).columns\n",
    "school_data['SPG Grade']= school_data[SPG_Grade_col].apply(lambda row:'A' if row.any()!=1 else \n",
    "                                 row[0]*'A+NG'+row[1]*'B'+row[2]*'C'+row[3]*'D'+row[4]*'F'+row[5]*'I',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expense=school_data[['SPG Score', 'SPG Grade', 'lea_salary_expense_pct', 'lea_services_expense_pct', 'lea_supplies_expense_pct', 'lea_instruct_equip_exp_pct']]\n",
    "\n",
    "sns.pairplot(expense, hue='SPG Grade');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minority=school_data[['SPG Score', 'SPG Grade', 'Asian', 'Black', 'Minority', 'Hispanic']]\n",
    "sns.pairplot(minority, hue='SPG Grade');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Features\n",
    "\n",
    "Are there other features that could be added to the data or created from existing features? Which ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert a new feature for SPG as continous values\n",
    "school_data['SPG']=6\n",
    "school_data['SPG'] = school_data['SPG Grade_A+NG']*7 + school_data['SPG Grade_B']*5 + school_data['SPG Grade_C']*4 + school_data['SPG Grade_D']*3 + school_data['SPG Grade_F']*2 + school_data['SPG Grade_I']*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "school_data['Asian']=school_data['AsianFemalePct']+school_data['AsianMalePct']\n",
    "school_data['Black']=school_data['BlackFemalePct']+school_data['BlackMalePct']\n",
    "school_data['Hispanic']=school_data['HispanicFemalePct']+school_data['HispanicMalePct']\n",
    "school_data['Minority']=school_data['MinorityFemalePct']+school_data['MinorityMalePct']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exceptional Work\n",
    "\n",
    "You have free reign to provide additional analyses. One idea: implement dimensionality reduction, then visualize and interpret the results.\n",
    "\n",
    "PCA - extracting important variables (in form of components) from a large set of variables available in a data set.\n",
    "\n",
    "when we have so many features, that means that we are working with high dimentional data. therefore we need to reduce our data dimentions. PCA is the Dimentionality reduction techniques that is very usefull in our project. There are 334 features, and only 2443 rows in the data set. Having too many features and limited data makes Principal Component Analysis a great candidate to find a model that predicts the NC school performance grades. We only consider the quantitative columns, and we would like to choose as many components as needed to explain around 80% of the SPG. Based on the variance explained graph, we must choose at least 62 principal components to explain 80% of the SPG. Variance explained graph illustrates the necessity of having 62 components to address 80% of the SPG.\n",
    "\n",
    "The other dimentionality reduction technique is using Random Forest Classifier.\n",
    "it lists the top 40 features along with their scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#split data into X and y dataframes\n",
    "\n",
    "X = school_data.loc[:,school_data.dtypes==float]\n",
    "y = school_data['SPG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#split X and y into test and train sets.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applied a scaling procedure to scale the size of variables in the x dataframe\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#run PCA on 62 components of the dataset, which explains 85% of the variance.\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=62)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "d = {'ratio':pca.explained_variance_ratio_,'total':pca.explained_variance_ratio_.cumsum()}\n",
    "\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pca.explained_variance_ratio_, '-o')\n",
    "plt.xlim(0,62)\n",
    "plt.ylim(0,0.2)\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run a Kfolds cross validation model on the data set and predicted y from the set\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "fold = KFold(len(y_train), n_folds=10, shuffle=True)\n",
    "classifier = LogisticRegressionCV(Cs=list(np.power(10.0, np.arange(-10, 10)))\n",
    "        ,penalty='l2'\n",
    "        ,scoring='roc_auc'\n",
    "        ,cv=fold\n",
    "        ,max_iter=4000\n",
    "        ,fit_intercept=True\n",
    "       ,solver='newton-cg')\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created a confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can't get this to work\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "y = label_binarize(y, classes=[0, 1, 2, 3, 4, 5, 6])\n",
    "\n",
    "n_classes = y.shape[1]\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "for i in range(n_classes):\n",
    "    plt.figure()\n",
    "    plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f)' % roc_auc[i])\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `RandomForestClassifier`\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Isolate Data, class labels and column values\n",
    "X = school_data.iloc[:,0:40]\n",
    "Y = school_data.iloc[:,-1]\n",
    "names = school_data.columns.values\n",
    "\n",
    "# Build the model\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# Fit the model\n",
    "rfc.fit(X, Y)\n",
    "\n",
    "# Print the results\n",
    "print(\"Features sorted by their score:\")\n",
    "print(sorted(zip(map(lambda x: round(x, 4), rfc.feature_importances_), names), reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
