{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minilab 2\n",
    "\n",
    "<b>Class:</b> MSDS 7331 Data Mining\n",
    "<br> <b>Dataset:</b> Belk Endowment Educational Attainment Data \n",
    "\n",
    "<h1 style=\"font-size:150%;\"> Teammates </h1>\n",
    "Maryam Shahini\n",
    "<br> Murtada Shubbar\n",
    "<br> Michael Toolin\n",
    "<br> Steven Millett"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "import seaborn as sns\n",
    "import math\n",
    "import re\n",
    "import statistics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# The 2017 Public Schools Machine Learning Date Set is being used throughout this analysis.  The _ML suffix is removed to less name space size\n",
    "#\n",
    "# Load Full Public School Data Frames for each year\n",
    "\n",
    "school_data = pd.read_csv('../Data/2017/machine Learning Datasets/PublicSchools2017_ML.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Models\n",
    "Create a logistic regression model and a support vector machine model for the classification task involved with your dataset. Assess how well each model performs (use 80/20 training/testing split for your data). Adjust parameters of the models to make them more accurate. If your dataset size requires the use of stochastic gradient descent, then linear kernel only is fine to use. That is, the SGDClassifier is fine to use for optimizing logistic regression and linear support vector machines. For many problems, SGD will be required in order to train the SVM model in a reasonable timeframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 30   4   6   0   1   0   0]\n",
      " [  1  10   2   1   0   0   0]\n",
      " [  7   5 102  21   0   0   0]\n",
      " [  1   0   9 161  11   2   0]\n",
      " [  0   0   1  21  59   7   0]\n",
      " [  0   0   0   0   7   9   0]\n",
      " [  1   0   0   0   0   0  10]]\n",
      "0.7791411042944786\n",
      "[[ 32   2   7   0   0   0   0]\n",
      " [  0  14   0   0   0   0   0]\n",
      " [  4   0 114  17   0   0   0]\n",
      " [  0   0  10 164   8   2   0]\n",
      " [  0   0   0  20  67   1   0]\n",
      " [  0   0   0   0   5  11   0]\n",
      " [  0   0   0   0   0   0  11]]\n",
      "0.8445807770961146\n",
      "[[  0   0   0  41   0   0   0]\n",
      " [  0   0   0  14   0   0   0]\n",
      " [  0   0   0 135   0   0   0]\n",
      " [  0   0   0 184   0   0   0]\n",
      " [  0   0   0  88   0   0   0]\n",
      " [  0   0   0  16   0   0   0]\n",
      " [  0   0   0  11   0   0   0]]\n",
      "0.37627811860940696\n",
      "[[ 35   2   4   0   0   0   0]\n",
      " [  0  14   0   0   0   0   0]\n",
      " [  5   0 117  13   0   0   0]\n",
      " [  1   0  11 165   7   0   0]\n",
      " [  0   0   0  20  67   1   0]\n",
      " [  0   0   0   0   4  12   0]\n",
      " [  1   0   0   0   0   0  10]]\n",
      "0.8588957055214724\n"
     ]
    }
   ],
   "source": [
    "# %load Data_Mining_Model_Creation_Part_1.py\n",
    "# In[1]:\n",
    "\n",
    "#split data into X and y dataframes\n",
    "\n",
    "SPG_Grade_col = school_data.filter(regex=('^SPG\\WGrade')).columns\n",
    "y = school_data[SPG_Grade_col].apply(lambda row:'A' if row.any()!=1 else \n",
    "                                 row[0]*'A+NG'+row[1]*'B'+row[2]*'C'+row[3]*'D'+row[4]*'F'+row[5]*'I',axis=1)\n",
    "\n",
    "X = school_data[school_data.columns.drop(list(school_data.filter(regex='^SPG\\WGrade|unit_code')))]\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "#split X and y into test and train sets.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "#applied a scaling procedure to scale the size of variables in the x dataframe\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train_scale = sc.fit_transform(X_train)\n",
    "X_test_scale = sc.transform(X_test)\n",
    "\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "#Initializing the Logistic Regression function to a classifier variable\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "#Fitting the Logistic Regression Classifier to the original dataset and then predicting.\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred_Log_Reg = classifier.predict(X_test)\n",
    "\n",
    "#Fitting the Logistic Regression Classifier to the scaled dataset and then predicting.\n",
    "classifier.fit(X_train_scale, y_train)\n",
    "y_pred_Log_Reg_scale = classifier.predict(X_test_scale)\n",
    "\n",
    "# =============================================================================\n",
    "# Below we have the confusion matrix and accuracy score for SVM without scaling. \n",
    "# Based on the accuracy score it is evident that Logistic Regression handles non-standardized variables relatively well.\n",
    "# \n",
    "# =============================================================================\n",
    "\n",
    "# In[52]:\n",
    "\n",
    "\n",
    "#Created a confusion matrix\n",
    "\n",
    "\n",
    "\n",
    "cm_Log_Reg = confusion_matrix(y_test, y_pred_Log_Reg)\n",
    "\n",
    "print(cm_Log_Reg)\n",
    "print(accuracy_score(y_test, y_pred_Log_Reg))\n",
    "\n",
    "cm_Log_Reg_scale = confusion_matrix(y_test, y_pred_Log_Reg_scale)\n",
    "\n",
    "print(cm_Log_Reg_scale)\n",
    "print(accuracy_score(y_test, y_pred_Log_Reg_scale))\n",
    "\n",
    "\n",
    "# In[53]:\n",
    "\n",
    "#Initializing the SVM function to a classifier variable\n",
    "classifier = svm.SVC()\n",
    "\n",
    "#Fitting the SVM Classifier to the original dataset and then predicting.\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred_svm = classifier.predict(X_test)\n",
    "\n",
    "#Fitting the SVM Classifier to the scaled dataset and then predicting.\n",
    "classifier.fit(X_train_scale, y_train)\n",
    "y_pred_svm_scale = classifier.predict(X_test_scale)\n",
    "\n",
    "# In[53]:\n",
    "cm_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "\n",
    "print(cm_svm)\n",
    "print(accuracy_score(y_test, y_pred_svm))\n",
    "\n",
    "#Assigning a confusion matrix with the scaled dataset.\n",
    "cm_svm_scale = confusion_matrix(y_test, y_pred_svm_scale)\n",
    "\n",
    "print(cm_svm_scale)\n",
    "print(accuracy_score(y_test, y_pred_svm_scale))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Advantages\n",
    "Discuss the advantages of each model for each classification task. Does one type of model offer superior performance over another in terms of prediction accuracy? In terms of training time or efficiency? Explain in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Looking at the accuracy of the two models there are a couple of obvious observations.\n",
    "\n",
    "1. SVM is negatively to non-standardized observations.\n",
    "2. Logistic Regression ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret Feature Importance\n",
    "Use the weights from logistic regression to interpret the importance of different features for the classification task. Explain your interpretation in detail. Why do you think some variables are more important?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret Support Vectors\n",
    "Look at the chosen support vectors for the classification task. Do these provide any insight into the data? Explain. If you used stochastic gradient descent (and therefore did not explicitly solve for support vectors), try subsampling your data to train the SVC modelâ€” then analyze the support vectors from the subsampled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
