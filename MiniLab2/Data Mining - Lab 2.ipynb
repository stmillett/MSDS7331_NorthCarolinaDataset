{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minilab 2\n",
    "\n",
    "<b>Class:</b> MSDS 7331 Data Mining\n",
    "<br> <b>Dataset:</b> Belk Endowment Educational Attainment Data \n",
    "\n",
    "<h1 style=\"font-size:150%;\"> Teammates </h1>\n",
    "Maryam Shahini\n",
    "<br> Murtada Shubbar\n",
    "<br> Michael Toolin\n",
    "<br> Steven Millett"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "import seaborn as sns\n",
    "import math\n",
    "import re\n",
    "import statistics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# The 2017 Public Schools Machine Learning Date Set is being used throughout this analysis.  The _ML suffix is removed to less name space size\n",
    "#\n",
    "# Load Full Public School Data Frames for each year\n",
    "\n",
    "school_data = pd.read_csv('../Data/2017/machine Learning Datasets/PublicSchools2017_ML.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Models\n",
    "Create a logistic regression model and a support vector machine model for the classification task involved with your dataset. Assess how well each model performs (use 80/20 training/testing split for your data). Adjust parameters of the models to make them more accurate. If your dataset size requires the use of stochastic gradient descent, then linear kernel only is fine to use. That is, the SGDClassifier is fine to use for optimizing logistic regression and linear support vector machines. For many problems, SGD will be required in order to train the SVM model in a reasonable timeframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#split data into X and y dataframes\n",
    "\n",
    "SPG_Grade_col = school_data.filter(regex=('^SPG\\WGrade')).columns\n",
    "y = school_data[SPG_Grade_col].apply(lambda row:'A' if row.any()!=1 else \n",
    "                                 row[0]*'A+NG'+row[1]*'B'+row[2]*'C'+row[3]*'D'+row[4]*'F'+row[5]*'I',axis=1)\n",
    "\n",
    "X = school_data[school_data.columns.drop(list(school_data.filter(regex='^SPG\\WGrade|unit_code')))]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#split X and y into test and train sets.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applied a scaling procedure to scale the size of variables in the x dataframe\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train_scale = sc.fit_transform(X_train)\n",
    "X_test_scale = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the Logistic Regression function to a classifier variable\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "#Fitting the Logistic Regression Classifier to the original dataset and then predicting.\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred_Log_Reg = classifier.predict(X_test)\n",
    "\n",
    "#Fitting the Logistic Regression Classifier to the scaled dataset and then predicting.\n",
    "classifier.fit(X_train_scale, y_train)\n",
    "y_pred_Log_Reg_scale = classifier.predict(X_test_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have the confusion matrix and accuracy score for SVM without scaling. Based on the accuracy score it is evident that Logistic Regression handles non-standardized variables relatively well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 18   5   6   0   0   0   1]\n",
      " [  0   8  10   0   0   0   0]\n",
      " [  5   4 106  22   0   0   0]\n",
      " [  0   1  15 159  10   0   0]\n",
      " [  0   0   0  18  66   5   0]\n",
      " [  0   0   0   0   9  11   0]\n",
      " [  0   0   0   0   0   0  10]]\n",
      "0.7730061349693251\n"
     ]
    }
   ],
   "source": [
    "# Created a confusion matrix for the \n",
    "cm_Log_Reg = confusion_matrix(y_test, y_pred_Log_Reg)\n",
    "\n",
    "print(cm_Log_Reg)\n",
    "print(accuracy_score(y_test, y_pred_Log_Reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have the confusion matrix and accuracy score for logistic regression with scaling. Based on the accuracy score it is obvious that the logistic model is negatively affected by the different scales of the dataset, but the scaled model does not dramatically improve the fit of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 24   1   5   0   0   0   0]\n",
      " [  2  16   0   0   0   0   0]\n",
      " [  5   0 116  16   0   0   0]\n",
      " [  1   0  20 157   7   0   0]\n",
      " [  0   0   2  17  66   4   0]\n",
      " [  1   0   0   0   7  12   0]\n",
      " [  0   0   0   0   0   0  10]]\n",
      "0.820040899795501\n"
     ]
    }
   ],
   "source": [
    "cm_Log_Reg_scale = confusion_matrix(y_test, y_pred_Log_Reg_scale)\n",
    "\n",
    "print(cm_Log_Reg_scale)\n",
    "print(accuracy_score(y_test, y_pred_Log_Reg_scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the SVM function to a classifier variable\n",
    "classifier = svm.SVC()\n",
    "\n",
    "#Fitting the SVM Classifier to the original dataset and then predicting.\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred_svm = classifier.predict(X_test)\n",
    "\n",
    "#Fitting the SVM Classifier to the scaled dataset and then predicting.\n",
    "classifier.fit(X_train_scale, y_train)\n",
    "y_pred_svm_scale = classifier.predict(X_test_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have the confusion matrix and accuracy score for SVM without scaling. Based on the accuracy score it is obvious that SVM is heavily influenced by non-standardized features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0  30   0   0   0]\n",
      " [  0   0   0  18   0   0   0]\n",
      " [  0   0   0 137   0   0   0]\n",
      " [  0   0   0 185   0   0   0]\n",
      " [  0   0   0  89   0   0   0]\n",
      " [  0   0   0  20   0   0   0]\n",
      " [  0   0   0  10   0   0   0]]\n",
      "0.3783231083844581\n"
     ]
    }
   ],
   "source": [
    "#Assigning a confusion matrix with the original dataset.\n",
    "cm_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "\n",
    "print(cm_svm)\n",
    "print(accuracy_score(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have the confusion matrix and accuracy score for SVM with scaling. Based on the accuracy score it is obvious that the SVM model is negatively affected by the different scales of the dataset. With the use of the standard scaler function we find that SVM is a superior model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 26   0   3   1   0   0   0]\n",
      " [  1  17   0   0   0   0   0]\n",
      " [  4   0 117  15   1   0   0]\n",
      " [  1   0  10 168   6   0   0]\n",
      " [  0   0   0  15  73   1   0]\n",
      " [  1   0   0   0   7  12   0]\n",
      " [  0   0   0   0   0   0  10]]\n",
      "0.8650306748466258\n"
     ]
    }
   ],
   "source": [
    "#Assigning a confusion matrix with the scaled dataset.\n",
    "cm_svm_scale = confusion_matrix(y_test, y_pred_svm_scale)\n",
    "\n",
    "print(cm_svm_scale)\n",
    "print(accuracy_score(y_test, y_pred_svm_scale))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Advantages\n",
    "Discuss the advantages of each model for each classification task. Does one type of model offer superior performance over another in terms of prediction accuracy? In terms of training time or efficiency? Explain in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Looking at the accuracy of the two models there are a couple of obvious observations.\n",
    "\n",
    "1. SVM is negatively to non-standardized observations.\n",
    "2. Logistic Regression ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret Feature Importance\n",
    "Use the weights from logistic regression to interpret the importance of different features for the classification task. Explain your interpretation in detail. Why do you think some variables are more important?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret Support Vectors\n",
    "Look at the chosen support vectors for the classification task. Do these provide any insight into the data? Explain. If you used stochastic gradient descent (and therefore did not explicitly solve for support vectors), try subsampling your data to train the SVC model— then analyze the support vectors from the subsampled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
