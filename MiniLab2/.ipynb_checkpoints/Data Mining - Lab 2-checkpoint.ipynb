{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minilab 2\n",
    "\n",
    "<b>Class:</b> MSDS 7331 Data Mining\n",
    "<br> <b>Dataset:</b> Belk Endowment Educational Attainment Data \n",
    "\n",
    "<h1 style=\"font-size:150%;\"> Teammates </h1>\n",
    "Maryam Shahini\n",
    "<br> Murtada Shubbar\n",
    "<br> Michael Toolin\n",
    "<br> Steven Millett"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "import seaborn as sns\n",
    "import math\n",
    "import re\n",
    "import statistics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# The 2017 Public Schools Machine Learning Date Set is being used throughout this analysis.  The _ML suffix is removed to less name space size\n",
    "#\n",
    "# Load Full Public School Data Frames for each year\n",
    "\n",
    "school_data = pd.read_csv('../Data/2017/machine Learning Datasets/PublicSchools2017_ML.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Models\n",
    "Create a logistic regression model and a support vector machine model for the classification task involved with your dataset. Assess how well each model performs (use 80/20 training/testing split for your data). Adjust parameters of the models to make them more accurate. If your dataset size requires the use of stochastic gradient descent, then linear kernel only is fine to use. That is, the SGDClassifier is fine to use for optimizing logistic regression and linear support vector machines. For many problems, SGD will be required in order to train the SVM model in a reasonable timeframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#split data into X and y dataframes\n",
    "\n",
    "SPG_Grade_col = school_data.filter(regex=('^SPG\\WGrade')).columns\n",
    "y = school_data[SPG_Grade_col].apply(lambda row:'A' if row.any()!=1 else \n",
    "                                 row[0]*'A+NG'+row[1]*'B'+row[2]*'C'+row[3]*'D'+row[4]*'F'+row[5]*'I',axis=1)\n",
    "\n",
    "X = school_data[school_data.columns.drop(list(school_data.filter(regex='^SPG\\WGrade|unit_code')))]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#split X and y into test and train sets.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applied a scaling procedure to scale the size of variables in the x dataframe\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train_scale = sc.fit_transform(X_train)\n",
    "X_test_scale = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-718293bdb2d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#Fitting the Logistic Regression Classifier to the scaled dataset and then predicting.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mLog_Reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_scale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0my_pred_Log_Reg_scale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLog_Reg_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_scale\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\rfe.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \"\"\"\n\u001b[1;32m--> 134\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\rfe.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, step_score)\u001b[0m\n\u001b[0;32m    171\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fitting estimator with %d features.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msupport_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m             \u001b[1;31m# Get coefs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1231\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1232\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1233\u001b[1;33m                 sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m   1234\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1235\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 890\u001b[1;33m         epsilon, sample_weight)\n\u001b[0m\u001b[0;32m    891\u001b[0m     \u001b[1;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m     \u001b[1;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# #Initializing the Logistic Regression function to a classifier variable\n",
    "# Log_Reg_classifier = LogisticRegression()\n",
    "\n",
    "# #Log_Reg = RFE(Log_Reg_classifier\n",
    "\n",
    "\n",
    "# #Fitting the Logistic Regression Classifier to the original dataset and then predicting.\n",
    "# Log_Reg_classifier.fit(X_train, y_train)\n",
    "# y_pred_Log_Reg = Log_Reg_classifier.predict(X_test)\n",
    "\n",
    "# #Fitting the Logistic Regression Classifier to the scaled dataset and then predicting.\n",
    "# Log_Reg.fit(X_train_scale, y_train)\n",
    "# y_pred_Log_Reg_scale = Log_Reg_classifier.predict(X_test_scale)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have the confusion matrix and accuracy score for SVM without scaling. Based on the accuracy score it is evident that Logistic Regression handles non-standardized variables relatively well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created a confusion matrix for the \n",
    "cm_Log_Reg = confusion_matrix(y_test, y_pred_Log_Reg)\n",
    "\n",
    "print(cm_Log_Reg)\n",
    "print('Test dataset accuracy '+round(accuracy_score(y_test, y_pred_Log_Reg),6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have the confusion matrix and accuracy score for logistic regression with scaling. Based on the accuracy score it is obvious that the logistic model is negatively affected by the different scales of the dataset, but the scaled model does not dramatically improve the fit of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_Log_Reg_scale = confusion_matrix(y_test, y_pred_Log_Reg_scale)\n",
    "\n",
    "print(cm_Log_Reg_scale)\n",
    "print('Test dataset with scaling accuracy '+round(accuracy_score(y_test, y_pred_Log_Reg_scale),6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the SVM function to a classifier variable\n",
    "classifier = svm.SVC()\n",
    "\n",
    "#Fitting the SVM Classifier to the original dataset and then predicting.\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred_svm = classifier.predict(X_test)\n",
    "\n",
    "#Fitting the SVM Classifier to the scaled dataset and then predicting.\n",
    "classifier.fit(X_train_scale, y_train)\n",
    "y_pred_svm_scale = classifier.predict(X_test_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have the confusion matrix and accuracy score for SVM without scaling. Based on the accuracy score it is obvious that SVM is heavily influenced by non-standardized features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0  30   0   0   0]\n",
      " [  0   0   0  18   0   0   0]\n",
      " [  0   0   0 137   0   0   0]\n",
      " [  0   0   0 185   0   0   0]\n",
      " [  0   0   0  89   0   0   0]\n",
      " [  0   0   0  20   0   0   0]\n",
      " [  0   0   0  10   0   0   0]]\n",
      "0.3783231083844581\n"
     ]
    }
   ],
   "source": [
    "#Assigning a confusion matrix with the original dataset.\n",
    "cm_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "\n",
    "print(cm_svm)\n",
    "print(accuracy_score(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have the confusion matrix and accuracy score for SVM with scaling. Based on the accuracy score it is obvious that the SVM model is negatively affected by the different scales of the dataset. With the use of the standard scaler function we find that SVM is a superior model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 26   0   3   1   0   0   0]\n",
      " [  1  17   0   0   0   0   0]\n",
      " [  4   0 117  15   1   0   0]\n",
      " [  1   0  10 168   6   0   0]\n",
      " [  0   0   0  15  73   1   0]\n",
      " [  1   0   0   0   7  12   0]\n",
      " [  0   0   0   0   0   0  10]]\n",
      "0.8650306748466258\n"
     ]
    }
   ],
   "source": [
    "#Assigning a confusion matrix with the scaled dataset.\n",
    "cm_svm_scale = confusion_matrix(y_test, y_pred_svm_scale)\n",
    "\n",
    "print(cm_svm_scale)\n",
    "print(accuracy_score(y_test, y_pred_svm_scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External CV accuracy score of LogisticRegression: 0.795501\n",
      "Internal CV accuracy score of LogisticRegression: 0.778359\n",
      "External CV accuracy score of SGDClassifier: 0.638037\n",
      "Internal CV accuracy score of SGDClassifier: 0.546033\n",
      "External CV accuracy score of SVC: 0.404908\n",
      "Internal CV accuracy score of SVC: 0.39967\n",
      "External CV accuracy score of LinearSVC: 0.486708\n",
      "Internal CV accuracy score of LinearSVC: 0.443124\n"
     ]
    }
   ],
   "source": [
    "numFolds = 10\n",
    "kf = KFold(len(y_train), numFolds, shuffle=True)\n",
    "\n",
    "# These are \"Class objects\". For each Class, find the AUC through\n",
    "# 10 fold cross validation.\n",
    "Models = [LogisticRegression, SGDClassifier, svm.SVC, LinearSVC]\n",
    "params = [{}, {\"loss\": \"log\", \"penalty\": \"l2\", 'max_iter':1000},{},{}]\n",
    "for param, Model in zip(params, Models):\n",
    "    total = 0\n",
    "    for train_indices, test_indices in kf:\n",
    "\n",
    "        #print(X_train.iloc[train_indices])\n",
    "        #print(y_train.iloc[train_indices])\n",
    "        train_X = X_train.iloc[train_indices]; \n",
    "        train_Y = y_train.iloc[train_indices]\n",
    "        test_X = X_train.iloc[test_indices]; \n",
    "        test_Y = y_train.iloc[test_indices]\n",
    "\n",
    "        reg = Model(**param)\n",
    "        reg.fit(train_X, train_Y)\n",
    "        predictions = reg.predict(test_X)\n",
    "        total += accuracy_score(test_Y, predictions)\n",
    "    accuracy = total / numFolds\n",
    "    external_predict = accuracy_score(y_test, reg.predict(X_test))\n",
    "\n",
    "    print (\"External CV accuracy score of {0}: {1}\".format(\n",
    "            Model.__name__,round(external_predict,6)))\n",
    "    print (\"Internal CV accuracy score of {0}: {1}\".format(Model.__name__, round(accuracy, 6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External scaled CV accuracy score of LogisticRegression: 0.807771\n",
      "Internal scaled CV accuracy score of LogisticRegression: 0.823956\n",
      "External scaled CV accuracy score of SGDClassifier: 0.777096\n",
      "Internal scaled CV accuracy score of SGDClassifier: 0.769212\n",
      "External scaled CV accuracy score of SVC: 0.844581\n",
      "Internal scaled CV accuracy score of SVC: 0.849537\n",
      "External scaled CV accuracy score of LinearSVC: 0.783231\n",
      "Internal scaled CV accuracy score of LinearSVC: 0.787622\n"
     ]
    }
   ],
   "source": [
    "numFolds = 10\n",
    "kf = KFold(len(y_train), numFolds, shuffle=True)\n",
    "\n",
    "# These are \"Class objects\". For each Class, find the AUC through\n",
    "# 10 fold cross validation.\n",
    "Models = [LogisticRegression, SGDClassifier, svm.SVC, LinearSVC]\n",
    "params = [{}, {\"loss\": \"log\", \"penalty\": \"l2\", 'max_iter':1000},{},{}]\n",
    "for param, Model in zip(params, Models):\n",
    "    total = 0\n",
    "    for train_indices, test_indices in kf:\n",
    "        #print(X_train.iloc[train_indices])\n",
    "        #print(y_train.iloc[train_indices])\n",
    "        train_X = X_train_scale[train_indices, :]; \n",
    "        train_Y = y_train.iloc[train_indices]\n",
    "        test_X = X_train_scale[test_indices, :]; \n",
    "        test_Y = y_train.iloc[test_indices]\n",
    "\n",
    "        reg = Model(**param)\n",
    "        reg.fit(train_X, train_Y)\n",
    "        predictions = reg.predict(test_X)\n",
    "        total += accuracy_score(test_Y, predictions)\n",
    "    accuracy = total / numFolds\n",
    "    external_predict = accuracy_score(y_test, reg.predict(X_test_scale))\n",
    "\n",
    "    print (\"External scaled CV accuracy score of {0}: {1}\".format(\n",
    "            Model.__name__,round(external_predict,6)))\n",
    "    print (\"Internal scaled CV accuracy score of {0}: {1}\".format(Model.__name__, round(accuracy, 6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Advantages\n",
    "Discuss the advantages of each model for each classification task. Does one type of model offer superior performance over another in terms of prediction accuracy? In terms of training time or efficiency? Explain in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the accuracy of the two models there are a couple of obvious observations.\n",
    "\n",
    "1. SVM is negatively to non-standardized observations.\n",
    "2. Logistic Regression ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret Feature Importance\n",
    "Use the weights from logistic regression to interpret the importance of different features for the classification task. Explain your interpretation in detail. Why do you think some variables are more important?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret Support Vectors\n",
    "Look at the chosen support vectors for the classification task. Do these provide any insight into the data? Explain. If you used stochastic gradient descent (and therefore did not explicitly solve for support vectors), try subsampling your data to train the SVC model— then analyze the support vectors from the subsampled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
